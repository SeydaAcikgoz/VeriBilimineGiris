{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sınıflandırma Problemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri Seti: Diabetis\n",
    "\n",
    "Hedef Değişken, Outcome: 1--> Şeker Hastası\n",
    "                         0--> Sağlıklı\n",
    "                         \n",
    "Amaç: Bir hasta geldiğinde onun şeker hastası olup olmadığını tahmin eden bir model oluşturmak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lojistik Regresyon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"Outcome\"].value_counts().plot.barh();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>768.0</td>\n",
       "      <td>3.845052</td>\n",
       "      <td>3.369578</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>768.0</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.00000</td>\n",
       "      <td>117.0000</td>\n",
       "      <td>140.25000</td>\n",
       "      <td>199.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>768.0</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>0.000</td>\n",
       "      <td>62.00000</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>122.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>768.0</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>768.0</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.5000</td>\n",
       "      <td>127.25000</td>\n",
       "      <td>846.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>768.0</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.30000</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>36.60000</td>\n",
       "      <td>67.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>768.0</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.24375</td>\n",
       "      <td>0.3725</td>\n",
       "      <td>0.62625</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>768.0</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>21.000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>81.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outcome</th>\n",
       "      <td>768.0</td>\n",
       "      <td>0.348958</td>\n",
       "      <td>0.476951</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count        mean         std     min       25%  \\\n",
       "Pregnancies               768.0    3.845052    3.369578   0.000   1.00000   \n",
       "Glucose                   768.0  120.894531   31.972618   0.000  99.00000   \n",
       "BloodPressure             768.0   69.105469   19.355807   0.000  62.00000   \n",
       "SkinThickness             768.0   20.536458   15.952218   0.000   0.00000   \n",
       "Insulin                   768.0   79.799479  115.244002   0.000   0.00000   \n",
       "BMI                       768.0   31.992578    7.884160   0.000  27.30000   \n",
       "DiabetesPedigreeFunction  768.0    0.471876    0.331329   0.078   0.24375   \n",
       "Age                       768.0   33.240885   11.760232  21.000  24.00000   \n",
       "Outcome                   768.0    0.348958    0.476951   0.000   0.00000   \n",
       "\n",
       "                               50%        75%     max  \n",
       "Pregnancies                 3.0000    6.00000   17.00  \n",
       "Glucose                   117.0000  140.25000  199.00  \n",
       "BloodPressure              72.0000   80.00000  122.00  \n",
       "SkinThickness              23.0000   32.00000   99.00  \n",
       "Insulin                    30.5000  127.25000  846.00  \n",
       "BMI                        32.0000   36.60000   67.10  \n",
       "DiabetesPedigreeFunction    0.3725    0.62625    2.42  \n",
       "Age                        29.0000   41.00000   81.00  \n",
       "Outcome                     0.0000    1.00000    1.00  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df[\"Outcome\"]\n",
    "X = df.drop([\"Outcome\"], axis=1)\n",
    "#Hedef (bağımlı) değişkeni y 'ye\n",
    "#Bağımsız değişkenleri X'e atadık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#scikit-learn kütüphanesi ile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.30, \n",
    "                                                    random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loj = LogisticRegression(solver = \"liblinear\")\n",
    "loj_model = loj.fit(X_train,y_train)\n",
    "loj_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahmin & Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_probs = loj_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66556744, 0.33443256],\n",
       "       [0.77598701, 0.22401299],\n",
       "       [0.80327736, 0.19672264],\n",
       "       [0.78003189, 0.21996811],\n",
       "       [0.55055019, 0.44944981],\n",
       "       [0.57722648, 0.42277352],\n",
       "       [0.94464545, 0.05535455],\n",
       "       [0.33847767, 0.66152233],\n",
       "       [0.46994761, 0.53005239],\n",
       "       [0.32280343, 0.67719657],\n",
       "       [0.68856705, 0.31143295],\n",
       "       [0.22117492, 0.77882508],\n",
       "       [0.53435024, 0.46564976],\n",
       "       [0.70728461, 0.29271539],\n",
       "       [0.86647924, 0.13352076],\n",
       "       [0.59978448, 0.40021552],\n",
       "       [0.82965   , 0.17035   ],\n",
       "       [0.86614247, 0.13385753],\n",
       "       [0.12600545, 0.87399455],\n",
       "       [0.47664276, 0.52335724],\n",
       "       [0.73631286, 0.26368714],\n",
       "       [0.88113271, 0.11886729],\n",
       "       [0.53182999, 0.46817001],\n",
       "       [0.84270371, 0.15729629],\n",
       "       [0.47624032, 0.52375968],\n",
       "       [0.21750727, 0.78249273],\n",
       "       [0.79516613, 0.20483387],\n",
       "       [0.92112943, 0.07887057],\n",
       "       [0.74284378, 0.25715622],\n",
       "       [0.83460494, 0.16539506],\n",
       "       [0.22845218, 0.77154782],\n",
       "       [0.25289188, 0.74710812],\n",
       "       [0.27749352, 0.72250648],\n",
       "       [0.17508317, 0.82491683],\n",
       "       [0.5439939 , 0.4560061 ],\n",
       "       [0.37996737, 0.62003263],\n",
       "       [0.13326024, 0.86673976],\n",
       "       [0.786781  , 0.213219  ],\n",
       "       [0.51694873, 0.48305127],\n",
       "       [0.25442554, 0.74557446],\n",
       "       [0.8640759 , 0.1359241 ],\n",
       "       [0.4542738 , 0.5457262 ],\n",
       "       [0.53514488, 0.46485512],\n",
       "       [0.62040479, 0.37959521],\n",
       "       [0.92994395, 0.07005605],\n",
       "       [0.50030488, 0.49969512],\n",
       "       [0.50190414, 0.49809586],\n",
       "       [0.75739979, 0.24260021],\n",
       "       [0.63159428, 0.36840572],\n",
       "       [0.13256862, 0.86743138],\n",
       "       [0.89195625, 0.10804375],\n",
       "       [0.42501305, 0.57498695],\n",
       "       [0.26671593, 0.73328407],\n",
       "       [0.69127581, 0.30872419],\n",
       "       [0.79049338, 0.20950662],\n",
       "       [0.9144398 , 0.0855602 ],\n",
       "       [0.29644685, 0.70355315],\n",
       "       [0.95116884, 0.04883116],\n",
       "       [0.62243577, 0.37756423],\n",
       "       [0.28486586, 0.71513414],\n",
       "       [0.36145373, 0.63854627],\n",
       "       [0.65339968, 0.34660032],\n",
       "       [0.7113664 , 0.2886336 ],\n",
       "       [0.6836407 , 0.3163593 ],\n",
       "       [0.88303305, 0.11696695],\n",
       "       [0.52042054, 0.47957946],\n",
       "       [0.89781053, 0.10218947],\n",
       "       [0.41731024, 0.58268976],\n",
       "       [0.91615335, 0.08384665],\n",
       "       [0.28696719, 0.71303281],\n",
       "       [0.34627058, 0.65372942],\n",
       "       [0.89239837, 0.10760163],\n",
       "       [0.7475032 , 0.2524968 ],\n",
       "       [0.8545448 , 0.1454552 ],\n",
       "       [0.8240803 , 0.1759197 ],\n",
       "       [0.6493017 , 0.3506983 ],\n",
       "       [0.79263769, 0.20736231],\n",
       "       [0.80662973, 0.19337027],\n",
       "       [0.792603  , 0.207397  ],\n",
       "       [0.74135619, 0.25864381],\n",
       "       [0.36229859, 0.63770141],\n",
       "       [0.80868678, 0.19131322],\n",
       "       [0.89000808, 0.10999192],\n",
       "       [0.66593533, 0.33406467],\n",
       "       [0.70152637, 0.29847363],\n",
       "       [0.21840322, 0.78159678],\n",
       "       [0.23703849, 0.76296151],\n",
       "       [0.65863389, 0.34136611],\n",
       "       [0.82424224, 0.17575776],\n",
       "       [0.8447623 , 0.1552377 ],\n",
       "       [0.87593086, 0.12406914],\n",
       "       [0.73872562, 0.26127438],\n",
       "       [0.96106085, 0.03893915],\n",
       "       [0.33823948, 0.66176052],\n",
       "       [0.4836946 , 0.5163054 ],\n",
       "       [0.49190302, 0.50809698],\n",
       "       [0.57054282, 0.42945718],\n",
       "       [0.83009928, 0.16990072],\n",
       "       [0.34918154, 0.65081846],\n",
       "       [0.87227337, 0.12772663],\n",
       "       [0.40054681, 0.59945319],\n",
       "       [0.91108084, 0.08891916],\n",
       "       [0.33428836, 0.66571164],\n",
       "       [0.51151037, 0.48848963],\n",
       "       [0.42247255, 0.57752745],\n",
       "       [0.75753084, 0.24246916],\n",
       "       [0.66745992, 0.33254008],\n",
       "       [0.35273734, 0.64726266],\n",
       "       [0.86137859, 0.13862141],\n",
       "       [0.58600869, 0.41399131],\n",
       "       [0.83603647, 0.16396353],\n",
       "       [0.62118963, 0.37881037],\n",
       "       [0.87994315, 0.12005685],\n",
       "       [0.3618694 , 0.6381306 ],\n",
       "       [0.76813765, 0.23186235],\n",
       "       [0.65481332, 0.34518668],\n",
       "       [0.3123311 , 0.6876689 ],\n",
       "       [0.71869904, 0.28130096],\n",
       "       [0.87311555, 0.12688445],\n",
       "       [0.31173136, 0.68826864],\n",
       "       [0.86502767, 0.13497233],\n",
       "       [0.73140187, 0.26859813],\n",
       "       [0.72009384, 0.27990616],\n",
       "       [0.87639355, 0.12360645],\n",
       "       [0.63414104, 0.36585896],\n",
       "       [0.59584299, 0.40415701],\n",
       "       [0.80605857, 0.19394143],\n",
       "       [0.23141328, 0.76858672],\n",
       "       [0.05587756, 0.94412244],\n",
       "       [0.41511906, 0.58488094],\n",
       "       [0.30644809, 0.69355191],\n",
       "       [0.21413728, 0.78586272],\n",
       "       [0.8308142 , 0.1691858 ],\n",
       "       [0.55937392, 0.44062608],\n",
       "       [0.28295448, 0.71704552],\n",
       "       [0.82037753, 0.17962247],\n",
       "       [0.76814193, 0.23185807],\n",
       "       [0.26534045, 0.73465955],\n",
       "       [0.30652177, 0.69347823],\n",
       "       [0.95577098, 0.04422902],\n",
       "       [0.83921527, 0.16078473],\n",
       "       [0.89886068, 0.10113932],\n",
       "       [0.71770688, 0.28229312],\n",
       "       [0.52881322, 0.47118678],\n",
       "       [0.81016577, 0.18983423],\n",
       "       [0.72022841, 0.27977159],\n",
       "       [0.8377863 , 0.1622137 ],\n",
       "       [0.93418594, 0.06581406],\n",
       "       [0.63432098, 0.36567902],\n",
       "       [0.32169337, 0.67830663],\n",
       "       [0.84860327, 0.15139673],\n",
       "       [0.56155784, 0.43844216],\n",
       "       [0.65789861, 0.34210139],\n",
       "       [0.78659969, 0.21340031],\n",
       "       [0.94857748, 0.05142252],\n",
       "       [0.61432133, 0.38567867],\n",
       "       [0.60072549, 0.39927451],\n",
       "       [0.37344739, 0.62655261],\n",
       "       [0.34716855, 0.65283145],\n",
       "       [0.77957393, 0.22042607],\n",
       "       [0.34854991, 0.65145009],\n",
       "       [0.355655  , 0.644345  ],\n",
       "       [0.75370838, 0.24629162],\n",
       "       [0.94154094, 0.05845906],\n",
       "       [0.82212215, 0.17787785],\n",
       "       [0.22295976, 0.77704024],\n",
       "       [0.90286346, 0.09713654],\n",
       "       [0.66982061, 0.33017939],\n",
       "       [0.2645581 , 0.7354419 ],\n",
       "       [0.4524179 , 0.5475821 ],\n",
       "       [0.38424078, 0.61575922],\n",
       "       [0.77710656, 0.22289344],\n",
       "       [0.6115703 , 0.3884297 ],\n",
       "       [0.44668832, 0.55331168],\n",
       "       [0.41636869, 0.58363131],\n",
       "       [0.8751148 , 0.1248852 ],\n",
       "       [0.64149236, 0.35850764],\n",
       "       [0.70124241, 0.29875759],\n",
       "       [0.72884359, 0.27115641],\n",
       "       [0.70741159, 0.29258841],\n",
       "       [0.46774036, 0.53225964],\n",
       "       [0.49724672, 0.50275328],\n",
       "       [0.61483487, 0.38516513],\n",
       "       [0.41620287, 0.58379713],\n",
       "       [0.4880414 , 0.5119586 ],\n",
       "       [0.85794285, 0.14205715],\n",
       "       [0.91251551, 0.08748449],\n",
       "       [0.82532854, 0.17467146],\n",
       "       [0.22761469, 0.77238531],\n",
       "       [0.61105818, 0.38894182],\n",
       "       [0.86378112, 0.13621888],\n",
       "       [0.84653319, 0.15346681],\n",
       "       [0.11760536, 0.88239464],\n",
       "       [0.70706585, 0.29293415],\n",
       "       [0.89390923, 0.10609077],\n",
       "       [0.89654326, 0.10345674],\n",
       "       [0.97803897, 0.02196103],\n",
       "       [0.8519599 , 0.1480401 ],\n",
       "       [0.7300802 , 0.2699198 ],\n",
       "       [0.34945486, 0.65054514],\n",
       "       [0.78982075, 0.21017925],\n",
       "       [0.84654467, 0.15345533],\n",
       "       [0.67248127, 0.32751873],\n",
       "       [0.66469697, 0.33530303],\n",
       "       [0.42145213, 0.57854787],\n",
       "       [0.85947974, 0.14052026],\n",
       "       [0.83405988, 0.16594012],\n",
       "       [0.75891669, 0.24108331],\n",
       "       [0.162219  , 0.837781  ],\n",
       "       [0.43011192, 0.56988808],\n",
       "       [0.68679769, 0.31320231],\n",
       "       [0.76290578, 0.23709422],\n",
       "       [0.77553661, 0.22446339],\n",
       "       [0.82017603, 0.17982397],\n",
       "       [0.37524852, 0.62475148],\n",
       "       [0.8234044 , 0.1765956 ],\n",
       "       [0.31817417, 0.68182583],\n",
       "       [0.66610631, 0.33389369],\n",
       "       [0.64767068, 0.35232932],\n",
       "       [0.18474576, 0.81525424],\n",
       "       [0.42583307, 0.57416693],\n",
       "       [0.84230054, 0.15769946],\n",
       "       [0.84316928, 0.15683072],\n",
       "       [0.78835759, 0.21164241],\n",
       "       [0.84750439, 0.15249561],\n",
       "       [0.2344386 , 0.7655614 ],\n",
       "       [0.50276359, 0.49723641],\n",
       "       [0.69523397, 0.30476603],\n",
       "       [0.70601447, 0.29398553],\n",
       "       [0.74737838, 0.25262162],\n",
       "       [0.85880589, 0.14119411]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs #çıktıdaki 0. sütun 0 sınıfına ait olasılık\n",
    "        #          1. sütun 1 sınıfına ait olasılık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = loj_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_t = pd.DataFrame({\"gercek_y\": y_test,\n",
    "                   \"tahmin_y\": y_pred}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gercek_y</th>\n",
       "      <th>tahmin_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gercek_y  tahmin_y\n",
       "668         0         0\n",
       "324         0         0\n",
       "624         0         0\n",
       "690         0         0\n",
       "473         0         0\n",
       "..        ...       ...\n",
       "619         1         0\n",
       "198         1         0\n",
       "538         0         0\n",
       "329         0         0\n",
       "302         0         0\n",
       "\n",
       "[231 rows x 2 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151\n",
       "1     80\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts() #test ayrımı yaptığımızda hangi sınıftan kaç adte almışız görelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126,  25],\n",
       "       [ 32,  48]], dtype=int64)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred) # sınıflara ait örneklerin sınıflandırılma sonuçlarını verir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532467532467533"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82       151\n",
      "           1       0.66      0.60      0.63        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.72      0.72       231\n",
      "weighted avg       0.75      0.75      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kesinlik (Precision)--> 0 sınıfı için; 126 / (126+32) (0. sınıf olarak tahminlenenlerden gerçekten                                                        kaç adetinin 0 olduğunu gösterir)\n",
    "                        1 sınıfı için; 48 / (48+25)   (1. sınıf olarak tahminlenenlerden gerçekten                                                        kaç adetinin 1 olduğunu gösterir)\n",
    "                        \n",
    "Recall (Duyarlılık)--> 0 sınıfı için; 126 / (126 + 25) (0. sınıfa ait olanların ne kadarının doğru                                                         sınıflandırıldığını verir. %83'ü doğru                                                             sınıflandırılmış)\n",
    "                       1 sınıfı için; 48 / (48 + 32)  (1. sınıfa ait olanların ne kadarının doğru                                                         sınıflandırıldığını verir. %60'ı doğru                                                             sınıflandırılmış)\n",
    "                        \n",
    "accuracy_score-->Sınıflandırma doğruluğunu verir. Toplam verilerin ne kadarı doğru sınıfandırılmış.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb_model = nb.fit(X_train, y_train)\n",
    "nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.predict(X_test)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73815858, 0.26184142],\n",
       "       [0.94027894, 0.05972106],\n",
       "       [0.97242831, 0.02757169],\n",
       "       [0.82840069, 0.17159931],\n",
       "       [0.47153473, 0.52846527],\n",
       "       [0.47274458, 0.52725542],\n",
       "       [0.99607705, 0.00392295],\n",
       "       [0.69925055, 0.30074945],\n",
       "       [0.53838117, 0.46161883],\n",
       "       [0.25004536, 0.74995464]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.predict_proba(X_test)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7445887445887446"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       151\n",
      "           1       0.62      0.66      0.64        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.73      0.72       231\n",
      "weighted avg       0.75      0.74      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn_model = knn.fit(X_train, y_train)\n",
    "knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6883116883116883"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       151\n",
      "           1       0.55      0.56      0.56        80\n",
      "\n",
      "    accuracy                           0.69       231\n",
      "   macro avg       0.66      0.66      0.66       231\n",
      "weighted avg       0.69      0.69      0.69       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'minkowski',\n",
       " 'metric_params': None,\n",
       " 'n_jobs': None,\n",
       " 'n_neighbors': 5,\n",
       " 'p': 2,\n",
       " 'weights': 'uniform'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\"n_neighbors\": np.arange(1,50)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, knn_params, cv=10).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'n_neighbors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(\"En iyi parametreler: \" + str(knn_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(11)\n",
    "knn_tuned = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7316017316017316"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_tuned.score(X_test, y_test)#bu şekilde de accuracy metriğine ulaşabiliriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7316017316017316"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80       151\n",
      "           1       0.62      0.57      0.60        80\n",
      "\n",
      "    accuracy                           0.73       231\n",
      "   macro avg       0.70      0.69      0.70       231\n",
      "weighted avg       0.73      0.73      0.73       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel = \"linear\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7445887445887446"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       151\n",
      "           1       0.63      0.62      0.63        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.72      0.72       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'linear',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "svc_params = {\"C\": np.arange(1,10)}#bir sözlük yapısı üzerinden C parametresi için değerler oluşturulan\n",
    "\n",
    "svc = SVC(kernel = \"linear\")\n",
    "\n",
    "svc_cv_model = GridSearchCV(svc,svc_params, \n",
    "                            cv = 10, \n",
    "                            n_jobs = -1, \n",
    "                            verbose = 2 ).fit(X_train, y_train)\n",
    "\n",
    "#n_jobs;işlemciyi tam performans kullanabilmek için kullanılan bir argüman\n",
    "#verbosa; çıktıları gözlemleyebilmek için kullanılan bir argümandır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'C': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"En iyi parametreler: \" + str(svc_cv_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tuned = SVC(kernel = \"linear\", C = 5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7445887445887446"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svc_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       151\n",
      "           1       0.63      0.62      0.63        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.72      0.72       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF SVC\n",
    "RBF_Radial Basis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(kernel = \"rbf\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7359307359307359"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svc_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       151\n",
      "           1       0.66      0.49      0.56        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.71      0.68      0.69       231\n",
      "weighted avg       0.73      0.74      0.72       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n",
    "             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\n",
    "#gamma; kullanmış olduğumuz kernel ile ilgili bir parametre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 64 candidates, totalling 640 fits\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc_cv_model = GridSearchCV(svc, svc_params, \n",
    "                         cv = 10, \n",
    "                         n_jobs = -1,\n",
    "                         verbose = 2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'C': 10, 'gamma': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print(\"En iyi parametreler: \" + str(svc_cv_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tuned = SVC(kernel=\"rbf\", C = 10, gamma = 0.0001).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7359307359307359"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svc_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80       151\n",
      "           1       0.62      0.62      0.62        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.71      0.71      0.71       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yapay Sinir Ağları"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Yapay sinir ağları değişkenlerin ölçekleri ve birbirlerinden farklılıklarından etkilenir, bu nedenle regresyonda olduğu gibi yine standartlaştırma işlemi yapıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc = MLPClassifier().fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7316017316017316"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlpc.predict(X_test_scaled)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79       151\n",
      "           1       0.61      0.61      0.61        80\n",
      "\n",
      "    accuracy                           0.73       231\n",
      "   macro avg       0.70      0.70      0.70       231\n",
      "weighted avg       0.73      0.73      0.73       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        MLPClassifier\n",
       "\u001b[1;31mString form:\u001b[0m MLPClassifier()\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Multi-layer Perceptron classifier.\n",
       "\n",
       "This model optimizes the log-loss function using LBFGS or stochastic\n",
       "gradient descent.\n",
       "\n",
       ".. versionadded:: 0.18\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "hidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)\n",
       "    The ith element represents the number of neurons in the ith\n",
       "    hidden layer.\n",
       "\n",
       "activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
       "    Activation function for the hidden layer.\n",
       "\n",
       "    - 'identity', no-op activation, useful to implement linear bottleneck,\n",
       "      returns f(x) = x\n",
       "\n",
       "    - 'logistic', the logistic sigmoid function,\n",
       "      returns f(x) = 1 / (1 + exp(-x)).\n",
       "\n",
       "    - 'tanh', the hyperbolic tan function,\n",
       "      returns f(x) = tanh(x).\n",
       "\n",
       "    - 'relu', the rectified linear unit function,\n",
       "      returns f(x) = max(0, x)\n",
       "\n",
       "solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
       "    The solver for weight optimization.\n",
       "\n",
       "    - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
       "\n",
       "    - 'sgd' refers to stochastic gradient descent.\n",
       "\n",
       "    - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
       "      by Kingma, Diederik, and Jimmy Ba\n",
       "\n",
       "    Note: The default solver 'adam' works pretty well on relatively\n",
       "    large datasets (with thousands of training samples or more) in terms of\n",
       "    both training time and validation score.\n",
       "    For small datasets, however, 'lbfgs' can converge faster and perform\n",
       "    better.\n",
       "\n",
       "alpha : float, default=0.0001\n",
       "    Strength of the L2 regularization term. The L2 regularization term\n",
       "    is divided by the sample size when added to the loss.\n",
       "\n",
       "batch_size : int, default='auto'\n",
       "    Size of minibatches for stochastic optimizers.\n",
       "    If the solver is 'lbfgs', the classifier will not use minibatch.\n",
       "    When set to \"auto\", `batch_size=min(200, n_samples)`.\n",
       "\n",
       "learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
       "    Learning rate schedule for weight updates.\n",
       "\n",
       "    - 'constant' is a constant learning rate given by\n",
       "      'learning_rate_init'.\n",
       "\n",
       "    - 'invscaling' gradually decreases the learning rate at each\n",
       "      time step 't' using an inverse scaling exponent of 'power_t'.\n",
       "      effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
       "\n",
       "    - 'adaptive' keeps the learning rate constant to\n",
       "      'learning_rate_init' as long as training loss keeps decreasing.\n",
       "      Each time two consecutive epochs fail to decrease training loss by at\n",
       "      least tol, or fail to increase validation score by at least tol if\n",
       "      'early_stopping' is on, the current learning rate is divided by 5.\n",
       "\n",
       "    Only used when ``solver='sgd'``.\n",
       "\n",
       "learning_rate_init : float, default=0.001\n",
       "    The initial learning rate used. It controls the step-size\n",
       "    in updating the weights. Only used when solver='sgd' or 'adam'.\n",
       "\n",
       "power_t : float, default=0.5\n",
       "    The exponent for inverse scaling learning rate.\n",
       "    It is used in updating effective learning rate when the learning_rate\n",
       "    is set to 'invscaling'. Only used when solver='sgd'.\n",
       "\n",
       "max_iter : int, default=200\n",
       "    Maximum number of iterations. The solver iterates until convergence\n",
       "    (determined by 'tol') or this number of iterations. For stochastic\n",
       "    solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
       "    (how many times each data point will be used), not the number of\n",
       "    gradient steps.\n",
       "\n",
       "shuffle : bool, default=True\n",
       "    Whether to shuffle samples in each iteration. Only used when\n",
       "    solver='sgd' or 'adam'.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Determines random number generation for weights and bias\n",
       "    initialization, train-test split if early stopping is used, and batch\n",
       "    sampling when solver='sgd' or 'adam'.\n",
       "    Pass an int for reproducible results across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for the optimization. When the loss or score is not improving\n",
       "    by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
       "    unless ``learning_rate`` is set to 'adaptive', convergence is\n",
       "    considered to be reached and training stops.\n",
       "\n",
       "verbose : bool, default=False\n",
       "    Whether to print progress messages to stdout.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous\n",
       "    call to fit as initialization, otherwise, just erase the\n",
       "    previous solution. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "momentum : float, default=0.9\n",
       "    Momentum for gradient descent update. Should be between 0 and 1. Only\n",
       "    used when solver='sgd'.\n",
       "\n",
       "nesterovs_momentum : bool, default=True\n",
       "    Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
       "    momentum > 0.\n",
       "\n",
       "early_stopping : bool, default=False\n",
       "    Whether to use early stopping to terminate training when validation\n",
       "    score is not improving. If set to true, it will automatically set\n",
       "    aside 10% of training data as validation and terminate training when\n",
       "    validation score is not improving by at least tol for\n",
       "    ``n_iter_no_change`` consecutive epochs. The split is stratified,\n",
       "    except in a multilabel setting.\n",
       "    If early stopping is False, then the training stops when the training\n",
       "    loss does not improve by more than tol for n_iter_no_change consecutive\n",
       "    passes over the training set.\n",
       "    Only effective when solver='sgd' or 'adam'.\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Must be between 0 and 1.\n",
       "    Only used if early_stopping is True.\n",
       "\n",
       "beta_1 : float, default=0.9\n",
       "    Exponential decay rate for estimates of first moment vector in adam,\n",
       "    should be in [0, 1). Only used when solver='adam'.\n",
       "\n",
       "beta_2 : float, default=0.999\n",
       "    Exponential decay rate for estimates of second moment vector in adam,\n",
       "    should be in [0, 1). Only used when solver='adam'.\n",
       "\n",
       "epsilon : float, default=1e-8\n",
       "    Value for numerical stability in adam. Only used when solver='adam'.\n",
       "\n",
       "n_iter_no_change : int, default=10\n",
       "    Maximum number of epochs to not meet ``tol`` improvement.\n",
       "    Only effective when solver='sgd' or 'adam'.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "max_fun : int, default=15000\n",
       "    Only used when solver='lbfgs'. Maximum number of loss function calls.\n",
       "    The solver iterates until convergence (determined by 'tol'), number\n",
       "    of iterations reaches max_iter, or this number of loss function calls.\n",
       "    Note that number of loss function calls will be greater than or equal\n",
       "    to the number of iterations for the `MLPClassifier`.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : ndarray or list of ndarray of shape (n_classes,)\n",
       "    Class labels for each output.\n",
       "\n",
       "loss_ : float\n",
       "    The current loss computed with the loss function.\n",
       "\n",
       "best_loss_ : float or None\n",
       "    The minimum loss reached by the solver throughout fitting.\n",
       "    If `early_stopping=True`, this attribute is set to `None`. Refer to\n",
       "    the `best_validation_score_` fitted attribute instead.\n",
       "\n",
       "loss_curve_ : list of shape (`n_iter_`,)\n",
       "    The ith element in the list represents the loss at the ith iteration.\n",
       "\n",
       "validation_scores_ : list of shape (`n_iter_`,) or None\n",
       "    The score at each iteration on a held-out validation set. The score\n",
       "    reported is the accuracy score. Only available if `early_stopping=True`,\n",
       "    otherwise the attribute is set to `None`.\n",
       "\n",
       "best_validation_score_ : float or None\n",
       "    The best validation score (i.e. accuracy score) that triggered the\n",
       "    early stopping. Only available if `early_stopping=True`, otherwise the\n",
       "    attribute is set to `None`.\n",
       "\n",
       "t_ : int\n",
       "    The number of training samples seen by the solver during fitting.\n",
       "\n",
       "coefs_ : list of shape (n_layers - 1,)\n",
       "    The ith element in the list represents the weight matrix corresponding\n",
       "    to layer i.\n",
       "\n",
       "intercepts_ : list of shape (n_layers - 1,)\n",
       "    The ith element in the list represents the bias vector corresponding to\n",
       "    layer i + 1.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : int\n",
       "    The number of iterations the solver has run.\n",
       "\n",
       "n_layers_ : int\n",
       "    Number of layers.\n",
       "\n",
       "n_outputs_ : int\n",
       "    Number of outputs.\n",
       "\n",
       "out_activation_ : str\n",
       "    Name of the output activation function.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "MLPRegressor : Multi-layer Perceptron regressor.\n",
       "BernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).\n",
       "\n",
       "Notes\n",
       "-----\n",
       "MLPClassifier trains iteratively since at each time step\n",
       "the partial derivatives of the loss function with respect to the model\n",
       "parameters are computed to update the parameters.\n",
       "\n",
       "It can also have a regularization term added to the loss function\n",
       "that shrinks model parameters to prevent overfitting.\n",
       "\n",
       "This implementation works with data represented as dense numpy arrays or\n",
       "sparse scipy arrays of floating point values.\n",
       "\n",
       "References\n",
       "----------\n",
       "Hinton, Geoffrey E. \"Connectionist learning procedures.\"\n",
       "Artificial intelligence 40.1 (1989): 185-234.\n",
       "\n",
       "Glorot, Xavier, and Yoshua Bengio.\n",
       "\"Understanding the difficulty of training deep feedforward neural networks.\"\n",
       "International Conference on Artificial Intelligence and Statistics. 2010.\n",
       "\n",
       ":arxiv:`He, Kaiming, et al (2015). \"Delving deep into rectifiers:\n",
       "Surpassing human-level performance on imagenet classification.\" <1502.01852>`\n",
       "\n",
       ":arxiv:`Kingma, Diederik, and Jimmy Ba (2014)\n",
       "\"Adam: A method for stochastic optimization.\" <1412.6980>`\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.neural_network import MLPClassifier\n",
       ">>> from sklearn.datasets import make_classification\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = make_classification(n_samples=100, random_state=1)\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
       "...                                                     random_state=1)\n",
       ">>> clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
       ">>> clf.predict_proba(X_test[:1])\n",
       "array([[0.038..., 0.961...]])\n",
       ">>> clf.predict(X_test[:5, :])\n",
       "array([1, 0, 1, 0, 1])\n",
       ">>> clf.score(X_test, y_test)\n",
       "0.8..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?mlpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n",
    "              \"hidden_layer_sizes\": [(10,10,10),\n",
    "                                     (100,100,100),\n",
    "                                     (100,100),\n",
    "                                     (3,5), \n",
    "                                     (5, 3)],\n",
    "              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n",
    "              \"activation\": [\"relu\",\"logistic\"]}\n",
    "\n",
    "#solver: ağırlık optimizasyon yöntemi. Büyük veri setlerinde daha çok kabul edilen ağırlık optimizasyon yöntemidir. Bizim elimizdeki veri çok karmaşık ve büyük değilse belki farklı bir ağırlık optimizasyon yöntemi bizim için daha iyidir.\n",
    "#hidden_layer_sizes (gizli katman) kaç katmandan ve nörondan oluşacak bilgisini verir. Her katman için nöron sayısını girerek kaç katmnadan oluşacağını da bildirmiş oluruz.\n",
    "#activation fonksiyonu ön tanımlı değeri relu dur biz bir de logistic (sigmoid) fonksiyonunu deneyelim\n",
    "#alpha; düzenleştirme parametresidir. Ön tanımlı değeri 0.0001 miş"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n"
     ]
    }
   ],
   "source": [
    "mlpc = MLPClassifier()\n",
    "mlpc_cv_model = GridSearchCV(mlpc, mlpc_params, \n",
    "                         cv = 10, \n",
    "                         n_jobs = -1,\n",
    "                         verbose = 2).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 100), 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "print(\"En iyi parametreler: \" + str(mlpc_cv_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_tuned = MLPClassifier(activation = \"relu\", \n",
    "                           alpha = 0.01, \n",
    "                           hidden_layer_sizes = (100, 100, 100),\n",
    "                          solver = \"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, hidden_layer_sizes=(100, 100, 100), solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, hidden_layer_sizes=(100, 100, 100), solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.01, hidden_layer_sizes=(100, 100, 100), solver='sgd')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc_tuned.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489177489177489"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mlpc_tuned.predict(X_test_scaled)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81       151\n",
      "           1       0.65      0.60      0.62        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.72      0.71      0.72       231\n",
      "weighted avg       0.75      0.75      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = DecisionTreeClassifier()\n",
    "cart_model = cart.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7186147186147186"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cart_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77       151\n",
      "           1       0.58      0.70      0.63        80\n",
      "\n",
      "    accuracy                           0.72       231\n",
      "   macro avg       0.70      0.71      0.70       231\n",
      "weighted avg       0.74      0.72      0.72       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'random_state': None,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        DecisionTreeClassifier\n",
       "\u001b[1;31mString form:\u001b[0m DecisionTreeClassifier()\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "A decision tree classifier.\n",
       "\n",
       "Read more in the :ref:`User Guide <tree>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
       "    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
       "\n",
       "splitter : {\"best\", \"random\"}, default=\"best\"\n",
       "    The strategy used to choose the split at each node. Supported\n",
       "    strategies are \"best\" to choose the best split and \"random\" to choose\n",
       "    the best random split.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "        - If int, then consider `max_features` features at each split.\n",
       "        - If float, then `max_features` is a fraction and\n",
       "          `max(1, int(max_features * n_features_in_))` features are considered at\n",
       "          each split.\n",
       "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "        - If \"log2\", then `max_features=log2(n_features)`.\n",
       "        - If None, then `max_features=n_features`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the randomness of the estimator. The features are always\n",
       "    randomly permuted at each split, even if ``splitter`` is set to\n",
       "    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
       "    select ``max_features`` at random at each split before finding the best\n",
       "    split among them. But the best found split may vary across different\n",
       "    runs, even if ``max_features=n_features``. That is the case, if the\n",
       "    improvement of the criterion is identical for several splits and one\n",
       "    split has to be selected at random. To obtain a deterministic behaviour\n",
       "    during fitting, ``random_state`` has to be fixed to an integer.\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "class_weight : dict, list of dict or \"balanced\", default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If None, all classes are supposed to have weight one. For\n",
       "    multi-output problems, a list of dicts can be provided in the same\n",
       "    order as the columns of y.\n",
       "\n",
       "    Note that for multioutput (including multilabel) weights should be\n",
       "    defined for each class of every column in its own dict. For example,\n",
       "    for four-class multilabel classification weights should be\n",
       "    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
       "    [{1:1}, {2:5}, {3:1}, {4:1}].\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``\n",
       "\n",
       "    For multi-output, the weights of each column of y will be multiplied.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
       "    The classes labels (single output problem),\n",
       "    or a list of arrays of class labels (multi-output problem).\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance [4]_.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "n_classes_ : int or list of int\n",
       "    The number of classes (for single output problems),\n",
       "    or a list containing the number of classes for each\n",
       "    output (for multi-output problems).\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "tree_ : Tree instance\n",
       "    The underlying Tree object. Please refer to\n",
       "    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
       "    for basic usage of these attributes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DecisionTreeRegressor : A decision tree regressor.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
       "function on the outputs of :meth:`predict_proba`. This means that in\n",
       "case the highest predicted probabilities are tied, the classifier will\n",
       "predict the tied class with the lowest index in :term:`classes_`.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       ".. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
       "\n",
       ".. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
       "       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
       "\n",
       ".. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
       "       Learning\", Springer, 2009.\n",
       "\n",
       ".. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
       "       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.model_selection import cross_val_score\n",
       ">>> from sklearn.tree import DecisionTreeClassifier\n",
       ">>> clf = DecisionTreeClassifier(random_state=0)\n",
       ">>> iris = load_iris()\n",
       ">>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
       "...                             # doctest: +SKIP\n",
       "...\n",
       "array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
       "        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?cart_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_grid = {\"max_depth\": range(1,10),\n",
    "            \"min_samples_split\" : list(range(2,50)) }\n",
    "\n",
    "#maxdepth: ağacın derinliği bilgisi, ağacın karmaşıklığını dallanmaları kontrol etmek için kullanılan bir parametre\n",
    "#Ağacın maksimum derinliği. \n",
    "#Hiçbiri yoksa, düğümler tüm yapraklar saf olana kadar veya tüm yapraklar min_samples_split'ten daha az örnek içerene kadar genişletilir.\n",
    "#dallanma dolayısıyla overfitting önüne geçmek için kullanılır\n",
    "\n",
    "#min_samples_split; Bölünme için gerekli olan minimum örnek sayısı. Bir bölünme gerçekleşti, tekrar bölünme gerçekleşip gerçekleşmeyeceğini belirlemek için kullanılan bir argüman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 432 candidates, totalling 4320 fits\n"
     ]
    }
   ],
   "source": [
    "cart = DecisionTreeClassifier()\n",
    "cart_cv_model = GridSearchCV(cart, cart_grid, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'max_depth': 5, 'min_samples_split': 19}\n"
     ]
    }
   ],
   "source": [
    "print(\"En iyi parametreler: \" + str(cart_cv_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = DecisionTreeClassifier(max_depth = 5, min_samples_split = 19)\n",
    "cart_tuned = cart.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cart_tuned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m cart_tuned\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      2\u001b[0m accuracy_score(y_test, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cart_tuned' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = cart_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       151\n",
      "           1       0.67      0.56      0.61        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.71      0.72       231\n",
      "weighted avg       0.75      0.75      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532467532467533"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       151\n",
      "           1       0.65      0.64      0.64        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.75      0.75      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        RandomForestClassifier\n",
       "\u001b[1;31mString form:\u001b[0m RandomForestClassifier()\n",
       "\u001b[1;31mLength:\u001b[0m      100\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "A random forest classifier.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of decision tree\n",
       "classifiers on various sub-samples of the dataset and uses averaging to\n",
       "improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "For a comparison between tree-based ensemble models see the example\n",
       ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_estimators : int, default=100\n",
       "    The number of trees in the forest.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``n_estimators`` changed from 10 to 100\n",
       "       in 0.22.\n",
       "\n",
       "criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
       "    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
       "    Note: This parameter is tree-specific.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
       "      split.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    .. versionchanged:: 1.1\n",
       "        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "bootstrap : bool, default=True\n",
       "    Whether bootstrap samples are used when building trees. If False, the\n",
       "    whole dataset is used to build each tree.\n",
       "\n",
       "oob_score : bool or callable, default=False\n",
       "    Whether to use out-of-bag samples to estimate the generalization score.\n",
       "    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
       "    Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
       "    custom metric. Only available if `bootstrap=True`.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
       "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
       "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors. See :term:`Glossary\n",
       "    <n_jobs>` for more details.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls both the randomness of the bootstrapping of the samples used\n",
       "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
       "    features to consider when looking for the best split at each node\n",
       "    (if ``max_features < n_features``).\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
       "    new forest. See :term:`Glossary <warm_start>` and\n",
       "    :ref:`gradient_boosting_warm_start` for details.\n",
       "\n",
       "class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one. For\n",
       "    multi-output problems, a list of dicts can be provided in the same\n",
       "    order as the columns of y.\n",
       "\n",
       "    Note that for multioutput (including multilabel) weights should be\n",
       "    defined for each class of every column in its own dict. For example,\n",
       "    for four-class multilabel classification weights should be\n",
       "    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
       "    [{1:1}, {2:5}, {3:1}, {4:1}].\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``\n",
       "\n",
       "    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
       "    weights are computed based on the bootstrap sample for every tree\n",
       "    grown.\n",
       "\n",
       "    For multi-output, the weights of each column of y will be multiplied.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "max_samples : int or float, default=None\n",
       "    If bootstrap is True, the number of samples to draw from X\n",
       "    to train each base estimator.\n",
       "\n",
       "    - If None (default), then draw `X.shape[0]` samples.\n",
       "    - If int, then draw `max_samples` samples.\n",
       "    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
       "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
       "    The child estimator template used to create the collection of fitted\n",
       "    sub-estimators.\n",
       "\n",
       "    .. versionadded:: 1.2\n",
       "       `base_estimator_` was renamed to `estimator_`.\n",
       "\n",
       "base_estimator_ : DecisionTreeClassifier\n",
       "    The child estimator template used to create the collection of fitted\n",
       "    sub-estimators.\n",
       "\n",
       "    .. deprecated:: 1.2\n",
       "        `base_estimator_` is deprecated and will be removed in 1.4.\n",
       "        Use `estimator_` instead.\n",
       "\n",
       "estimators_ : list of DecisionTreeClassifier\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
       "    The classes labels (single output problem), or a list of arrays of\n",
       "    class labels (multi-output problem).\n",
       "\n",
       "n_classes_ : int or list\n",
       "    The number of classes (single output problem), or a list containing the\n",
       "    number of classes for each output (multi-output problem).\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
       "    Decision function computed with out-of-bag estimate on the training\n",
       "    set. If n_estimators is small it might be possible that a data point\n",
       "    was never left out during the bootstrap. In this case,\n",
       "    `oob_decision_function_` might contain NaN. This attribute exists\n",
       "    only when ``oob_score`` is True.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
       "sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
       "    tree classifiers.\n",
       "sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n",
       "    Boosting Classification Tree, very fast for big datasets (n_samples >=\n",
       "    10_000).\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data,\n",
       "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
       "of the criterion is identical for several splits enumerated during the\n",
       "search of the best split. To obtain a deterministic behaviour during\n",
       "fitting, ``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.ensemble import RandomForestClassifier\n",
       ">>> from sklearn.datasets import make_classification\n",
       ">>> X, y = make_classification(n_samples=1000, n_features=4,\n",
       "...                            n_informative=2, n_redundant=0,\n",
       "...                            random_state=0, shuffle=False)\n",
       ">>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
       ">>> clf.fit(X, y)\n",
       "RandomForestClassifier(...)\n",
       ">>> print(clf.predict([[0, 0, 0, 0]]))\n",
       "[1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\"max_depth\": [2,5,8,10],\n",
    "            \"max_features\": [2,5,8],\n",
    "            \"n_estimators\": [10,500,1000],\n",
    "            \"min_samples_split\": [2,5,10]}\n",
    "\n",
    "#\"n_estimators\"; kullanılacak ağaç sayısı\n",
    "#max_features;  bölünmeler için maksimum kaç özellik gerekli bilgisidir\n",
    "#\"max_depth\", \"min_samples_split\" CART algoritmasında da bahsedilen parametreler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "rf_cv_model = GridSearchCV(rf_model, \n",
    "                           rf_params, \n",
    "                           cv = 10, \n",
    "                           n_jobs = -1, \n",
    "                           verbose = 2).fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'max_depth': 8, 'max_features': 8, 'min_samples_split': 2, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(\"En iyi parametreler: \" + str(rf_cv_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=8, max_features=8, n_estimators=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=8, max_features=8, n_estimators=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=8, max_features=8, n_estimators=10)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_tuned = RandomForestClassifier(max_depth = 8, \n",
    "                                  max_features = 8, \n",
    "                                  min_samples_split = 2,\n",
    "                                  n_estimators = 1000)\n",
    "\n",
    "rf_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619047619047619"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81       151\n",
      "           1       0.65      0.69      0.67        80\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.74      0.74       231\n",
      "weighted avg       0.77      0.76      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n",
    "                         index = X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Değişken Önem Düzeyleri')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Importance.sort_values(by = \"Importance\", \n",
    "                       axis = 0, \n",
    "                       ascending = True).plot(kind =\"barh\", color = \"r\")\n",
    "\n",
    "plt.xlabel(\"Değişken Önem Düzeyleri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model = GradientBoostingClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489177489177489"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gbm_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81       151\n",
      "           1       0.63      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.72      0.73      0.73       231\n",
      "weighted avg       0.75      0.75      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        GradientBoostingClassifier\n",
       "\u001b[1;31mString form:\u001b[0m GradientBoostingClassifier()\n",
       "\u001b[1;31mLength:\u001b[0m      100\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Gradient Boosting for classification.\n",
       "\n",
       "This algorithm builds an additive model in a forward stage-wise fashion; it\n",
       "allows for the optimization of arbitrary differentiable loss functions. In\n",
       "each stage ``n_classes_`` regression trees are fit on the negative gradient\n",
       "of the loss function, e.g. binary or multiclass log loss. Binary\n",
       "classification is a special case where only a single regression tree is\n",
       "induced.\n",
       "\n",
       ":class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster\n",
       "variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n",
       "\n",
       "Read more in the :ref:`User Guide <gradient_boosting>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "loss : {'log_loss', 'exponential'}, default='log_loss'\n",
       "    The loss function to be optimized. 'log_loss' refers to binomial and\n",
       "    multinomial deviance, the same as used in logistic regression.\n",
       "    It is a good choice for classification with probabilistic outputs.\n",
       "    For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\n",
       "\n",
       "learning_rate : float, default=0.1\n",
       "    Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
       "    There is a trade-off between learning_rate and n_estimators.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "n_estimators : int, default=100\n",
       "    The number of boosting stages to perform. Gradient boosting\n",
       "    is fairly robust to over-fitting so a large number usually\n",
       "    results in better performance.\n",
       "    Values must be in the range `[1, inf)`.\n",
       "\n",
       "subsample : float, default=1.0\n",
       "    The fraction of samples to be used for fitting the individual base\n",
       "    learners. If smaller than 1.0 this results in Stochastic Gradient\n",
       "    Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
       "    Choosing `subsample < 1.0` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "    Values must be in the range `(0.0, 1.0]`.\n",
       "\n",
       "criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    'friedman_mse' for the mean squared error with improvement score by\n",
       "    Friedman, 'squared_error' for mean squared error. The default value of\n",
       "    'friedman_mse' is generally the best as it can provide a better\n",
       "    approximation in some cases.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, values must be in the range `[2, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n",
       "      will be `ceil(min_samples_split * n_samples)`.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, values must be in the range `[1, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n",
       "      will be `ceil(min_samples_leaf * n_samples)`.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "    Values must be in the range `[0.0, 0.5]`.\n",
       "\n",
       "max_depth : int or None, default=3\n",
       "    Maximum depth of the individual regression estimators. The maximum\n",
       "    depth limits the number of nodes in the tree. Tune this parameter\n",
       "    for best performance; the best value depends on the interaction\n",
       "    of the input variables. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "    If int, values must be in the range `[1, inf)`.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "init : estimator or 'zero', default=None\n",
       "    An estimator object that is used to compute the initial predictions.\n",
       "    ``init`` has to provide :term:`fit` and :term:`predict_proba`. If\n",
       "    'zero', the initial raw predictions are set to zero. By default, a\n",
       "    ``DummyEstimator`` predicting the classes priors is used.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the random seed given to each Tree estimator at each\n",
       "    boosting iteration.\n",
       "    In addition, it controls the random permutation of the features at\n",
       "    each split (see Notes for more details).\n",
       "    It also controls the random splitting of the training data to obtain a\n",
       "    validation set if `n_iter_no_change` is not None.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "max_features : {'sqrt', 'log2'}, int or float, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, values must be in the range `[1, inf)`.\n",
       "    - If float, values must be in the range `(0.0, 1.0]` and the features\n",
       "      considered at each split will be `max(1, int(max_features * n_features_in_))`.\n",
       "    - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
       "    - If 'log2', then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Choosing `max_features < n_features` leads to a reduction of variance\n",
       "    and an increase in bias.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Enable verbose output. If 1 then it prints progress and performance\n",
       "    once in a while (the more trees the lower the frequency). If greater\n",
       "    than 1 then it prints progress and performance for every tree.\n",
       "    Values must be in the range `[0, inf)`.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    Values must be in the range `[2, inf)`.\n",
       "    If `None`, then unlimited number of leaf nodes.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just erase the\n",
       "    previous solution. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Values must be in the range `(0.0, 1.0)`.\n",
       "    Only used if ``n_iter_no_change`` is set to an integer.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "n_iter_no_change : int, default=None\n",
       "    ``n_iter_no_change`` is used to decide if early stopping will be used\n",
       "    to terminate training when validation score is not improving. By\n",
       "    default it is set to None to disable early stopping. If set to a\n",
       "    number, it will set aside ``validation_fraction`` size of the training\n",
       "    data as validation and terminate training when validation score is not\n",
       "    improving in all of the previous ``n_iter_no_change`` numbers of\n",
       "    iterations. The split is stratified.\n",
       "    Values must be in the range `[1, inf)`.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for the early stopping. When the loss is not improving\n",
       "    by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
       "    number), the training stops.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "    See :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "n_estimators_ : int\n",
       "    The number of estimators as selected by early stopping (if\n",
       "    ``n_iter_no_change`` is specified). Otherwise it is set to\n",
       "    ``n_estimators``.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_improvement_ : ndarray of shape (n_estimators,)\n",
       "    The improvement in loss on the out-of-bag samples\n",
       "    relative to the previous iteration.\n",
       "    ``oob_improvement_[0]`` is the improvement in\n",
       "    loss of the first stage over the ``init`` estimator.\n",
       "    Only available if ``subsample < 1.0``.\n",
       "\n",
       "oob_scores_ : ndarray of shape (n_estimators,)\n",
       "    The full history of the loss values on the out-of-bag\n",
       "    samples. Only available if `subsample < 1.0`.\n",
       "\n",
       "    .. versionadded:: 1.3\n",
       "\n",
       "oob_score_ : float\n",
       "    The last value of the loss on the out-of-bag samples. It is\n",
       "    the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n",
       "\n",
       "    .. versionadded:: 1.3\n",
       "\n",
       "train_score_ : ndarray of shape (n_estimators,)\n",
       "    The i-th score ``train_score_[i]`` is the loss of the\n",
       "    model at iteration ``i`` on the in-bag sample.\n",
       "    If ``subsample == 1`` this is the loss on the training data.\n",
       "\n",
       "init_ : estimator\n",
       "    The estimator that provides the initial predictions.\n",
       "    Set via the ``init`` argument or ``loss.init_estimator``.\n",
       "\n",
       "estimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``loss_.K``)\n",
       "    The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
       "    classification, otherwise n_classes.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    The classes labels.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_classes_ : int\n",
       "    The number of classes.\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n",
       "    Classification Tree.\n",
       "sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
       "RandomForestClassifier : A meta-estimator that fits a number of decision\n",
       "    tree classifiers on various sub-samples of the dataset and uses\n",
       "    averaging to improve the predictive accuracy and control over-fitting.\n",
       "AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
       "    on the original dataset and then fits additional copies of the\n",
       "    classifier on the same dataset where the weights of incorrectly\n",
       "    classified instances are adjusted such that subsequent classifiers\n",
       "    focus more on difficult cases.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data and\n",
       "``max_features=n_features``, if the improvement of the criterion is\n",
       "identical for several splits enumerated during the search of the best\n",
       "split. To obtain a deterministic behaviour during fitting,\n",
       "``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       "J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
       "Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
       "\n",
       "J. Friedman, Stochastic Gradient Boosting, 1999\n",
       "\n",
       "T. Hastie, R. Tibshirani and J. Friedman.\n",
       "Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "The following example shows how to fit a gradient boosting classifier with\n",
       "100 decision stumps as weak learners.\n",
       "\n",
       ">>> from sklearn.datasets import make_hastie_10_2\n",
       ">>> from sklearn.ensemble import GradientBoostingClassifier\n",
       "\n",
       ">>> X, y = make_hastie_10_2(random_state=0)\n",
       ">>> X_train, X_test = X[:2000], X[2000:]\n",
       ">>> y_train, y_test = y[:2000], y[2000:]\n",
       "\n",
       ">>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
       "...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
       ">>> clf.score(X_test, y_test)\n",
       "0.913..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?gbm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n",
    "             \"n_estimators\": [100,500,100],\n",
    "             \"max_depth\": [3,5,10],\n",
    "             \"min_samples_split\": [2,5,10]}\n",
    "\n",
    "#\"learning_rate\"; Öğrenme oranı, her ağacın katkısını \"öğrenme_oranı\" kadar azaltır.\n",
    "#Learning_rate ve n_estimators arasında bir denge vardır.\n",
    "#\"n_estimators\", \"max_depth\", \"min_samples_split\" daha önceki ağaç yapıalrındaki gibidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "gbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "print(\"En iyi parametreler: \" + str(gbm_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(learning_rate = 0.01, \n",
    "                                 max_depth = 5,\n",
    "                                min_samples_split = 2,\n",
    "                                n_estimators = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_tuned =  gbm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7359307359307359"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gbm_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.76      0.79       151\n",
      "           1       0.60      0.69      0.64        80\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.71      0.72      0.72       231\n",
      "weighted avg       0.75      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/bc/43/242432efc3f60052a4a534dc4926b21e236ab4ec8d4920c593da3f65c65d/xgboost-2.0.2-py3-none-win_amd64.whl.metadata\n",
      "  Downloading xgboost-2.0.2-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages (from xgboost) (1.10.1)\n",
      "Downloading xgboost-2.0.2-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB 660.6 kB/s eta 0:02:31\n",
      "   ---------------------------------------- 0.1/99.8 MB 653.6 kB/s eta 0:02:33\n",
      "   ---------------------------------------- 0.1/99.8 MB 901.1 kB/s eta 0:01:51\n",
      "   ---------------------------------------- 0.1/99.8 MB 950.9 kB/s eta 0:01:45\n",
      "   ---------------------------------------- 0.3/99.8 MB 1.1 MB/s eta 0:01:35\n",
      "   ---------------------------------------- 0.3/99.8 MB 1.1 MB/s eta 0:01:28\n",
      "   ---------------------------------------- 0.4/99.8 MB 1.1 MB/s eta 0:01:34\n",
      "   ---------------------------------------- 0.5/99.8 MB 1.2 MB/s eta 0:01:25\n",
      "   ---------------------------------------- 0.5/99.8 MB 1.2 MB/s eta 0:01:21\n",
      "   ---------------------------------------- 0.6/99.8 MB 1.3 MB/s eta 0:01:17\n",
      "   ---------------------------------------- 0.7/99.8 MB 1.3 MB/s eta 0:01:17\n",
      "   ---------------------------------------- 0.7/99.8 MB 1.3 MB/s eta 0:01:15\n",
      "   ---------------------------------------- 0.8/99.8 MB 1.3 MB/s eta 0:01:16\n",
      "   ---------------------------------------- 0.9/99.8 MB 1.4 MB/s eta 0:01:12\n",
      "   ---------------------------------------- 1.0/99.8 MB 1.4 MB/s eta 0:01:11\n",
      "   ---------------------------------------- 1.1/99.8 MB 1.4 MB/s eta 0:01:11\n",
      "   ---------------------------------------- 1.1/99.8 MB 1.4 MB/s eta 0:01:09\n",
      "   ---------------------------------------- 1.1/99.8 MB 1.4 MB/s eta 0:01:10\n",
      "   ---------------------------------------- 1.2/99.8 MB 1.4 MB/s eta 0:01:12\n",
      "    --------------------------------------- 1.3/99.8 MB 1.4 MB/s eta 0:01:10\n",
      "    --------------------------------------- 1.4/99.8 MB 1.4 MB/s eta 0:01:09\n",
      "    --------------------------------------- 1.5/99.8 MB 1.4 MB/s eta 0:01:09\n",
      "    --------------------------------------- 1.5/99.8 MB 1.4 MB/s eta 0:01:09\n",
      "    --------------------------------------- 1.5/99.8 MB 1.3 MB/s eta 0:01:14\n",
      "    --------------------------------------- 1.6/99.8 MB 1.4 MB/s eta 0:01:13\n",
      "    --------------------------------------- 1.7/99.8 MB 1.4 MB/s eta 0:01:12\n",
      "    --------------------------------------- 1.7/99.8 MB 1.4 MB/s eta 0:01:12\n",
      "    --------------------------------------- 1.8/99.8 MB 1.4 MB/s eta 0:01:13\n",
      "    --------------------------------------- 1.9/99.8 MB 1.4 MB/s eta 0:01:11\n",
      "    --------------------------------------- 2.0/99.8 MB 1.4 MB/s eta 0:01:10\n",
      "    --------------------------------------- 2.0/99.8 MB 1.4 MB/s eta 0:01:09\n",
      "    --------------------------------------- 2.1/99.8 MB 1.4 MB/s eta 0:01:09\n",
      "    --------------------------------------- 2.1/99.8 MB 1.4 MB/s eta 0:01:11\n",
      "    --------------------------------------- 2.2/99.8 MB 1.4 MB/s eta 0:01:09\n",
      "    --------------------------------------- 2.3/99.8 MB 1.5 MB/s eta 0:01:08\n",
      "    --------------------------------------- 2.4/99.8 MB 1.4 MB/s eta 0:01:08\n",
      "   - -------------------------------------- 2.6/99.8 MB 1.5 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 2.7/99.8 MB 1.5 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 2.8/99.8 MB 1.5 MB/s eta 0:01:04\n",
      "   - -------------------------------------- 2.9/99.8 MB 1.6 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 2.9/99.8 MB 1.6 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 3.0/99.8 MB 1.5 MB/s eta 0:01:04\n",
      "   - -------------------------------------- 3.1/99.8 MB 1.5 MB/s eta 0:01:03\n",
      "   - -------------------------------------- 3.2/99.8 MB 1.6 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 3.3/99.8 MB 1.6 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 3.4/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 3.5/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 3.5/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 3.6/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 3.7/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 3.8/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 3.9/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 4.0/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 4.1/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   - -------------------------------------- 4.2/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   - -------------------------------------- 4.2/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   - -------------------------------------- 4.2/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   - -------------------------------------- 4.3/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 4.4/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 4.4/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 4.5/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 4.6/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 4.6/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 4.7/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 4.7/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 4.7/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 4.8/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 4.9/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 4.9/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 5.0/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 5.1/99.8 MB 1.6 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 5.3/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 5.3/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 5.4/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 5.5/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 5.6/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 5.7/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 5.8/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 5.9/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 5.9/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 5.9/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 6.0/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 6.1/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.1/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.1/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.1/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.1/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.1/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.6/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.6/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.7/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.7/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 6.7/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 6.8/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 6.9/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 6.9/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 6.9/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 7.0/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 7.2/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 7.2/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 7.3/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 7.4/99.8 MB 1.6 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 7.4/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   -- ------------------------------------- 7.5/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 7.5/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 7.5/99.8 MB 1.6 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 7.6/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 7.6/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 7.7/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 7.8/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 7.9/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 7.9/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 8.0/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.1/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.2/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.2/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.3/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.4/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.4/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.4/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 8.5/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 8.6/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.7/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.7/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.7/99.8 MB 1.5 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 8.7/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 8.8/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 8.9/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 8.9/99.8 MB 1.5 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 8.9/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 8.9/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 9.0/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 9.0/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 9.0/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 9.0/99.8 MB 1.5 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 9.1/99.8 MB 1.4 MB/s eta 0:01:03\n",
      "   --- ------------------------------------ 9.2/99.8 MB 1.4 MB/s eta 0:01:03\n",
      "   --- ------------------------------------ 9.2/99.8 MB 1.4 MB/s eta 0:01:03\n",
      "   --- ------------------------------------ 9.2/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.2/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.2/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.3/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.3/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.5/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.6/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.6/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.6/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.7/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.7/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.7/99.8 MB 1.4 MB/s eta 0:01:04\n",
      "   --- ------------------------------------ 9.8/99.8 MB 1.4 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 9.8/99.8 MB 1.4 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 9.8/99.8 MB 1.4 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 9.9/99.8 MB 1.4 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 9.9/99.8 MB 1.4 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 10.0/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.0/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.0/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.1/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.1/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.2/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.2/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.3/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.3/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.3/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.4/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.5/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.5/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.6/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.7/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.7/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.8/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.8/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.9/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 10.9/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 11.0/99.8 MB 1.4 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 11.0/99.8 MB 1.3 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 11.1/99.8 MB 1.3 MB/s eta 0:01:06\n",
      "   ---- ----------------------------------- 11.2/99.8 MB 1.3 MB/s eta 0:01:07\n",
      "   ---- ----------------------------------- 11.2/99.8 MB 1.3 MB/s eta 0:01:07\n",
      "   ---- ----------------------------------- 11.2/99.8 MB 1.3 MB/s eta 0:01:07\n",
      "   ---- ----------------------------------- 11.2/99.8 MB 1.3 MB/s eta 0:01:07\n",
      "   ---- ----------------------------------- 11.3/99.8 MB 1.3 MB/s eta 0:01:07\n",
      "   ---- ----------------------------------- 11.3/99.8 MB 1.3 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 11.4/99.8 MB 1.3 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 11.4/99.8 MB 1.3 MB/s eta 0:01:07\n",
      "   ---- ----------------------------------- 11.4/99.8 MB 1.3 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 11.5/99.8 MB 1.3 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 11.5/99.8 MB 1.3 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 11.5/99.8 MB 1.3 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 11.5/99.8 MB 1.3 MB/s eta 0:01:08\n",
      "   ---- ----------------------------------- 11.6/99.8 MB 1.3 MB/s eta 0:01:09\n",
      "   ---- ----------------------------------- 11.6/99.8 MB 1.3 MB/s eta 0:01:09\n",
      "   ---- ----------------------------------- 11.6/99.8 MB 1.3 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 11.7/99.8 MB 1.3 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 11.7/99.8 MB 1.3 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 11.7/99.8 MB 1.3 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 11.7/99.8 MB 1.3 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 11.7/99.8 MB 1.3 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 11.8/99.8 MB 1.3 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 11.8/99.8 MB 1.3 MB/s eta 0:01:10\n",
      "   ---- ----------------------------------- 11.9/99.8 MB 1.3 MB/s eta 0:01:11\n",
      "   ---- ----------------------------------- 11.9/99.8 MB 1.3 MB/s eta 0:01:11\n",
      "   ---- ----------------------------------- 11.9/99.8 MB 1.3 MB/s eta 0:01:11\n",
      "   ---- ----------------------------------- 12.0/99.8 MB 1.2 MB/s eta 0:01:11\n",
      "   ---- ----------------------------------- 12.0/99.8 MB 1.2 MB/s eta 0:01:11\n",
      "   ---- ----------------------------------- 12.0/99.8 MB 1.2 MB/s eta 0:01:11\n",
      "   ---- ----------------------------------- 12.0/99.8 MB 1.2 MB/s eta 0:01:11\n",
      "   ---- ----------------------------------- 12.1/99.8 MB 1.2 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 12.1/99.8 MB 1.2 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 12.2/99.8 MB 1.2 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 12.2/99.8 MB 1.2 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 1.2 MB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 1.2 MB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 1.2 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 1.2 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 1.2 MB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 1.2 MB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 1.2 MB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 12.4/99.8 MB 1.2 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 12.5/99.8 MB 1.2 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 12.5/99.8 MB 1.2 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 12.5/99.8 MB 1.2 MB/s eta 0:01:14\n",
      "   ----- ---------------------------------- 12.5/99.8 MB 1.2 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 12.5/99.8 MB 1.2 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 12.5/99.8 MB 1.2 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 12.6/99.8 MB 1.2 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 12.6/99.8 MB 1.2 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 12.6/99.8 MB 1.1 MB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 12.6/99.8 MB 1.1 MB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 12.7/99.8 MB 1.1 MB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 12.7/99.8 MB 1.1 MB/s eta 0:01:18\n",
      "   ----- ---------------------------------- 12.7/99.8 MB 1.1 MB/s eta 0:01:18\n",
      "   ----- ---------------------------------- 12.7/99.8 MB 1.1 MB/s eta 0:01:18\n",
      "   ----- ---------------------------------- 12.8/99.8 MB 1.1 MB/s eta 0:01:19\n",
      "   ----- ---------------------------------- 12.8/99.8 MB 1.1 MB/s eta 0:01:19\n",
      "   ----- ---------------------------------- 12.8/99.8 MB 1.1 MB/s eta 0:01:19\n",
      "   ----- ---------------------------------- 12.9/99.8 MB 1.1 MB/s eta 0:01:19\n",
      "   ----- ---------------------------------- 12.9/99.8 MB 1.1 MB/s eta 0:01:20\n",
      "   ----- ---------------------------------- 12.9/99.8 MB 1.1 MB/s eta 0:01:20\n",
      "   ----- ---------------------------------- 12.9/99.8 MB 1.1 MB/s eta 0:01:20\n",
      "   ----- ---------------------------------- 12.9/99.8 MB 1.1 MB/s eta 0:01:21\n",
      "   ----- ---------------------------------- 13.0/99.8 MB 1.1 MB/s eta 0:01:21\n",
      "   ----- ---------------------------------- 13.0/99.8 MB 1.1 MB/s eta 0:01:21\n",
      "   ----- ---------------------------------- 13.0/99.8 MB 1.1 MB/s eta 0:01:21\n",
      "   ----- ---------------------------------- 13.0/99.8 MB 1.1 MB/s eta 0:01:21\n",
      "   ----- ---------------------------------- 13.1/99.8 MB 1.1 MB/s eta 0:01:22\n",
      "   ----- ---------------------------------- 13.1/99.8 MB 1.1 MB/s eta 0:01:22\n",
      "   ----- ---------------------------------- 13.1/99.8 MB 1.1 MB/s eta 0:01:23\n",
      "   ----- ---------------------------------- 13.1/99.8 MB 1.1 MB/s eta 0:01:22\n",
      "   ----- ---------------------------------- 13.1/99.8 MB 1.1 MB/s eta 0:01:23\n",
      "   ----- ---------------------------------- 13.1/99.8 MB 1.0 MB/s eta 0:01:23\n",
      "   ----- ---------------------------------- 13.2/99.8 MB 1.0 MB/s eta 0:01:23\n",
      "   ----- ---------------------------------- 13.2/99.8 MB 1.0 MB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 13.2/99.8 MB 1.0 MB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 13.2/99.8 MB 1.0 MB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 13.2/99.8 MB 1.0 MB/s eta 0:01:25\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 1.0 MB/s eta 0:01:25\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 1.0 MB/s eta 0:01:25\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 1.0 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 1.0 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 13.4/99.8 MB 1.0 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 13.4/99.8 MB 1.0 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 13.4/99.8 MB 1.0 MB/s eta 0:01:27\n",
      "   ----- ---------------------------------- 13.4/99.8 MB 999.6 kB/s eta 0:01:27\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 994.9 kB/s eta 0:01:27\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 992.0 kB/s eta 0:01:27\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 989.0 kB/s eta 0:01:28\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 986.0 kB/s eta 0:01:28\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 980.1 kB/s eta 0:01:28\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 980.1 kB/s eta 0:01:28\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 980.1 kB/s eta 0:01:28\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 980.1 kB/s eta 0:01:28\n",
      "   ----- ---------------------------------- 13.6/99.8 MB 968.5 kB/s eta 0:01:30\n",
      "   ----- ---------------------------------- 13.6/99.8 MB 965.6 kB/s eta 0:01:30\n",
      "   ----- ---------------------------------- 13.6/99.8 MB 958.6 kB/s eta 0:01:30\n",
      "   ----- ---------------------------------- 13.6/99.8 MB 954.4 kB/s eta 0:01:31\n",
      "   ----- ---------------------------------- 13.6/99.8 MB 954.4 kB/s eta 0:01:31\n",
      "   ----- ---------------------------------- 13.7/99.8 MB 948.9 kB/s eta 0:01:31\n",
      "   ----- ---------------------------------- 13.7/99.8 MB 946.1 kB/s eta 0:01:31\n",
      "   ----- ---------------------------------- 13.7/99.8 MB 942.1 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 13.7/99.8 MB 940.7 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 13.7/99.8 MB 936.6 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 13.8/99.8 MB 934.0 kB/s eta 0:01:33\n",
      "   ----- ---------------------------------- 13.8/99.8 MB 931.2 kB/s eta 0:01:33\n",
      "   ----- ---------------------------------- 13.8/99.8 MB 931.2 kB/s eta 0:01:33\n",
      "   ----- ---------------------------------- 13.8/99.8 MB 924.8 kB/s eta 0:01:33\n",
      "   ----- ---------------------------------- 13.8/99.8 MB 922.1 kB/s eta 0:01:34\n",
      "   ----- ---------------------------------- 13.9/99.8 MB 920.8 kB/s eta 0:01:34\n",
      "   ----- ---------------------------------- 13.9/99.8 MB 917.0 kB/s eta 0:01:34\n",
      "   ----- ---------------------------------- 13.9/99.8 MB 914.4 kB/s eta 0:01:34\n",
      "   ----- ---------------------------------- 13.9/99.8 MB 914.4 kB/s eta 0:01:34\n",
      "   ----- ---------------------------------- 13.9/99.8 MB 908.0 kB/s eta 0:01:35\n",
      "   ----- ---------------------------------- 14.0/99.8 MB 905.5 kB/s eta 0:01:35\n",
      "   ----- ---------------------------------- 14.0/99.8 MB 903.0 kB/s eta 0:01:35\n",
      "   ----- ---------------------------------- 14.0/99.8 MB 903.0 kB/s eta 0:01:35\n",
      "   ----- ---------------------------------- 14.0/99.8 MB 894.4 kB/s eta 0:01:36\n",
      "   ----- ---------------------------------- 14.0/99.8 MB 891.9 kB/s eta 0:01:37\n",
      "   ----- ---------------------------------- 14.0/99.8 MB 889.5 kB/s eta 0:01:37\n",
      "   ----- ---------------------------------- 14.1/99.8 MB 885.9 kB/s eta 0:01:37\n",
      "   ----- ---------------------------------- 14.1/99.8 MB 884.7 kB/s eta 0:01:37\n",
      "   ----- ---------------------------------- 14.1/99.8 MB 881.2 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 14.1/99.8 MB 880.0 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 876.4 kB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 872.9 kB/s eta 0:01:39\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 870.6 kB/s eta 0:01:39\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 870.6 kB/s eta 0:01:39\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 862.6 kB/s eta 0:01:40\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 860.4 kB/s eta 0:01:40\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 860.4 kB/s eta 0:01:40\n",
      "   ----- ---------------------------------- 14.3/99.8 MB 855.8 kB/s eta 0:01:40\n",
      "   ----- ---------------------------------- 14.3/99.8 MB 853.5 kB/s eta 0:01:41\n",
      "   ----- ---------------------------------- 14.3/99.8 MB 851.4 kB/s eta 0:01:41\n",
      "   ----- ---------------------------------- 14.3/99.8 MB 848.1 kB/s eta 0:01:41\n",
      "   ----- ---------------------------------- 14.3/99.8 MB 844.7 kB/s eta 0:01:42\n",
      "   ----- ---------------------------------- 14.4/99.8 MB 842.6 kB/s eta 0:01:42\n",
      "   ----- ---------------------------------- 14.4/99.8 MB 840.4 kB/s eta 0:01:42\n",
      "   ----- ---------------------------------- 14.4/99.8 MB 836.2 kB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 14.4/99.8 MB 835.1 kB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 14.4/99.8 MB 832.9 kB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 14.5/99.8 MB 831.9 kB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 14.5/99.8 MB 834.1 kB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 14.5/99.8 MB 834.1 kB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 14.5/99.8 MB 828.7 kB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 14.6/99.8 MB 825.6 kB/s eta 0:01:44\n",
      "   ----- ---------------------------------- 14.6/99.8 MB 824.6 kB/s eta 0:01:44\n",
      "   ----- ---------------------------------- 14.6/99.8 MB 823.5 kB/s eta 0:01:44\n",
      "   ----- ---------------------------------- 14.7/99.8 MB 821.5 kB/s eta 0:01:44\n",
      "   ----- ---------------------------------- 14.7/99.8 MB 818.4 kB/s eta 0:01:44\n",
      "   ----- ---------------------------------- 14.7/99.8 MB 817.3 kB/s eta 0:01:45\n",
      "   ----- ---------------------------------- 14.7/99.8 MB 814.3 kB/s eta 0:01:45\n",
      "   ----- ---------------------------------- 14.8/99.8 MB 812.3 kB/s eta 0:01:45\n",
      "   ----- ---------------------------------- 14.8/99.8 MB 812.3 kB/s eta 0:01:45\n",
      "   ----- ---------------------------------- 14.8/99.8 MB 810.3 kB/s eta 0:01:45\n",
      "   ----- ---------------------------------- 14.8/99.8 MB 811.2 kB/s eta 0:01:45\n",
      "   ----- ---------------------------------- 14.8/99.8 MB 807.3 kB/s eta 0:01:46\n",
      "   ----- ---------------------------------- 14.9/99.8 MB 806.3 kB/s eta 0:01:46\n",
      "   ----- ---------------------------------- 14.9/99.8 MB 803.3 kB/s eta 0:01:46\n",
      "   ----- ---------------------------------- 14.9/99.8 MB 806.3 kB/s eta 0:01:46\n",
      "   ----- ---------------------------------- 15.0/99.8 MB 804.3 kB/s eta 0:01:46\n",
      "   ------ --------------------------------- 15.0/99.8 MB 802.3 kB/s eta 0:01:46\n",
      "   ------ --------------------------------- 15.0/99.8 MB 799.4 kB/s eta 0:01:47\n",
      "   ------ --------------------------------- 15.0/99.8 MB 797.5 kB/s eta 0:01:47\n",
      "   ------ --------------------------------- 15.1/99.8 MB 795.5 kB/s eta 0:01:47\n",
      "   ------ --------------------------------- 15.1/99.8 MB 794.5 kB/s eta 0:01:47\n",
      "   ------ --------------------------------- 15.1/99.8 MB 790.7 kB/s eta 0:01:48\n",
      "   ------ --------------------------------- 15.1/99.8 MB 790.7 kB/s eta 0:01:48\n",
      "   ------ --------------------------------- 15.1/99.8 MB 787.9 kB/s eta 0:01:48\n",
      "   ------ --------------------------------- 15.2/99.8 MB 785.9 kB/s eta 0:01:48\n",
      "   ------ --------------------------------- 15.2/99.8 MB 785.0 kB/s eta 0:01:48\n",
      "   ------ --------------------------------- 15.2/99.8 MB 782.2 kB/s eta 0:01:49\n",
      "   ------ --------------------------------- 15.3/99.8 MB 779.4 kB/s eta 0:01:49\n",
      "   ------ --------------------------------- 15.3/99.8 MB 779.4 kB/s eta 0:01:49\n",
      "   ------ --------------------------------- 15.3/99.8 MB 775.7 kB/s eta 0:01:49\n",
      "   ------ --------------------------------- 15.4/99.8 MB 775.7 kB/s eta 0:01:49\n",
      "   ------ --------------------------------- 15.4/99.8 MB 773.0 kB/s eta 0:01:50\n",
      "   ------ --------------------------------- 15.4/99.8 MB 770.3 kB/s eta 0:01:50\n",
      "   ------ --------------------------------- 15.4/99.8 MB 768.5 kB/s eta 0:01:50\n",
      "   ------ --------------------------------- 15.5/99.8 MB 766.6 kB/s eta 0:01:50\n",
      "   ------ --------------------------------- 15.5/99.8 MB 763.9 kB/s eta 0:01:51\n",
      "   ------ --------------------------------- 15.5/99.8 MB 764.0 kB/s eta 0:01:51\n",
      "   ------ --------------------------------- 15.5/99.8 MB 760.4 kB/s eta 0:01:51\n",
      "   ------ --------------------------------- 15.6/99.8 MB 759.5 kB/s eta 0:01:51\n",
      "   ------ --------------------------------- 15.6/99.8 MB 756.9 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 15.7/99.8 MB 756.9 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 15.7/99.8 MB 756.0 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 15.7/99.8 MB 753.4 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 15.8/99.8 MB 752.5 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 15.8/99.8 MB 751.6 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 15.9/99.8 MB 750.8 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 15.9/99.8 MB 748.2 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.0/99.8 MB 747.4 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.0/99.8 MB 746.5 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.1/99.8 MB 744.8 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.1/99.8 MB 743.1 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.1/99.8 MB 745.7 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.2/99.8 MB 744.8 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.2/99.8 MB 743.1 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.3/99.8 MB 742.3 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.3/99.8 MB 741.4 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.4/99.8 MB 750.8 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 16.4/99.8 MB 749.9 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 16.4/99.8 MB 748.2 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 16.4/99.8 MB 745.7 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 16.4/99.8 MB 745.7 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 16.4/99.8 MB 745.7 kB/s eta 0:01:52\n",
      "   ------ --------------------------------- 16.5/99.8 MB 737.3 kB/s eta 0:01:53\n",
      "   ------ --------------------------------- 16.5/99.8 MB 734.8 kB/s eta 0:01:54\n",
      "   ------ --------------------------------- 16.6/99.8 MB 732.3 kB/s eta 0:01:54\n",
      "   ------ --------------------------------- 16.6/99.8 MB 729.9 kB/s eta 0:01:54\n",
      "   ------ --------------------------------- 16.6/99.8 MB 726.6 kB/s eta 0:01:55\n",
      "   ------ --------------------------------- 16.7/99.8 MB 725.1 kB/s eta 0:01:55\n",
      "   ------ --------------------------------- 16.7/99.8 MB 722.6 kB/s eta 0:01:55\n",
      "   ------ --------------------------------- 16.7/99.8 MB 720.3 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 16.8/99.8 MB 718.7 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 16.8/99.8 MB 721.1 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 16.8/99.8 MB 718.7 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 16.9/99.8 MB 717.1 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 16.9/99.8 MB 714.0 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 16.9/99.8 MB 717.1 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 17.0/99.8 MB 715.5 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 17.0/99.8 MB 714.0 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 17.0/99.8 MB 712.4 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 17.1/99.8 MB 711.6 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 17.1/99.8 MB 710.1 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 17.1/99.8 MB 714.0 kB/s eta 0:01:56\n",
      "   ------ --------------------------------- 17.2/99.8 MB 711.6 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 17.2/99.8 MB 711.6 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 17.2/99.8 MB 709.3 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 17.2/99.8 MB 707.0 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 17.2/99.8 MB 705.5 kB/s eta 0:01:57\n",
      "   ------ --------------------------------- 17.3/99.8 MB 703.2 kB/s eta 0:01:58\n",
      "   ------ --------------------------------- 17.3/99.8 MB 701.7 kB/s eta 0:01:58\n",
      "   ------ --------------------------------- 17.3/99.8 MB 698.7 kB/s eta 0:01:58\n",
      "   ------ --------------------------------- 17.4/99.8 MB 697.2 kB/s eta 0:01:59\n",
      "   ------ --------------------------------- 17.4/99.8 MB 695.0 kB/s eta 0:01:59\n",
      "   ------ --------------------------------- 17.4/99.8 MB 693.5 kB/s eta 0:01:59\n",
      "   ------- -------------------------------- 17.5/99.8 MB 692.8 kB/s eta 0:01:59\n",
      "   ------- -------------------------------- 17.5/99.8 MB 691.3 kB/s eta 0:01:59\n",
      "   ------- -------------------------------- 17.5/99.8 MB 689.9 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 17.6/99.8 MB 690.6 kB/s eta 0:01:59\n",
      "   ------- -------------------------------- 17.6/99.8 MB 689.2 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 17.7/99.8 MB 688.5 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 17.7/99.8 MB 687.7 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 17.8/99.8 MB 687.7 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 17.8/99.8 MB 689.9 kB/s eta 0:01:59\n",
      "   ------- -------------------------------- 17.8/99.8 MB 688.5 kB/s eta 0:01:59\n",
      "   ------- -------------------------------- 17.9/99.8 MB 687.7 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 17.9/99.8 MB 687.7 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 17.9/99.8 MB 686.3 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 18.0/99.8 MB 684.8 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 18.0/99.8 MB 684.9 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 18.0/99.8 MB 682.0 kB/s eta 0:02:00\n",
      "   ------- -------------------------------- 18.0/99.8 MB 679.9 kB/s eta 0:02:01\n",
      "   ------- -------------------------------- 18.1/99.8 MB 679.1 kB/s eta 0:02:01\n",
      "   ------- -------------------------------- 18.1/99.8 MB 677.7 kB/s eta 0:02:01\n",
      "   ------- -------------------------------- 18.1/99.8 MB 675.6 kB/s eta 0:02:01\n",
      "   ------- -------------------------------- 18.1/99.8 MB 675.7 kB/s eta 0:02:01\n",
      "   ------- -------------------------------- 18.1/99.8 MB 673.6 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 18.2/99.8 MB 672.2 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 18.2/99.8 MB 671.5 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 18.2/99.8 MB 668.8 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 18.3/99.8 MB 668.1 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 18.3/99.8 MB 666.7 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.4/99.8 MB 666.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.4/99.8 MB 665.4 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.5/99.8 MB 664.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.5/99.8 MB 663.3 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.5/99.8 MB 663.3 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.6/99.8 MB 662.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.6/99.8 MB 660.6 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.7/99.8 MB 662.7 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.7/99.8 MB 662.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.7/99.8 MB 661.3 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.8/99.8 MB 660.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.8/99.8 MB 659.3 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.8/99.8 MB 657.3 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 18.9/99.8 MB 656.7 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 18.9/99.8 MB 655.3 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 18.9/99.8 MB 658.6 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 18.9/99.8 MB 656.7 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 19.0/99.8 MB 656.7 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.0/99.8 MB 655.4 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 19.1/99.8 MB 654.7 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 19.1/99.8 MB 653.4 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 19.1/99.8 MB 655.3 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 19.1/99.8 MB 655.3 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.2/99.8 MB 654.0 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 19.2/99.8 MB 652.8 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 19.3/99.8 MB 651.5 kB/s eta 0:02:04\n",
      "   ------- -------------------------------- 19.3/99.8 MB 657.4 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.4/99.8 MB 656.7 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.4/99.8 MB 656.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.4/99.8 MB 655.3 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.4/99.8 MB 654.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.5/99.8 MB 658.0 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 19.5/99.8 MB 656.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.6/99.8 MB 657.4 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 19.7/99.8 MB 656.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.7/99.8 MB 654.0 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.7/99.8 MB 652.7 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.8/99.8 MB 652.1 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.8/99.8 MB 656.7 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 19.8/99.8 MB 655.4 kB/s eta 0:02:02\n",
      "   ------- -------------------------------- 19.9/99.8 MB 652.7 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.9/99.8 MB 650.8 kB/s eta 0:02:03\n",
      "   ------- -------------------------------- 19.9/99.8 MB 650.8 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.0/99.8 MB 651.5 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.0/99.8 MB 651.5 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.0/99.8 MB 648.2 kB/s eta 0:02:04\n",
      "   -------- ------------------------------- 20.0/99.8 MB 648.8 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.1/99.8 MB 650.8 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.1/99.8 MB 650.2 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.2/99.8 MB 650.1 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.2/99.8 MB 649.5 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.2/99.8 MB 650.2 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.3/99.8 MB 650.8 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.3/99.8 MB 650.2 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.4/99.8 MB 650.2 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.4/99.8 MB 650.8 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 20.4/99.8 MB 650.8 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 20.5/99.8 MB 651.5 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 20.5/99.8 MB 650.1 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 20.6/99.8 MB 649.5 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 20.6/99.8 MB 649.5 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 20.7/99.8 MB 648.2 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.7/99.8 MB 647.0 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.7/99.8 MB 645.7 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.8/99.8 MB 645.7 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.8/99.8 MB 645.0 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.8/99.8 MB 644.4 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.9/99.8 MB 643.7 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 20.9/99.8 MB 643.8 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.0/99.8 MB 643.7 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.0/99.8 MB 643.1 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.1/99.8 MB 643.1 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.1/99.8 MB 641.9 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.1/99.8 MB 642.5 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.2/99.8 MB 640.6 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.2/99.8 MB 640.0 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.3/99.8 MB 640.0 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.3/99.8 MB 639.4 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.4/99.8 MB 640.0 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.4/99.8 MB 641.9 kB/s eta 0:02:03\n",
      "   -------- ------------------------------- 21.5/99.8 MB 642.5 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 21.5/99.8 MB 643.1 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 21.6/99.8 MB 643.1 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 21.6/99.8 MB 642.5 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 21.7/99.8 MB 643.8 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 21.7/99.8 MB 643.8 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 21.7/99.8 MB 642.5 kB/s eta 0:02:02\n",
      "   -------- ------------------------------- 21.8/99.8 MB 648.2 kB/s eta 0:02:01\n",
      "   -------- ------------------------------- 21.8/99.8 MB 648.8 kB/s eta 0:02:01\n",
      "   -------- ------------------------------- 21.9/99.8 MB 647.6 kB/s eta 0:02:01\n",
      "   -------- ------------------------------- 21.9/99.8 MB 650.8 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.0/99.8 MB 652.1 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.0/99.8 MB 651.5 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.0/99.8 MB 650.8 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.1/99.8 MB 652.1 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.1/99.8 MB 652.1 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.1/99.8 MB 654.1 kB/s eta 0:01:59\n",
      "   -------- ------------------------------- 22.1/99.8 MB 652.8 kB/s eta 0:01:59\n",
      "   -------- ------------------------------- 22.2/99.8 MB 650.8 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.2/99.8 MB 650.8 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.2/99.8 MB 650.8 kB/s eta 0:02:00\n",
      "   -------- ------------------------------- 22.3/99.8 MB 654.1 kB/s eta 0:01:59\n",
      "   -------- ------------------------------- 22.4/99.8 MB 653.4 kB/s eta 0:01:59\n",
      "   -------- ------------------------------- 22.4/99.8 MB 654.1 kB/s eta 0:01:59\n",
      "   -------- ------------------------------- 22.4/99.8 MB 652.1 kB/s eta 0:01:59\n",
      "   --------- ------------------------------ 22.5/99.8 MB 652.1 kB/s eta 0:01:59\n",
      "   --------- ------------------------------ 22.5/99.8 MB 652.1 kB/s eta 0:01:59\n",
      "   --------- ------------------------------ 22.5/99.8 MB 650.1 kB/s eta 0:01:59\n",
      "   --------- ------------------------------ 22.6/99.8 MB 654.7 kB/s eta 0:01:58\n",
      "   --------- ------------------------------ 22.6/99.8 MB 658.6 kB/s eta 0:01:58\n",
      "   --------- ------------------------------ 22.7/99.8 MB 657.3 kB/s eta 0:01:58\n",
      "   --------- ------------------------------ 22.7/99.8 MB 662.6 kB/s eta 0:01:57\n",
      "   --------- ------------------------------ 22.7/99.8 MB 661.9 kB/s eta 0:01:57\n",
      "   --------- ------------------------------ 22.8/99.8 MB 665.4 kB/s eta 0:01:56\n",
      "   --------- ------------------------------ 22.8/99.8 MB 662.6 kB/s eta 0:01:57\n",
      "   --------- ------------------------------ 22.8/99.8 MB 662.0 kB/s eta 0:01:57\n",
      "   --------- ------------------------------ 22.8/99.8 MB 663.4 kB/s eta 0:01:56\n",
      "   --------- ------------------------------ 22.9/99.8 MB 666.0 kB/s eta 0:01:56\n",
      "   --------- ------------------------------ 22.9/99.8 MB 664.7 kB/s eta 0:01:56\n",
      "   --------- ------------------------------ 22.9/99.8 MB 666.7 kB/s eta 0:01:56\n",
      "   --------- ------------------------------ 23.0/99.8 MB 668.0 kB/s eta 0:01:55\n",
      "   --------- ------------------------------ 23.1/99.8 MB 668.7 kB/s eta 0:01:55\n",
      "   --------- ------------------------------ 23.1/99.8 MB 672.2 kB/s eta 0:01:55\n",
      "   --------- ------------------------------ 23.1/99.8 MB 672.8 kB/s eta 0:01:54\n",
      "   --------- ------------------------------ 23.1/99.8 MB 672.2 kB/s eta 0:01:54\n",
      "   --------- ------------------------------ 23.2/99.8 MB 674.3 kB/s eta 0:01:54\n",
      "   --------- ------------------------------ 23.2/99.8 MB 678.5 kB/s eta 0:01:53\n",
      "   --------- ------------------------------ 23.3/99.8 MB 676.3 kB/s eta 0:01:54\n",
      "   --------- ------------------------------ 23.3/99.8 MB 679.1 kB/s eta 0:01:53\n",
      "   --------- ------------------------------ 23.4/99.8 MB 684.1 kB/s eta 0:01:52\n",
      "   --------- ------------------------------ 23.4/99.8 MB 686.2 kB/s eta 0:01:52\n",
      "   --------- ------------------------------ 23.4/99.8 MB 687.0 kB/s eta 0:01:52\n",
      "   --------- ------------------------------ 23.4/99.8 MB 689.2 kB/s eta 0:01:51\n",
      "   --------- ------------------------------ 23.5/99.8 MB 692.8 kB/s eta 0:01:51\n",
      "   --------- ------------------------------ 23.6/99.8 MB 692.8 kB/s eta 0:01:50\n",
      "   --------- ------------------------------ 23.6/99.8 MB 693.5 kB/s eta 0:01:50\n",
      "   --------- ------------------------------ 23.7/99.8 MB 695.8 kB/s eta 0:01:50\n",
      "   --------- ------------------------------ 23.7/99.8 MB 697.3 kB/s eta 0:01:50\n",
      "   --------- ------------------------------ 23.7/99.8 MB 698.8 kB/s eta 0:01:49\n",
      "   --------- ------------------------------ 23.8/99.8 MB 701.7 kB/s eta 0:01:49\n",
      "   --------- ------------------------------ 23.8/99.8 MB 708.6 kB/s eta 0:01:48\n",
      "   --------- ------------------------------ 23.8/99.8 MB 712.4 kB/s eta 0:01:47\n",
      "   --------- ------------------------------ 23.9/99.8 MB 714.7 kB/s eta 0:01:47\n",
      "   --------- ------------------------------ 23.9/99.8 MB 714.7 kB/s eta 0:01:47\n",
      "   --------- ------------------------------ 23.9/99.8 MB 717.1 kB/s eta 0:01:46\n",
      "   --------- ------------------------------ 24.0/99.8 MB 718.7 kB/s eta 0:01:46\n",
      "   --------- ------------------------------ 24.0/99.8 MB 717.9 kB/s eta 0:01:46\n",
      "   --------- ------------------------------ 24.0/99.8 MB 717.9 kB/s eta 0:01:46\n",
      "   --------- ------------------------------ 24.0/99.8 MB 719.4 kB/s eta 0:01:46\n",
      "   --------- ------------------------------ 24.1/99.8 MB 718.7 kB/s eta 0:01:46\n",
      "   --------- ------------------------------ 24.1/99.8 MB 721.0 kB/s eta 0:01:45\n",
      "   --------- ------------------------------ 24.2/99.8 MB 726.6 kB/s eta 0:01:45\n",
      "   --------- ------------------------------ 24.2/99.8 MB 725.1 kB/s eta 0:01:45\n",
      "   --------- ------------------------------ 24.2/99.8 MB 731.5 kB/s eta 0:01:44\n",
      "   --------- ------------------------------ 24.2/99.8 MB 734.8 kB/s eta 0:01:43\n",
      "   --------- ------------------------------ 24.3/99.8 MB 732.3 kB/s eta 0:01:44\n",
      "   --------- ------------------------------ 24.3/99.8 MB 732.4 kB/s eta 0:01:44\n",
      "   --------- ------------------------------ 24.3/99.8 MB 733.9 kB/s eta 0:01:43\n",
      "   --------- ------------------------------ 24.4/99.8 MB 732.3 kB/s eta 0:01:43\n",
      "   --------- ------------------------------ 24.4/99.8 MB 733.9 kB/s eta 0:01:43\n",
      "   --------- ------------------------------ 24.4/99.8 MB 738.1 kB/s eta 0:01:43\n",
      "   --------- ------------------------------ 24.4/99.8 MB 738.1 kB/s eta 0:01:43\n",
      "   --------- ------------------------------ 24.5/99.8 MB 745.7 kB/s eta 0:01:41\n",
      "   --------- ------------------------------ 24.5/99.8 MB 746.5 kB/s eta 0:01:41\n",
      "   --------- ------------------------------ 24.6/99.8 MB 751.7 kB/s eta 0:01:41\n",
      "   --------- ------------------------------ 24.6/99.8 MB 752.6 kB/s eta 0:01:40\n",
      "   --------- ------------------------------ 24.7/99.8 MB 756.0 kB/s eta 0:01:40\n",
      "   --------- ------------------------------ 24.7/99.8 MB 754.3 kB/s eta 0:01:40\n",
      "   --------- ------------------------------ 24.7/99.8 MB 756.0 kB/s eta 0:01:40\n",
      "   --------- ------------------------------ 24.7/99.8 MB 755.1 kB/s eta 0:01:40\n",
      "   --------- ------------------------------ 24.8/99.8 MB 758.7 kB/s eta 0:01:39\n",
      "   --------- ------------------------------ 24.8/99.8 MB 757.8 kB/s eta 0:01:39\n",
      "   --------- ------------------------------ 24.8/99.8 MB 759.6 kB/s eta 0:01:39\n",
      "   --------- ------------------------------ 24.9/99.8 MB 757.8 kB/s eta 0:01:39\n",
      "   --------- ------------------------------ 24.9/99.8 MB 761.3 kB/s eta 0:01:39\n",
      "   ---------- ----------------------------- 24.9/99.8 MB 762.2 kB/s eta 0:01:39\n",
      "   ---------- ----------------------------- 25.0/99.8 MB 763.9 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 25.0/99.8 MB 766.6 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 25.1/99.8 MB 769.3 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 25.1/99.8 MB 769.3 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 25.1/99.8 MB 770.3 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.2/99.8 MB 770.3 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.2/99.8 MB 772.1 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.2/99.8 MB 772.1 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.3/99.8 MB 772.1 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.3/99.8 MB 772.1 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.3/99.8 MB 773.0 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.3/99.8 MB 773.0 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.3/99.8 MB 773.0 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.3/99.8 MB 773.0 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.4/99.8 MB 772.9 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.5/99.8 MB 772.1 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.5/99.8 MB 770.3 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.5/99.8 MB 769.3 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.5/99.8 MB 768.4 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.5/99.8 MB 768.4 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.6/99.8 MB 766.6 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.6/99.8 MB 766.6 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.6/99.8 MB 764.8 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.6/99.8 MB 767.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.7/99.8 MB 766.6 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.7/99.8 MB 764.8 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.7/99.8 MB 766.6 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.7/99.8 MB 765.7 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.8/99.8 MB 768.4 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.8/99.8 MB 767.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.8/99.8 MB 769.3 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.9/99.8 MB 768.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 25.9/99.8 MB 769.3 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 25.9/99.8 MB 766.6 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.0/99.8 MB 767.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.0/99.8 MB 766.6 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.1/99.8 MB 767.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.1/99.8 MB 767.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.1/99.8 MB 767.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.2/99.8 MB 763.1 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.2/99.8 MB 763.9 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.2/99.8 MB 760.4 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.2/99.8 MB 759.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.2/99.8 MB 756.9 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.3/99.8 MB 758.7 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.3/99.8 MB 756.0 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.3/99.8 MB 756.9 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.4/99.8 MB 755.2 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.4/99.8 MB 755.1 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.4/99.8 MB 754.3 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.5/99.8 MB 754.3 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.5/99.8 MB 752.5 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.5/99.8 MB 753.4 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.6/99.8 MB 753.4 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 26.6/99.8 MB 755.1 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.7/99.8 MB 761.3 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.7/99.8 MB 759.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.7/99.8 MB 756.9 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.8/99.8 MB 756.9 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.8/99.8 MB 758.7 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.8/99.8 MB 758.6 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.9/99.8 MB 758.7 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.9/99.8 MB 757.8 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.9/99.8 MB 757.8 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.9/99.8 MB 756.0 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 26.9/99.8 MB 754.2 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 27.0/99.8 MB 752.6 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 27.0/99.8 MB 750.8 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 27.0/99.8 MB 750.8 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 27.0/99.8 MB 749.9 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 27.1/99.8 MB 749.1 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.1/99.8 MB 749.1 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.1/99.8 MB 746.5 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.1/99.8 MB 747.4 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.2/99.8 MB 746.5 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.2/99.8 MB 745.7 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.2/99.8 MB 746.5 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.3/99.8 MB 745.7 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.3/99.8 MB 745.7 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.3/99.8 MB 745.7 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.3/99.8 MB 746.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 27.4/99.8 MB 746.5 kB/s eta 0:01:37\n",
      "   ---------- ----------------------------- 27.4/99.8 MB 745.7 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 27.4/99.8 MB 748.2 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.5/99.8 MB 747.4 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.5/99.8 MB 749.1 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.5/99.8 MB 749.1 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.6/99.8 MB 749.1 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.6/99.8 MB 749.1 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.6/99.8 MB 749.9 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.6/99.8 MB 749.1 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.7/99.8 MB 748.2 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.7/99.8 MB 748.2 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.7/99.8 MB 748.2 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.8/99.8 MB 748.2 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.8/99.8 MB 747.4 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.8/99.8 MB 747.4 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.9/99.8 MB 746.5 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.9/99.8 MB 744.8 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 27.9/99.8 MB 744.8 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 28.0/99.8 MB 744.0 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 28.0/99.8 MB 743.2 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 28.0/99.8 MB 743.2 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 28.1/99.8 MB 744.0 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 28.1/99.8 MB 743.1 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 28.1/99.8 MB 744.0 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 28.2/99.8 MB 744.8 kB/s eta 0:01:37\n",
      "   ----------- ---------------------------- 28.2/99.8 MB 746.5 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.3/99.8 MB 746.5 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.3/99.8 MB 748.3 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.3/99.8 MB 747.4 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.4/99.8 MB 748.2 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.4/99.8 MB 749.1 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.4/99.8 MB 753.4 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 28.4/99.8 MB 750.8 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 28.5/99.8 MB 749.1 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.5/99.8 MB 748.2 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.5/99.8 MB 748.2 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.6/99.8 MB 748.2 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.6/99.8 MB 747.4 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.6/99.8 MB 747.4 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.7/99.8 MB 745.7 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.7/99.8 MB 745.7 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.7/99.8 MB 745.7 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.8/99.8 MB 747.4 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 28.8/99.8 MB 744.0 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.8/99.8 MB 744.8 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.9/99.8 MB 745.7 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 28.9/99.8 MB 745.7 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 28.9/99.8 MB 745.7 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.0/99.8 MB 744.8 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.0/99.8 MB 746.5 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.0/99.8 MB 744.0 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 29.1/99.8 MB 744.8 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.1/99.8 MB 744.0 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.1/99.8 MB 743.1 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 29.2/99.8 MB 742.3 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 29.2/99.8 MB 743.1 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.2/99.8 MB 743.1 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.3/99.8 MB 743.1 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.3/99.8 MB 743.1 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.3/99.8 MB 743.1 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.3/99.8 MB 742.3 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.4/99.8 MB 740.6 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 29.4/99.8 MB 742.3 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.5/99.8 MB 742.3 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.5/99.8 MB 742.3 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.5/99.8 MB 741.4 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.6/99.8 MB 740.6 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.6/99.8 MB 739.7 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.6/99.8 MB 739.8 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.6/99.8 MB 738.1 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.7/99.8 MB 740.6 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.7/99.8 MB 738.9 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.8/99.8 MB 739.7 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.8/99.8 MB 738.9 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.8/99.8 MB 738.1 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.9/99.8 MB 737.3 kB/s eta 0:01:35\n",
      "   ----------- ---------------------------- 29.9/99.8 MB 738.1 kB/s eta 0:01:35\n",
      "   ------------ --------------------------- 29.9/99.8 MB 738.1 kB/s eta 0:01:35\n",
      "   ------------ --------------------------- 30.0/99.8 MB 737.3 kB/s eta 0:01:35\n",
      "   ------------ --------------------------- 30.0/99.8 MB 735.6 kB/s eta 0:01:35\n",
      "   ------------ --------------------------- 30.0/99.8 MB 736.4 kB/s eta 0:01:35\n",
      "   ------------ --------------------------- 30.1/99.8 MB 732.3 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.1/99.8 MB 733.9 kB/s eta 0:01:35\n",
      "   ------------ --------------------------- 30.1/99.8 MB 733.9 kB/s eta 0:01:35\n",
      "   ------------ --------------------------- 30.1/99.8 MB 733.9 kB/s eta 0:01:35\n",
      "   ------------ --------------------------- 30.1/99.8 MB 731.5 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.2/99.8 MB 729.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.2/99.8 MB 729.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.2/99.8 MB 727.5 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.2/99.8 MB 725.8 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.2/99.8 MB 729.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.3/99.8 MB 729.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.3/99.8 MB 727.4 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.3/99.8 MB 727.4 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.4/99.8 MB 726.6 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.4/99.8 MB 725.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.4/99.8 MB 726.6 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.5/99.8 MB 725.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.5/99.8 MB 724.2 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.5/99.8 MB 723.4 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.6/99.8 MB 723.4 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.6/99.8 MB 722.6 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.6/99.8 MB 721.0 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.7/99.8 MB 723.4 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.7/99.8 MB 721.8 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.7/99.8 MB 721.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.8/99.8 MB 721.0 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.8/99.8 MB 722.6 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.8/99.8 MB 721.9 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.9/99.8 MB 720.3 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.9/99.8 MB 721.0 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 30.9/99.8 MB 721.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.0/99.8 MB 718.7 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.0/99.8 MB 718.6 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.0/99.8 MB 717.9 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.1/99.8 MB 716.3 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.1/99.8 MB 717.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.1/99.8 MB 717.1 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.1/99.8 MB 715.5 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.2/99.8 MB 715.6 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.2/99.8 MB 714.8 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.3/99.8 MB 715.5 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.3/99.8 MB 715.5 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.3/99.8 MB 713.2 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.4/99.8 MB 712.4 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.4/99.8 MB 714.0 kB/s eta 0:01:36\n",
      "   ------------ --------------------------- 31.4/99.8 MB 711.6 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.4/99.8 MB 710.1 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.5/99.8 MB 710.8 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.5/99.8 MB 710.1 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.5/99.8 MB 708.6 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.6/99.8 MB 707.0 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.6/99.8 MB 707.0 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.6/99.8 MB 706.3 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.6/99.8 MB 704.7 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.6/99.8 MB 704.7 kB/s eta 0:01:37\n",
      "   ------------ --------------------------- 31.7/99.8 MB 701.0 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.7/99.8 MB 699.5 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.7/99.8 MB 699.5 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.7/99.8 MB 697.2 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.8/99.8 MB 697.2 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.8/99.8 MB 695.8 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.8/99.8 MB 695.8 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.8/99.8 MB 695.0 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.9/99.8 MB 695.0 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 31.9/99.8 MB 693.5 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.0/99.8 MB 693.5 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.0/99.8 MB 692.1 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.0/99.8 MB 691.3 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.1/99.8 MB 691.3 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.1/99.8 MB 692.1 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.1/99.8 MB 691.3 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.2/99.8 MB 690.6 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.2/99.8 MB 690.6 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.2/99.8 MB 688.5 kB/s eta 0:01:39\n",
      "   ------------ --------------------------- 32.2/99.8 MB 689.2 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.3/99.8 MB 687.7 kB/s eta 0:01:39\n",
      "   ------------ --------------------------- 32.3/99.8 MB 687.7 kB/s eta 0:01:39\n",
      "   ------------ --------------------------- 32.3/99.8 MB 689.2 kB/s eta 0:01:38\n",
      "   ------------ --------------------------- 32.4/99.8 MB 688.4 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.4/99.8 MB 688.4 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.5/99.8 MB 691.3 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.5/99.8 MB 689.9 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.5/99.8 MB 687.7 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.6/99.8 MB 687.7 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.6/99.8 MB 687.0 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.6/99.8 MB 686.2 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.7/99.8 MB 687.0 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.7/99.8 MB 687.0 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.7/99.8 MB 685.5 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.7/99.8 MB 686.3 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.8/99.8 MB 685.5 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.8/99.8 MB 684.8 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.8/99.8 MB 684.1 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.8/99.8 MB 682.7 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 32.9/99.8 MB 682.0 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 32.9/99.8 MB 681.3 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 32.9/99.8 MB 679.9 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.0/99.8 MB 679.1 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.0/99.8 MB 679.9 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.0/99.8 MB 679.2 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.0/99.8 MB 679.2 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.0/99.8 MB 676.3 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.0/99.8 MB 676.3 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.1/99.8 MB 676.3 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.1/99.8 MB 675.7 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.1/99.8 MB 675.0 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.2/99.8 MB 675.7 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.2/99.8 MB 675.7 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.2/99.8 MB 672.9 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.2/99.8 MB 672.2 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 33.2/99.8 MB 670.8 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.3/99.8 MB 669.4 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.3/99.8 MB 668.7 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.3/99.8 MB 667.4 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.3/99.8 MB 666.7 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.4/99.8 MB 664.7 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.4/99.8 MB 666.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.4/99.8 MB 664.7 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.4/99.8 MB 663.3 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.5/99.8 MB 664.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.5/99.8 MB 663.3 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.5/99.8 MB 663.3 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.6/99.8 MB 662.6 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.6/99.8 MB 662.6 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.6/99.8 MB 661.3 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.7/99.8 MB 661.3 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.7/99.8 MB 664.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.8/99.8 MB 662.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.8/99.8 MB 661.3 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.8/99.8 MB 659.3 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 33.8/99.8 MB 660.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.8/99.8 MB 660.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 33.9/99.8 MB 657.4 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 33.9/99.8 MB 658.0 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 33.9/99.8 MB 658.0 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 33.9/99.8 MB 656.7 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 34.0/99.8 MB 655.4 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 34.0/99.8 MB 655.3 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 34.0/99.8 MB 654.7 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 34.1/99.8 MB 654.7 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 34.1/99.8 MB 654.7 kB/s eta 0:01:41\n",
      "   ------------- -------------------------- 34.2/99.8 MB 656.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.2/99.8 MB 656.7 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.2/99.8 MB 656.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.3/99.8 MB 658.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.3/99.8 MB 657.4 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.3/99.8 MB 657.4 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.4/99.8 MB 657.3 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.4/99.8 MB 658.0 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.5/99.8 MB 658.6 kB/s eta 0:01:40\n",
      "   ------------- -------------------------- 34.5/99.8 MB 660.7 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 34.5/99.8 MB 661.3 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 34.6/99.8 MB 663.3 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 34.6/99.8 MB 663.3 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 34.7/99.8 MB 663.3 kB/s eta 0:01:39\n",
      "   ------------- -------------------------- 34.7/99.8 MB 664.0 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 34.8/99.8 MB 663.4 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 34.8/99.8 MB 663.3 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 34.8/99.8 MB 663.3 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 34.9/99.8 MB 662.7 kB/s eta 0:01:38\n",
      "   ------------- -------------------------- 34.9/99.8 MB 664.0 kB/s eta 0:01:38\n",
      "   -------------- ------------------------- 35.0/99.8 MB 664.7 kB/s eta 0:01:38\n",
      "   -------------- ------------------------- 35.0/99.8 MB 665.4 kB/s eta 0:01:38\n",
      "   -------------- ------------------------- 35.0/99.8 MB 666.7 kB/s eta 0:01:38\n",
      "   -------------- ------------------------- 35.1/99.8 MB 667.4 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.1/99.8 MB 667.4 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.2/99.8 MB 666.7 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.2/99.8 MB 666.7 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.2/99.8 MB 666.0 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.2/99.8 MB 664.7 kB/s eta 0:01:38\n",
      "   -------------- ------------------------- 35.3/99.8 MB 664.7 kB/s eta 0:01:38\n",
      "   -------------- ------------------------- 35.3/99.8 MB 664.7 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.3/99.8 MB 664.0 kB/s eta 0:01:38\n",
      "   -------------- ------------------------- 35.4/99.8 MB 664.7 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.4/99.8 MB 666.7 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.5/99.8 MB 667.4 kB/s eta 0:01:37\n",
      "   -------------- ------------------------- 35.5/99.8 MB 670.1 kB/s eta 0:01:36\n",
      "   -------------- ------------------------- 35.6/99.8 MB 670.1 kB/s eta 0:01:36\n",
      "   -------------- ------------------------- 35.6/99.8 MB 674.3 kB/s eta 0:01:36\n",
      "   -------------- ------------------------- 35.7/99.8 MB 673.5 kB/s eta 0:01:36\n",
      "   -------------- ------------------------- 35.7/99.8 MB 676.3 kB/s eta 0:01:35\n",
      "   -------------- ------------------------- 35.7/99.8 MB 677.1 kB/s eta 0:01:35\n",
      "   -------------- ------------------------- 35.8/99.8 MB 679.9 kB/s eta 0:01:35\n",
      "   -------------- ------------------------- 35.8/99.8 MB 681.3 kB/s eta 0:01:34\n",
      "   -------------- ------------------------- 35.9/99.8 MB 684.8 kB/s eta 0:01:34\n",
      "   -------------- ------------------------- 36.0/99.8 MB 687.0 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.0/99.8 MB 687.0 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.0/99.8 MB 687.7 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.1/99.8 MB 688.4 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.2/99.8 MB 690.6 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.2/99.8 MB 690.6 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.2/99.8 MB 687.7 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.2/99.8 MB 687.0 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.2/99.8 MB 687.7 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.3/99.8 MB 686.3 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.3/99.8 MB 689.9 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.4/99.8 MB 688.4 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.4/99.8 MB 687.7 kB/s eta 0:01:33\n",
      "   -------------- ------------------------- 36.4/99.8 MB 690.6 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.5/99.8 MB 692.1 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.5/99.8 MB 692.0 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.6/99.8 MB 692.8 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.6/99.8 MB 692.1 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.6/99.8 MB 692.0 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.6/99.8 MB 692.0 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.7/99.8 MB 689.9 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.7/99.8 MB 691.3 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.8/99.8 MB 692.1 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.8/99.8 MB 692.1 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.8/99.8 MB 691.3 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.8/99.8 MB 690.6 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.8/99.8 MB 690.6 kB/s eta 0:01:32\n",
      "   -------------- ------------------------- 36.9/99.8 MB 692.8 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.0/99.8 MB 692.8 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.0/99.8 MB 692.0 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.0/99.8 MB 692.1 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.0/99.8 MB 692.1 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.0/99.8 MB 689.9 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.1/99.8 MB 689.9 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.1/99.8 MB 689.9 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.2/99.8 MB 692.1 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.2/99.8 MB 693.5 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.2/99.8 MB 695.0 kB/s eta 0:01:30\n",
      "   -------------- ------------------------- 37.3/99.8 MB 695.8 kB/s eta 0:01:30\n",
      "   -------------- ------------------------- 37.3/99.8 MB 697.2 kB/s eta 0:01:30\n",
      "   -------------- ------------------------- 37.3/99.8 MB 699.5 kB/s eta 0:01:30\n",
      "   -------------- ------------------------- 37.4/99.8 MB 701.0 kB/s eta 0:01:30\n",
      "   -------------- ------------------------- 37.4/99.8 MB 698.7 kB/s eta 0:01:30\n",
      "   --------------- ------------------------ 37.4/99.8 MB 698.7 kB/s eta 0:01:30\n",
      "   --------------- ------------------------ 37.5/99.8 MB 699.5 kB/s eta 0:01:30\n",
      "   --------------- ------------------------ 37.5/99.8 MB 704.0 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 37.6/99.8 MB 701.7 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 37.6/99.8 MB 703.2 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 37.7/99.8 MB 705.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 37.7/99.8 MB 705.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 37.7/99.8 MB 701.7 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 37.7/99.8 MB 703.9 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 37.8/99.8 MB 705.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 37.8/99.8 MB 706.3 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 37.9/99.8 MB 707.0 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 37.9/99.8 MB 706.3 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 37.9/99.8 MB 705.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 37.9/99.8 MB 705.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.0/99.8 MB 705.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.0/99.8 MB 703.2 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.0/99.8 MB 703.3 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.1/99.8 MB 703.2 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.1/99.8 MB 703.2 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.1/99.8 MB 703.2 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.2/99.8 MB 704.0 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.2/99.8 MB 704.0 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.3/99.8 MB 705.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.3/99.8 MB 705.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 38.3/99.8 MB 707.0 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.4/99.8 MB 707.0 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.4/99.8 MB 707.0 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.5/99.8 MB 707.1 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.5/99.8 MB 707.8 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.6/99.8 MB 708.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.6/99.8 MB 708.6 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.6/99.8 MB 710.1 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.7/99.8 MB 708.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 38.7/99.8 MB 710.8 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 38.7/99.8 MB 710.8 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 38.8/99.8 MB 712.4 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 38.8/99.8 MB 711.6 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 38.8/99.8 MB 712.4 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 38.9/99.8 MB 713.2 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 38.9/99.8 MB 713.2 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 38.9/99.8 MB 711.6 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.0/99.8 MB 710.8 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.0/99.8 MB 710.1 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.0/99.8 MB 710.1 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.1/99.8 MB 709.3 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.1/99.8 MB 710.1 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.1/99.8 MB 709.3 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.2/99.8 MB 708.5 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.2/99.8 MB 711.6 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.2/99.8 MB 708.5 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.2/99.8 MB 706.2 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.2/99.8 MB 706.2 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.3/99.8 MB 707.1 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.3/99.8 MB 705.5 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.3/99.8 MB 705.5 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.3/99.8 MB 704.0 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.3/99.8 MB 704.0 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.4/99.8 MB 702.4 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.4/99.8 MB 703.2 kB/s eta 0:01:26\n",
      "   --------------- ------------------------ 39.4/99.8 MB 701.0 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.4/99.8 MB 701.0 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.4/99.8 MB 699.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.5/99.8 MB 699.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.5/99.8 MB 698.8 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.5/99.8 MB 698.0 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.5/99.8 MB 696.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.5/99.8 MB 697.2 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.6/99.8 MB 696.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.6/99.8 MB 695.0 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.6/99.8 MB 693.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.6/99.8 MB 693.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.6/99.8 MB 693.5 kB/s eta 0:01:27\n",
      "   --------------- ------------------------ 39.6/99.8 MB 688.4 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 39.7/99.8 MB 688.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 39.7/99.8 MB 686.3 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 39.7/99.8 MB 685.5 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 39.7/99.8 MB 684.9 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 39.7/99.8 MB 684.1 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 39.7/99.8 MB 682.0 kB/s eta 0:01:28\n",
      "   --------------- ------------------------ 39.8/99.8 MB 680.6 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 39.8/99.8 MB 680.5 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 39.8/99.8 MB 680.6 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 39.8/99.8 MB 680.6 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 39.8/99.8 MB 677.1 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 39.8/99.8 MB 675.6 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 39.8/99.8 MB 675.0 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 39.8/99.8 MB 675.0 kB/s eta 0:01:29\n",
      "   --------------- ------------------------ 39.9/99.8 MB 672.2 kB/s eta 0:01:30\n",
      "   ---------------- ----------------------- 39.9/99.8 MB 671.5 kB/s eta 0:01:30\n",
      "   ---------------- ----------------------- 39.9/99.8 MB 671.5 kB/s eta 0:01:30\n",
      "   ---------------- ----------------------- 39.9/99.8 MB 669.4 kB/s eta 0:01:30\n",
      "   ---------------- ----------------------- 39.9/99.8 MB 668.0 kB/s eta 0:01:30\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 666.7 kB/s eta 0:01:30\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 664.7 kB/s eta 0:01:30\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 664.7 kB/s eta 0:01:30\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 663.4 kB/s eta 0:01:31\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 662.0 kB/s eta 0:01:31\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 660.7 kB/s eta 0:01:31\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 659.3 kB/s eta 0:01:31\n",
      "   ---------------- ----------------------- 40.1/99.8 MB 658.7 kB/s eta 0:01:31\n",
      "   ---------------- ----------------------- 40.1/99.8 MB 657.4 kB/s eta 0:01:31\n",
      "   ---------------- ----------------------- 40.1/99.8 MB 656.0 kB/s eta 0:01:31\n",
      "   ---------------- ----------------------- 40.1/99.8 MB 655.3 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.1/99.8 MB 654.0 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.1/99.8 MB 653.4 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 654.1 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 652.7 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 651.5 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 650.8 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 650.8 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 648.2 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 649.5 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 648.2 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 649.5 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 647.0 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 648.2 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.4/99.8 MB 648.2 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.4/99.8 MB 649.5 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.4/99.8 MB 649.5 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.4/99.8 MB 648.8 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.5/99.8 MB 648.2 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.5/99.8 MB 647.6 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.5/99.8 MB 647.6 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.5/99.8 MB 646.9 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.6/99.8 MB 647.0 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.6/99.8 MB 645.7 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.6/99.8 MB 645.7 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.6/99.8 MB 645.7 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.7/99.8 MB 645.0 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.7/99.8 MB 644.4 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.7/99.8 MB 644.4 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.8/99.8 MB 643.7 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.8/99.8 MB 643.7 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.8/99.8 MB 643.7 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.8/99.8 MB 642.5 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.8/99.8 MB 642.5 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.9/99.8 MB 640.6 kB/s eta 0:01:32\n",
      "   ---------------- ----------------------- 40.9/99.8 MB 639.4 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 40.9/99.8 MB 638.8 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 40.9/99.8 MB 638.1 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 40.9/99.8 MB 636.9 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.0/99.8 MB 636.3 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.0/99.8 MB 636.3 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.0/99.8 MB 635.0 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.0/99.8 MB 634.4 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.0/99.8 MB 633.1 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.1/99.8 MB 633.8 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.1/99.8 MB 631.9 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.1/99.8 MB 631.3 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.1/99.8 MB 630.1 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.1/99.8 MB 630.1 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.2/99.8 MB 631.3 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.2/99.8 MB 630.1 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.2/99.8 MB 628.9 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.2/99.8 MB 627.7 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.2/99.8 MB 627.1 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.3/99.8 MB 626.5 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.3/99.8 MB 626.5 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.3/99.8 MB 624.7 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.3/99.8 MB 625.3 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.3/99.8 MB 624.1 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.4/99.8 MB 624.1 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.4/99.8 MB 623.5 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.4/99.8 MB 622.3 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 621.8 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 621.8 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 622.9 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 621.2 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.6/99.8 MB 620.5 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.6/99.8 MB 620.0 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.6/99.8 MB 619.4 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.7/99.8 MB 621.8 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.7/99.8 MB 620.6 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.7/99.8 MB 620.6 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.7/99.8 MB 620.5 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.8/99.8 MB 621.2 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.8/99.8 MB 621.2 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.8/99.8 MB 621.2 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.9/99.8 MB 620.6 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.9/99.8 MB 619.4 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 41.9/99.8 MB 622.9 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.9/99.8 MB 622.4 kB/s eta 0:01:33\n",
      "   ---------------- ----------------------- 41.9/99.8 MB 621.2 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.0/99.8 MB 620.6 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.0/99.8 MB 621.2 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.0/99.8 MB 619.4 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.0/99.8 MB 620.6 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.1/99.8 MB 620.0 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.1/99.8 MB 618.2 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.1/99.8 MB 619.4 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.1/99.8 MB 618.2 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.1/99.8 MB 618.8 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.1/99.8 MB 617.1 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.2/99.8 MB 617.6 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.2/99.8 MB 616.5 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.2/99.8 MB 616.5 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.3/99.8 MB 616.5 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.3/99.8 MB 615.3 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.3/99.8 MB 614.7 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.4/99.8 MB 614.8 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.4/99.8 MB 614.7 kB/s eta 0:01:34\n",
      "   ---------------- ----------------------- 42.4/99.8 MB 614.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.4/99.8 MB 616.5 kB/s eta 0:01:33\n",
      "   ----------------- ---------------------- 42.5/99.8 MB 614.8 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.5/99.8 MB 615.9 kB/s eta 0:01:33\n",
      "   ----------------- ---------------------- 42.5/99.8 MB 614.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.5/99.8 MB 614.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.6/99.8 MB 613.0 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.6/99.8 MB 612.5 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.6/99.8 MB 611.9 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.6/99.8 MB 611.9 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.6/99.8 MB 611.9 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.7/99.8 MB 610.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.7/99.8 MB 610.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.7/99.8 MB 610.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.7/99.8 MB 607.9 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 606.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 606.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 606.7 kB/s eta 0:01:34\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 603.4 kB/s eta 0:01:35\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 603.4 kB/s eta 0:01:35\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 601.8 kB/s eta 0:01:35\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 600.1 kB/s eta 0:01:35\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 600.7 kB/s eta 0:01:35\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 600.7 kB/s eta 0:01:35\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 600.7 kB/s eta 0:01:35\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 596.3 kB/s eta 0:01:36\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 595.7 kB/s eta 0:01:36\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 595.7 kB/s eta 0:01:36\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 595.8 kB/s eta 0:01:36\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 595.8 kB/s eta 0:01:36\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 593.0 kB/s eta 0:01:36\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 591.5 kB/s eta 0:01:37\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 589.3 kB/s eta 0:01:37\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 588.2 kB/s eta 0:01:37\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 588.2 kB/s eta 0:01:37\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 586.1 kB/s eta 0:01:37\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 586.1 kB/s eta 0:01:37\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 586.1 kB/s eta 0:01:37\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 583.5 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 584.6 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 582.5 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 581.5 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 581.5 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 581.4 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 580.4 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 580.4 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 578.9 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.2/99.8 MB 578.9 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.2/99.8 MB 578.4 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.2/99.8 MB 577.3 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.2/99.8 MB 577.3 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.2/99.8 MB 575.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.2/99.8 MB 574.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.3/99.8 MB 576.8 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.3/99.8 MB 575.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.3/99.8 MB 576.3 kB/s eta 0:01:38\n",
      "   ----------------- ---------------------- 43.3/99.8 MB 575.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.3/99.8 MB 575.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.3/99.8 MB 575.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 574.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 574.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 571.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 573.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 573.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 573.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.5/99.8 MB 571.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.5/99.8 MB 570.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.5/99.8 MB 570.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.5/99.8 MB 570.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.5/99.8 MB 571.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.5/99.8 MB 570.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 570.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 570.3 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 568.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 568.8 kB/s eta 0:01:39\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 566.8 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 564.4 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 564.4 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 563.9 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 563.9 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 562.9 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 562.9 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 562.9 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 558.6 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 558.6 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 557.2 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 556.7 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.8/99.8 MB 555.8 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.8/99.8 MB 554.8 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.8/99.8 MB 555.3 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.8/99.8 MB 553.9 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.8/99.8 MB 553.9 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.8/99.8 MB 552.0 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.8/99.8 MB 552.0 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.9/99.8 MB 550.6 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.9/99.8 MB 549.7 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.9/99.8 MB 549.2 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.9/99.8 MB 548.8 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.9/99.8 MB 548.8 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.9/99.8 MB 547.4 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 43.9/99.8 MB 547.4 kB/s eta 0:01:42\n",
      "   ----------------- ---------------------- 44.0/99.8 MB 545.1 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.0/99.8 MB 545.1 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.0/99.8 MB 542.9 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.0/99.8 MB 542.9 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.0/99.8 MB 542.9 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.1/99.8 MB 542.4 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.1/99.8 MB 541.5 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.1/99.8 MB 541.5 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.1/99.8 MB 541.5 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.1/99.8 MB 540.6 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 540.2 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 540.2 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 539.7 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 539.7 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 539.7 kB/s eta 0:01:43\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 538.4 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 538.4 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.3/99.8 MB 537.1 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.3/99.8 MB 536.2 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.3/99.8 MB 535.8 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.3/99.8 MB 535.8 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.4/99.8 MB 534.4 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.4/99.8 MB 534.4 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.4/99.8 MB 534.4 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.4/99.8 MB 533.1 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.4/99.8 MB 532.3 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.4/99.8 MB 532.3 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.5/99.8 MB 531.8 kB/s eta 0:01:44\n",
      "   ----------------- ---------------------- 44.5/99.8 MB 531.0 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.5/99.8 MB 531.0 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.5/99.8 MB 531.0 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.5/99.8 MB 528.8 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 527.6 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 527.1 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 526.7 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 525.9 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 525.4 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 525.4 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 525.4 kB/s eta 0:01:45\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 522.1 kB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 44.7/99.8 MB 522.1 kB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 44.7/99.8 MB 522.1 kB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 44.7/99.8 MB 520.0 kB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 44.7/99.8 MB 519.2 kB/s eta 0:01:46\n",
      "   ----------------- ---------------------- 44.7/99.8 MB 518.0 kB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 44.8/99.8 MB 518.0 kB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 44.8/99.8 MB 517.6 kB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 44.8/99.8 MB 517.6 kB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 44.8/99.8 MB 515.1 kB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 44.8/99.8 MB 515.1 kB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 44.9/99.8 MB 514.7 kB/s eta 0:01:47\n",
      "   ----------------- ---------------------- 44.9/99.8 MB 513.9 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 44.9/99.8 MB 514.3 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 44.9/99.8 MB 513.1 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 44.9/99.8 MB 513.1 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 44.9/99.8 MB 513.1 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 44.9/99.8 MB 513.1 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 45.0/99.8 MB 510.3 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.0/99.8 MB 510.3 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.0/99.8 MB 508.3 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.0/99.8 MB 507.9 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.1/99.8 MB 508.3 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.1/99.8 MB 507.5 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.1/99.8 MB 507.5 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.1/99.8 MB 507.5 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.2/99.8 MB 506.0 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.2/99.8 MB 505.2 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.2/99.8 MB 504.8 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.3/99.8 MB 504.4 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.3/99.8 MB 504.4 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.3/99.8 MB 504.4 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.3/99.8 MB 503.2 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.4/99.8 MB 503.2 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.4/99.8 MB 503.2 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.4/99.8 MB 502.8 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.5/99.8 MB 502.9 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.5/99.8 MB 503.2 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.5/99.8 MB 502.8 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.6/99.8 MB 503.2 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.6/99.8 MB 502.8 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.6/99.8 MB 502.5 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.6/99.8 MB 502.5 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.6/99.8 MB 502.5 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.7/99.8 MB 500.9 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 45.7/99.8 MB 499.8 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.7/99.8 MB 499.0 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.7/99.8 MB 499.0 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.8/99.8 MB 497.9 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.8/99.8 MB 497.5 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.8/99.8 MB 497.1 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.9/99.8 MB 497.1 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.9/99.8 MB 496.7 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 45.9/99.8 MB 496.7 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.0/99.8 MB 496.4 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.0/99.8 MB 496.0 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.0/99.8 MB 495.6 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.1/99.8 MB 495.2 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.1/99.8 MB 494.9 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.1/99.8 MB 494.9 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.2/99.8 MB 494.5 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.2/99.8 MB 494.1 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.2/99.8 MB 493.8 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.3/99.8 MB 493.4 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.3/99.8 MB 493.0 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.3/99.8 MB 492.6 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.4/99.8 MB 492.3 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.4/99.8 MB 493.7 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.4/99.8 MB 493.4 kB/s eta 0:01:49\n",
      "   ------------------ --------------------- 46.5/99.8 MB 493.4 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.5/99.8 MB 493.7 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.6/99.8 MB 493.4 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.6/99.8 MB 493.7 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.6/99.8 MB 493.8 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.7/99.8 MB 493.8 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.7/99.8 MB 493.7 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.8/99.8 MB 493.7 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.8/99.8 MB 493.4 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.8/99.8 MB 493.4 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.8/99.8 MB 493.8 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.9/99.8 MB 492.6 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.9/99.8 MB 493.4 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 46.9/99.8 MB 493.7 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 47.0/99.8 MB 493.0 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 47.0/99.8 MB 492.6 kB/s eta 0:01:48\n",
      "   ------------------ --------------------- 47.0/99.8 MB 493.8 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.1/99.8 MB 495.2 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.1/99.8 MB 494.1 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.1/99.8 MB 493.8 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.1/99.8 MB 493.8 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.2/99.8 MB 493.4 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.2/99.8 MB 493.4 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.2/99.8 MB 493.4 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.3/99.8 MB 494.1 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.3/99.8 MB 494.1 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.3/99.8 MB 492.6 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.3/99.8 MB 493.4 kB/s eta 0:01:47\n",
      "   ------------------ --------------------- 47.4/99.8 MB 492.6 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.4/99.8 MB 492.2 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.4/99.8 MB 492.3 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.4/99.8 MB 491.5 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.5/99.8 MB 490.8 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.5/99.8 MB 490.4 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.5/99.8 MB 490.8 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.5/99.8 MB 489.3 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.6/99.8 MB 489.7 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.6/99.8 MB 490.0 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.7/99.8 MB 490.0 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.7/99.8 MB 490.0 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.7/99.8 MB 489.7 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.7/99.8 MB 488.6 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.8/99.8 MB 488.6 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.8/99.8 MB 488.6 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.9/99.8 MB 488.2 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.9/99.8 MB 487.5 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 47.9/99.8 MB 489.3 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 47.9/99.8 MB 488.9 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 48.0/99.8 MB 488.2 kB/s eta 0:01:47\n",
      "   ------------------- -------------------- 48.0/99.8 MB 488.6 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 48.1/99.8 MB 488.6 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 48.1/99.8 MB 488.2 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 48.1/99.8 MB 488.9 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 48.2/99.8 MB 488.6 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 48.2/99.8 MB 490.0 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 48.2/99.8 MB 490.4 kB/s eta 0:01:46\n",
      "   ------------------- -------------------- 48.3/99.8 MB 490.4 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.3/99.8 MB 490.0 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.4/99.8 MB 490.4 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.4/99.8 MB 490.4 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.4/99.8 MB 491.5 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.4/99.8 MB 490.4 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.5/99.8 MB 489.7 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.5/99.8 MB 489.0 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.6/99.8 MB 489.3 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.6/99.8 MB 489.3 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.7/99.8 MB 489.7 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.7/99.8 MB 488.9 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.7/99.8 MB 489.7 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.8/99.8 MB 488.6 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.8/99.8 MB 489.3 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.8/99.8 MB 488.9 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.9/99.8 MB 488.6 kB/s eta 0:01:45\n",
      "   ------------------- -------------------- 48.9/99.8 MB 489.3 kB/s eta 0:01:44\n",
      "   ------------------- -------------------- 48.9/99.8 MB 488.6 kB/s eta 0:01:44\n",
      "   ------------------- -------------------- 49.0/99.8 MB 489.3 kB/s eta 0:01:44\n",
      "   ------------------- -------------------- 49.0/99.8 MB 488.6 kB/s eta 0:01:44\n",
      "   ------------------- -------------------- 49.1/99.8 MB 489.3 kB/s eta 0:01:44\n",
      "   ------------------- -------------------- 49.1/99.8 MB 491.2 kB/s eta 0:01:44\n",
      "   ------------------- -------------------- 49.2/99.8 MB 490.4 kB/s eta 0:01:44\n",
      "   ------------------- -------------------- 49.2/99.8 MB 491.2 kB/s eta 0:01:43\n",
      "   ------------------- -------------------- 49.2/99.8 MB 491.5 kB/s eta 0:01:43\n",
      "   ------------------- -------------------- 49.3/99.8 MB 492.3 kB/s eta 0:01:43\n",
      "   ------------------- -------------------- 49.3/99.8 MB 492.3 kB/s eta 0:01:43\n",
      "   ------------------- -------------------- 49.4/99.8 MB 493.4 kB/s eta 0:01:43\n",
      "   ------------------- -------------------- 49.4/99.8 MB 494.9 kB/s eta 0:01:42\n",
      "   ------------------- -------------------- 49.5/99.8 MB 496.0 kB/s eta 0:01:42\n",
      "   ------------------- -------------------- 49.5/99.8 MB 497.9 kB/s eta 0:01:41\n",
      "   ------------------- -------------------- 49.6/99.8 MB 499.4 kB/s eta 0:01:41\n",
      "   ------------------- -------------------- 49.6/99.8 MB 497.9 kB/s eta 0:01:41\n",
      "   ------------------- -------------------- 49.6/99.8 MB 499.8 kB/s eta 0:01:41\n",
      "   ------------------- -------------------- 49.7/99.8 MB 498.6 kB/s eta 0:01:41\n",
      "   ------------------- -------------------- 49.7/99.8 MB 499.4 kB/s eta 0:01:41\n",
      "   ------------------- -------------------- 49.7/99.8 MB 501.3 kB/s eta 0:01:40\n",
      "   ------------------- -------------------- 49.8/99.8 MB 502.1 kB/s eta 0:01:40\n",
      "   ------------------- -------------------- 49.8/99.8 MB 501.7 kB/s eta 0:01:40\n",
      "   ------------------- -------------------- 49.8/99.8 MB 502.8 kB/s eta 0:01:40\n",
      "   ------------------- -------------------- 49.9/99.8 MB 506.3 kB/s eta 0:01:39\n",
      "   -------------------- ------------------- 49.9/99.8 MB 507.5 kB/s eta 0:01:39\n",
      "   -------------------- ------------------- 50.0/99.8 MB 509.1 kB/s eta 0:01:38\n",
      "   -------------------- ------------------- 50.0/99.8 MB 509.9 kB/s eta 0:01:38\n",
      "   -------------------- ------------------- 50.0/99.8 MB 513.9 kB/s eta 0:01:37\n",
      "   -------------------- ------------------- 50.1/99.8 MB 517.1 kB/s eta 0:01:37\n",
      "   -------------------- ------------------- 50.1/99.8 MB 519.6 kB/s eta 0:01:36\n",
      "   -------------------- ------------------- 50.2/99.8 MB 522.5 kB/s eta 0:01:35\n",
      "   -------------------- ------------------- 50.2/99.8 MB 524.2 kB/s eta 0:01:35\n",
      "   -------------------- ------------------- 50.3/99.8 MB 526.7 kB/s eta 0:01:34\n",
      "   -------------------- ------------------- 50.3/99.8 MB 529.3 kB/s eta 0:01:34\n",
      "   -------------------- ------------------- 50.4/99.8 MB 531.4 kB/s eta 0:01:33\n",
      "   -------------------- ------------------- 50.4/99.8 MB 533.6 kB/s eta 0:01:33\n",
      "   -------------------- ------------------- 50.5/99.8 MB 534.9 kB/s eta 0:01:33\n",
      "   -------------------- ------------------- 50.5/99.8 MB 535.8 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 50.5/99.8 MB 535.8 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 50.6/99.8 MB 537.1 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 50.6/99.8 MB 538.8 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 50.6/99.8 MB 540.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.7/99.8 MB 540.6 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.7/99.8 MB 540.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.7/99.8 MB 540.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.7/99.8 MB 540.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.7/99.8 MB 540.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.7/99.8 MB 540.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.7/99.8 MB 540.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.8/99.8 MB 539.3 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.8/99.8 MB 539.3 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.8/99.8 MB 537.5 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 50.9/99.8 MB 537.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.9/99.8 MB 537.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 50.9/99.8 MB 537.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.0/99.8 MB 538.4 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.0/99.8 MB 538.9 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.0/99.8 MB 541.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.0/99.8 MB 541.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.0/99.8 MB 538.4 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.0/99.8 MB 537.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.1/99.8 MB 538.0 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.1/99.8 MB 537.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.1/99.8 MB 537.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.1/99.8 MB 537.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.1/99.8 MB 536.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.2/99.8 MB 537.1 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.2/99.8 MB 536.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.2/99.8 MB 535.3 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.2/99.8 MB 535.8 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.2/99.8 MB 536.2 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.3/99.8 MB 535.8 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.3/99.8 MB 537.1 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.3/99.8 MB 537.5 kB/s eta 0:01:31\n",
      "   -------------------- ------------------- 51.4/99.8 MB 538.4 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.4/99.8 MB 539.3 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.4/99.8 MB 539.7 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.4/99.8 MB 542.0 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.5/99.8 MB 541.1 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.5/99.8 MB 541.5 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.5/99.8 MB 541.5 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.5/99.8 MB 541.5 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.5/99.8 MB 538.4 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.6/99.8 MB 541.1 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.6/99.8 MB 541.1 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.6/99.8 MB 541.1 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 51.6/99.8 MB 540.2 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.6/99.8 MB 540.2 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.7/99.8 MB 541.1 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 51.7/99.8 MB 540.2 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 51.7/99.8 MB 540.2 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 51.7/99.8 MB 538.4 kB/s eta 0:01:30\n",
      "   -------------------- ------------------- 51.8/99.8 MB 539.3 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 51.8/99.8 MB 539.7 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 51.9/99.8 MB 539.7 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 51.9/99.8 MB 541.1 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 51.9/99.8 MB 539.7 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.0/99.8 MB 541.5 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.0/99.8 MB 541.5 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.0/99.8 MB 541.5 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.0/99.8 MB 538.0 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.0/99.8 MB 540.2 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.0/99.8 MB 538.4 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.1/99.8 MB 539.7 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.1/99.8 MB 540.6 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.1/99.8 MB 540.6 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.2/99.8 MB 539.3 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.2/99.8 MB 540.6 kB/s eta 0:01:28\n",
      "   -------------------- ------------------- 52.2/99.8 MB 540.6 kB/s eta 0:01:28\n",
      "   -------------------- ------------------- 52.2/99.8 MB 540.6 kB/s eta 0:01:28\n",
      "   -------------------- ------------------- 52.2/99.8 MB 538.9 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.3/99.8 MB 538.0 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.3/99.8 MB 538.0 kB/s eta 0:01:29\n",
      "   -------------------- ------------------- 52.3/99.8 MB 539.7 kB/s eta 0:01:28\n",
      "   -------------------- ------------------- 52.3/99.8 MB 539.7 kB/s eta 0:01:28\n",
      "   -------------------- ------------------- 52.3/99.8 MB 539.7 kB/s eta 0:01:28\n",
      "   -------------------- ------------------- 52.4/99.8 MB 538.0 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.4/99.8 MB 538.9 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 52.4/99.8 MB 539.3 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 52.4/99.8 MB 539.3 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 52.5/99.8 MB 537.5 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 52.5/99.8 MB 537.5 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 52.5/99.8 MB 537.5 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 52.5/99.8 MB 537.1 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 52.5/99.8 MB 537.1 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 52.6/99.8 MB 535.3 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.6/99.8 MB 535.3 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.6/99.8 MB 534.9 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.6/99.8 MB 534.9 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.6/99.8 MB 534.9 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.6/99.8 MB 532.7 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.6/99.8 MB 532.7 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.7/99.8 MB 531.9 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.7/99.8 MB 531.4 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.7/99.8 MB 529.7 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.7/99.8 MB 529.7 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.7/99.8 MB 529.7 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.8/99.8 MB 531.0 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.8/99.8 MB 530.6 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.8/99.8 MB 530.6 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.8/99.8 MB 528.0 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.8/99.8 MB 527.6 kB/s eta 0:01:30\n",
      "   --------------------- ------------------ 52.8/99.8 MB 528.4 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.8/99.8 MB 526.7 kB/s eta 0:01:30\n",
      "   --------------------- ------------------ 52.9/99.8 MB 525.9 kB/s eta 0:01:30\n",
      "   --------------------- ------------------ 52.9/99.8 MB 527.6 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 52.9/99.8 MB 525.9 kB/s eta 0:01:30\n",
      "   --------------------- ------------------ 53.0/99.8 MB 528.4 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 53.0/99.8 MB 528.0 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 53.0/99.8 MB 530.6 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 53.0/99.8 MB 530.6 kB/s eta 0:01:29\n",
      "   --------------------- ------------------ 53.0/99.8 MB 531.9 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 53.0/99.8 MB 531.9 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 53.0/99.8 MB 531.9 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 53.1/99.8 MB 533.6 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 53.1/99.8 MB 535.8 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 53.1/99.8 MB 534.9 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 53.1/99.8 MB 534.9 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 53.2/99.8 MB 538.0 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.2/99.8 MB 538.0 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.2/99.8 MB 538.0 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.2/99.8 MB 537.5 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.2/99.8 MB 538.9 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.2/99.8 MB 538.9 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.2/99.8 MB 538.4 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.3/99.8 MB 537.1 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.3/99.8 MB 537.1 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.3/99.8 MB 539.7 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.3/99.8 MB 540.2 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.3/99.8 MB 540.2 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.3/99.8 MB 540.2 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.3/99.8 MB 540.2 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.4/99.8 MB 534.9 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.4/99.8 MB 538.4 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.4/99.8 MB 538.4 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.4/99.8 MB 535.8 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.4/99.8 MB 535.8 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.4/99.8 MB 535.8 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.4/99.8 MB 533.6 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.4/99.8 MB 533.6 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.5/99.8 MB 536.6 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.5/99.8 MB 536.6 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.5/99.8 MB 535.3 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.5/99.8 MB 535.3 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.5/99.8 MB 533.1 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.5/99.8 MB 533.6 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.5/99.8 MB 533.6 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.6/99.8 MB 533.1 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.6/99.8 MB 533.2 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.6/99.8 MB 531.9 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.6/99.8 MB 533.1 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.6/99.8 MB 532.3 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.6/99.8 MB 531.4 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.7/99.8 MB 533.2 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.7/99.8 MB 531.0 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.7/99.8 MB 534.0 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.7/99.8 MB 533.2 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.7/99.8 MB 534.5 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.8/99.8 MB 534.0 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.8/99.8 MB 534.0 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.8/99.8 MB 534.0 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.8/99.8 MB 532.3 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.8/99.8 MB 532.3 kB/s eta 0:01:27\n",
      "   --------------------- ------------------ 53.8/99.8 MB 536.6 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.8/99.8 MB 536.2 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.9/99.8 MB 535.8 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.9/99.8 MB 536.7 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.9/99.8 MB 535.8 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.9/99.8 MB 535.8 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 53.9/99.8 MB 535.8 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 54.0/99.8 MB 538.8 kB/s eta 0:01:25\n",
      "   --------------------- ------------------ 54.0/99.8 MB 538.8 kB/s eta 0:01:25\n",
      "   --------------------- ------------------ 54.0/99.8 MB 538.8 kB/s eta 0:01:25\n",
      "   --------------------- ------------------ 54.0/99.8 MB 535.8 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 54.0/99.8 MB 535.8 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 54.0/99.8 MB 536.2 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 54.0/99.8 MB 535.8 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 54.0/99.8 MB 536.6 kB/s eta 0:01:26\n",
      "   --------------------- ------------------ 54.1/99.8 MB 537.5 kB/s eta 0:01:25\n",
      "   --------------------- ------------------ 54.1/99.8 MB 539.7 kB/s eta 0:01:25\n",
      "   --------------------- ------------------ 54.1/99.8 MB 541.5 kB/s eta 0:01:25\n",
      "   --------------------- ------------------ 54.1/99.8 MB 540.6 kB/s eta 0:01:25\n",
      "   --------------------- ------------------ 54.2/99.8 MB 541.5 kB/s eta 0:01:25\n",
      "   --------------------- ------------------ 54.2/99.8 MB 542.9 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.2/99.8 MB 543.8 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.3/99.8 MB 544.7 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.3/99.8 MB 544.7 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.3/99.8 MB 544.7 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.3/99.8 MB 543.8 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.3/99.8 MB 545.6 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.3/99.8 MB 545.6 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.4/99.8 MB 545.6 kB/s eta 0:01:24\n",
      "   --------------------- ------------------ 54.4/99.8 MB 547.0 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.4/99.8 MB 549.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.5/99.8 MB 548.8 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.5/99.8 MB 549.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.5/99.8 MB 548.3 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.5/99.8 MB 549.7 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.5/99.8 MB 549.7 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.6/99.8 MB 549.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.6/99.8 MB 549.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.6/99.8 MB 548.8 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.6/99.8 MB 550.2 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 54.7/99.8 MB 549.7 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 54.7/99.8 MB 550.6 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 54.7/99.8 MB 554.8 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 54.8/99.8 MB 553.9 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 54.8/99.8 MB 556.3 kB/s eta 0:01:21\n",
      "   --------------------- ------------------ 54.8/99.8 MB 556.7 kB/s eta 0:01:21\n",
      "   ---------------------- ----------------- 54.9/99.8 MB 561.5 kB/s eta 0:01:20\n",
      "   ---------------------- ----------------- 54.9/99.8 MB 560.5 kB/s eta 0:01:21\n",
      "   ---------------------- ----------------- 54.9/99.8 MB 562.4 kB/s eta 0:01:20\n",
      "   ---------------------- ----------------- 55.0/99.8 MB 564.4 kB/s eta 0:01:20\n",
      "   ---------------------- ----------------- 55.0/99.8 MB 568.3 kB/s eta 0:01:19\n",
      "   ---------------------- ----------------- 55.1/99.8 MB 567.3 kB/s eta 0:01:19\n",
      "   ---------------------- ----------------- 55.1/99.8 MB 567.8 kB/s eta 0:01:19\n",
      "   ---------------------- ----------------- 55.1/99.8 MB 568.8 kB/s eta 0:01:19\n",
      "   ---------------------- ----------------- 55.1/99.8 MB 570.3 kB/s eta 0:01:19\n",
      "   ---------------------- ----------------- 55.2/99.8 MB 568.8 kB/s eta 0:01:19\n",
      "   ---------------------- ----------------- 55.2/99.8 MB 572.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.2/99.8 MB 571.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.3/99.8 MB 573.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.3/99.8 MB 573.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.3/99.8 MB 574.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.3/99.8 MB 573.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.4/99.8 MB 575.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.4/99.8 MB 573.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.4/99.8 MB 574.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.4/99.8 MB 574.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.5/99.8 MB 574.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.5/99.8 MB 574.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.5/99.8 MB 573.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.5/99.8 MB 573.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.5/99.8 MB 573.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.5/99.8 MB 573.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.6/99.8 MB 573.3 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 55.6/99.8 MB 573.8 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 55.7/99.8 MB 572.8 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 55.7/99.8 MB 571.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.7/99.8 MB 571.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.7/99.8 MB 571.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.8/99.8 MB 570.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.8/99.8 MB 570.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.8/99.8 MB 569.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.8/99.8 MB 569.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.8/99.8 MB 569.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 55.8/99.8 MB 571.3 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 55.9/99.8 MB 570.8 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 55.9/99.8 MB 569.8 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 55.9/99.8 MB 569.8 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 55.9/99.8 MB 567.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.0/99.8 MB 568.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.0/99.8 MB 567.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.0/99.8 MB 567.8 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.0/99.8 MB 567.3 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.0/99.8 MB 566.4 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.1/99.8 MB 565.9 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.1/99.8 MB 565.4 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.1/99.8 MB 564.4 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.1/99.8 MB 565.4 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.1/99.8 MB 564.4 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.2/99.8 MB 564.4 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.2/99.8 MB 563.4 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.2/99.8 MB 563.9 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.3/99.8 MB 563.9 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.3/99.8 MB 562.9 kB/s eta 0:01:18\n",
      "   ---------------------- ----------------- 56.3/99.8 MB 563.9 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.4/99.8 MB 563.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.4/99.8 MB 563.9 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.4/99.8 MB 563.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.5/99.8 MB 563.9 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.5/99.8 MB 564.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.6/99.8 MB 564.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.6/99.8 MB 564.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.6/99.8 MB 562.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.6/99.8 MB 562.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.7/99.8 MB 562.9 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.7/99.8 MB 562.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.7/99.8 MB 562.9 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.8/99.8 MB 563.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.8/99.8 MB 563.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.8/99.8 MB 563.4 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.9/99.8 MB 561.0 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.9/99.8 MB 561.5 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 56.9/99.8 MB 561.0 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 57.0/99.8 MB 561.0 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 57.0/99.8 MB 560.5 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 57.0/99.8 MB 562.0 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 57.1/99.8 MB 560.5 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 57.1/99.8 MB 562.0 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.1/99.8 MB 562.0 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.1/99.8 MB 562.0 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.2/99.8 MB 561.5 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.2/99.8 MB 561.0 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.2/99.8 MB 560.5 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.3/99.8 MB 560.5 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.3/99.8 MB 560.5 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.3/99.8 MB 560.5 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 57.3/99.8 MB 560.5 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.4/99.8 MB 561.0 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.4/99.8 MB 560.5 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.4/99.8 MB 557.7 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.4/99.8 MB 556.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.5/99.8 MB 556.2 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.5/99.8 MB 557.7 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.5/99.8 MB 557.2 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.5/99.8 MB 557.2 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.5/99.8 MB 554.8 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 555.8 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 555.3 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 555.3 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 553.9 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 554.4 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 554.4 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 554.4 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.7/99.8 MB 553.4 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.7/99.8 MB 552.5 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.7/99.8 MB 552.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.7/99.8 MB 551.1 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.8/99.8 MB 552.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.8/99.8 MB 552.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.8/99.8 MB 550.6 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.8/99.8 MB 551.6 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.9/99.8 MB 551.6 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 57.9/99.8 MB 550.2 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.9/99.8 MB 550.2 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 57.9/99.8 MB 550.2 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 550.2 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 550.2 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 548.3 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 547.9 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 547.9 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.1/99.8 MB 547.4 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.1/99.8 MB 547.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.1/99.8 MB 547.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.1/99.8 MB 545.6 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.2/99.8 MB 545.6 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.2/99.8 MB 544.2 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.2/99.8 MB 544.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.2/99.8 MB 543.8 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.2/99.8 MB 542.9 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.3/99.8 MB 542.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.3/99.8 MB 542.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.3/99.8 MB 541.1 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.3/99.8 MB 541.1 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.4/99.8 MB 539.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.4/99.8 MB 539.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.4/99.8 MB 539.3 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.4/99.8 MB 539.3 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.4/99.8 MB 538.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.5/99.8 MB 538.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.5/99.8 MB 537.5 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.5/99.8 MB 535.8 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.5/99.8 MB 535.3 kB/s eta 0:01:18\n",
      "   ----------------------- ---------------- 58.6/99.8 MB 535.3 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.6/99.8 MB 534.4 kB/s eta 0:01:18\n",
      "   ----------------------- ---------------- 58.6/99.8 MB 533.6 kB/s eta 0:01:18\n",
      "   ----------------------- ---------------- 58.6/99.8 MB 534.9 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.6/99.8 MB 534.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.7/99.8 MB 534.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.7/99.8 MB 533.6 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.7/99.8 MB 533.6 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.7/99.8 MB 533.2 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.8/99.8 MB 532.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.8/99.8 MB 531.8 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.8/99.8 MB 531.8 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.9/99.8 MB 531.4 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.9/99.8 MB 531.4 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 58.9/99.8 MB 531.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.0/99.8 MB 530.6 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.0/99.8 MB 530.6 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.0/99.8 MB 529.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.1/99.8 MB 529.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.1/99.8 MB 530.5 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.1/99.8 MB 529.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.1/99.8 MB 529.3 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.2/99.8 MB 530.1 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.2/99.8 MB 528.9 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.2/99.8 MB 529.3 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.3/99.8 MB 528.8 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.3/99.8 MB 528.4 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.4/99.8 MB 528.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.4/99.8 MB 528.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.4/99.8 MB 528.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.5/99.8 MB 527.1 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.5/99.8 MB 526.7 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.5/99.8 MB 526.3 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.6/99.8 MB 526.3 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.6/99.8 MB 525.9 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.6/99.8 MB 525.9 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.7/99.8 MB 525.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.7/99.8 MB 525.0 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.8/99.8 MB 525.4 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.8/99.8 MB 524.2 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.8/99.8 MB 525.4 kB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 59.8/99.8 MB 524.2 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 59.9/99.8 MB 525.9 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 59.9/99.8 MB 525.4 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.0/99.8 MB 525.4 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.0/99.8 MB 526.3 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.0/99.8 MB 526.7 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.1/99.8 MB 526.7 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.1/99.8 MB 525.9 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.1/99.8 MB 525.8 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.2/99.8 MB 524.2 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.2/99.8 MB 525.0 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.2/99.8 MB 524.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.3/99.8 MB 524.2 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.3/99.8 MB 524.2 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.3/99.8 MB 522.9 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.4/99.8 MB 522.1 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.4/99.8 MB 521.7 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.4/99.8 MB 521.3 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.4/99.8 MB 520.9 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.5/99.8 MB 520.0 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.5/99.8 MB 519.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.5/99.8 MB 519.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.5/99.8 MB 518.4 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.6/99.8 MB 518.8 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.6/99.8 MB 518.4 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.6/99.8 MB 518.4 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.7/99.8 MB 517.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.7/99.8 MB 517.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.7/99.8 MB 517.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.7/99.8 MB 517.1 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.7/99.8 MB 517.1 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.8/99.8 MB 514.7 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.8/99.8 MB 514.7 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.8/99.8 MB 513.9 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.8/99.8 MB 513.9 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.8/99.8 MB 512.3 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 60.9/99.8 MB 511.5 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 60.9/99.8 MB 511.5 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 60.9/99.8 MB 510.3 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 60.9/99.8 MB 510.3 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 60.9/99.8 MB 509.9 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 60.9/99.8 MB 509.5 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 60.9/99.8 MB 513.5 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.0/99.8 MB 512.3 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.0/99.8 MB 510.7 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.0/99.8 MB 510.7 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.0/99.8 MB 508.7 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.0/99.8 MB 507.9 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.0/99.8 MB 507.1 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.1/99.8 MB 507.9 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.1/99.8 MB 507.1 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.1/99.8 MB 507.1 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.1/99.8 MB 506.3 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.1/99.8 MB 506.3 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.1/99.8 MB 505.5 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.1/99.8 MB 504.8 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.2/99.8 MB 504.4 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.2/99.8 MB 503.6 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.2/99.8 MB 502.9 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.2/99.8 MB 502.9 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.2/99.8 MB 501.7 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.3/99.8 MB 503.2 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.3/99.8 MB 503.6 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.3/99.8 MB 502.8 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.3/99.8 MB 504.8 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.4/99.8 MB 504.4 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.4/99.8 MB 504.4 kB/s eta 0:01:17\n",
      "   ------------------------ --------------- 61.4/99.8 MB 505.2 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.4/99.8 MB 506.3 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.5/99.8 MB 505.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.5/99.8 MB 506.3 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.5/99.8 MB 506.7 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.5/99.8 MB 505.9 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.6/99.8 MB 507.1 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.6/99.8 MB 505.5 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.6/99.8 MB 505.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.6/99.8 MB 505.2 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.6/99.8 MB 504.0 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.7/99.8 MB 505.5 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.7/99.8 MB 505.5 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.7/99.8 MB 504.0 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.7/99.8 MB 507.1 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 61.8/99.8 MB 506.7 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 61.8/99.8 MB 505.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.8/99.8 MB 504.8 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.8/99.8 MB 503.6 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.8/99.8 MB 504.4 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.8/99.8 MB 504.0 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.9/99.8 MB 504.8 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.9/99.8 MB 504.4 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.9/99.8 MB 503.2 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 61.9/99.8 MB 502.5 kB/s eta 0:01:16\n",
      "   ------------------------ --------------- 62.0/99.8 MB 504.4 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.0/99.8 MB 504.8 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.0/99.8 MB 504.0 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.1/99.8 MB 503.2 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.1/99.8 MB 503.2 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.1/99.8 MB 502.4 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.1/99.8 MB 502.1 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.1/99.8 MB 502.1 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.2/99.8 MB 501.7 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.2/99.8 MB 501.7 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.2/99.8 MB 500.9 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.2/99.8 MB 502.5 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.2/99.8 MB 502.1 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.3/99.8 MB 501.3 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.3/99.8 MB 501.7 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.3/99.8 MB 500.9 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.3/99.8 MB 499.8 kB/s eta 0:01:15\n",
      "   ------------------------ --------------- 62.3/99.8 MB 499.4 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.4/99.8 MB 500.5 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.4/99.8 MB 499.8 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.4/99.8 MB 499.8 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.4/99.8 MB 498.6 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.4/99.8 MB 497.1 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.4/99.8 MB 500.5 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.5/99.8 MB 500.1 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.5/99.8 MB 499.0 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.5/99.8 MB 499.8 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.5/99.8 MB 498.3 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.5/99.8 MB 497.5 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.5/99.8 MB 497.1 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.6/99.8 MB 495.2 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.6/99.8 MB 497.5 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.6/99.8 MB 496.4 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.6/99.8 MB 496.0 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.6/99.8 MB 495.6 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.6/99.8 MB 495.6 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.7/99.8 MB 494.9 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.7/99.8 MB 495.6 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.7/99.8 MB 494.5 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.7/99.8 MB 496.4 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.7/99.8 MB 495.6 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.7/99.8 MB 494.1 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.8/99.8 MB 493.0 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.8/99.8 MB 493.0 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.8/99.8 MB 493.0 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.8/99.8 MB 492.3 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.8/99.8 MB 493.7 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.8/99.8 MB 492.3 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.8/99.8 MB 491.5 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.8/99.8 MB 490.4 kB/s eta 0:01:16\n",
      "   ------------------------- -------------- 62.9/99.8 MB 492.3 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.9/99.8 MB 492.3 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.9/99.8 MB 493.4 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.9/99.8 MB 494.1 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 62.9/99.8 MB 494.1 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 63.0/99.8 MB 494.9 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 63.0/99.8 MB 493.7 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 63.0/99.8 MB 495.6 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 63.0/99.8 MB 496.0 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 63.1/99.8 MB 495.2 kB/s eta 0:01:15\n",
      "   ------------------------- -------------- 63.1/99.8 MB 496.7 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.1/99.8 MB 496.4 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.1/99.8 MB 497.1 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.2/99.8 MB 496.7 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.2/99.8 MB 496.7 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.2/99.8 MB 496.7 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.2/99.8 MB 496.0 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.2/99.8 MB 495.2 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.3/99.8 MB 496.0 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.3/99.8 MB 498.6 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.3/99.8 MB 497.1 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.3/99.8 MB 497.9 kB/s eta 0:01:14\n",
      "   ------------------------- -------------- 63.4/99.8 MB 498.6 kB/s eta 0:01:13\n",
      "   ------------------------- -------------- 63.4/99.8 MB 498.3 kB/s eta 0:01:13\n",
      "   ------------------------- -------------- 63.4/99.8 MB 501.3 kB/s eta 0:01:13\n",
      "   ------------------------- -------------- 63.5/99.8 MB 502.5 kB/s eta 0:01:13\n",
      "   ------------------------- -------------- 63.5/99.8 MB 504.4 kB/s eta 0:01:12\n",
      "   ------------------------- -------------- 63.5/99.8 MB 503.6 kB/s eta 0:01:12\n",
      "   ------------------------- -------------- 63.6/99.8 MB 508.7 kB/s eta 0:01:12\n",
      "   ------------------------- -------------- 63.6/99.8 MB 508.7 kB/s eta 0:01:12\n",
      "   ------------------------- -------------- 63.6/99.8 MB 507.5 kB/s eta 0:01:12\n",
      "   ------------------------- -------------- 63.6/99.8 MB 510.3 kB/s eta 0:01:11\n",
      "   ------------------------- -------------- 63.7/99.8 MB 512.7 kB/s eta 0:01:11\n",
      "   ------------------------- -------------- 63.7/99.8 MB 511.9 kB/s eta 0:01:11\n",
      "   ------------------------- -------------- 63.7/99.8 MB 513.5 kB/s eta 0:01:11\n",
      "   ------------------------- -------------- 63.8/99.8 MB 518.0 kB/s eta 0:01:10\n",
      "   ------------------------- -------------- 63.8/99.8 MB 516.7 kB/s eta 0:01:10\n",
      "   ------------------------- -------------- 63.8/99.8 MB 517.5 kB/s eta 0:01:10\n",
      "   ------------------------- -------------- 63.9/99.8 MB 519.2 kB/s eta 0:01:10\n",
      "   ------------------------- -------------- 63.9/99.8 MB 520.0 kB/s eta 0:01:09\n",
      "   ------------------------- -------------- 63.9/99.8 MB 521.7 kB/s eta 0:01:09\n",
      "   ------------------------- -------------- 63.9/99.8 MB 523.3 kB/s eta 0:01:09\n",
      "   ------------------------- -------------- 64.0/99.8 MB 522.5 kB/s eta 0:01:09\n",
      "   ------------------------- -------------- 64.0/99.8 MB 521.3 kB/s eta 0:01:09\n",
      "   ------------------------- -------------- 64.0/99.8 MB 526.3 kB/s eta 0:01:08\n",
      "   ------------------------- -------------- 64.1/99.8 MB 525.9 kB/s eta 0:01:08\n",
      "   ------------------------- -------------- 64.1/99.8 MB 528.0 kB/s eta 0:01:08\n",
      "   ------------------------- -------------- 64.1/99.8 MB 531.4 kB/s eta 0:01:08\n",
      "   ------------------------- -------------- 64.2/99.8 MB 531.0 kB/s eta 0:01:08\n",
      "   ------------------------- -------------- 64.2/99.8 MB 536.2 kB/s eta 0:01:07\n",
      "   ------------------------- -------------- 64.2/99.8 MB 535.3 kB/s eta 0:01:07\n",
      "   ------------------------- -------------- 64.3/99.8 MB 535.3 kB/s eta 0:01:07\n",
      "   ------------------------- -------------- 64.3/99.8 MB 536.2 kB/s eta 0:01:07\n",
      "   ------------------------- -------------- 64.3/99.8 MB 536.2 kB/s eta 0:01:07\n",
      "   ------------------------- -------------- 64.3/99.8 MB 535.3 kB/s eta 0:01:07\n",
      "   ------------------------- -------------- 64.4/99.8 MB 536.7 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.4/99.8 MB 536.2 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.5/99.8 MB 538.0 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.5/99.8 MB 538.0 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.5/99.8 MB 538.0 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.5/99.8 MB 539.7 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.6/99.8 MB 539.7 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.6/99.8 MB 538.4 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.6/99.8 MB 538.0 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.6/99.8 MB 536.6 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.6/99.8 MB 537.5 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.6/99.8 MB 535.8 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.7/99.8 MB 536.2 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.7/99.8 MB 534.9 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.7/99.8 MB 536.2 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.7/99.8 MB 536.2 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.7/99.8 MB 533.2 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.8/99.8 MB 534.5 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.8/99.8 MB 534.5 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.8/99.8 MB 534.9 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.8/99.8 MB 534.4 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.8/99.8 MB 533.6 kB/s eta 0:01:06\n",
      "   ------------------------- -------------- 64.8/99.8 MB 533.6 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 64.8/99.8 MB 533.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 64.9/99.8 MB 532.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 64.9/99.8 MB 532.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 64.9/99.8 MB 531.4 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 64.9/99.8 MB 531.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 64.9/99.8 MB 531.0 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.0/99.8 MB 530.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.0/99.8 MB 528.8 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.0/99.8 MB 529.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.0/99.8 MB 528.0 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.0/99.8 MB 528.0 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.1/99.8 MB 527.6 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.1/99.8 MB 526.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.1/99.8 MB 526.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.1/99.8 MB 526.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.2/99.8 MB 525.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.2/99.8 MB 525.4 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.2/99.8 MB 524.6 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.2/99.8 MB 524.2 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.2/99.8 MB 523.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.3/99.8 MB 522.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.3/99.8 MB 522.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.3/99.8 MB 522.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.3/99.8 MB 522.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.4/99.8 MB 522.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.4/99.8 MB 521.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.4/99.8 MB 522.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.4/99.8 MB 522.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.5/99.8 MB 520.8 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.5/99.8 MB 521.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.5/99.8 MB 520.8 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.5/99.8 MB 520.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.6/99.8 MB 520.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.6/99.8 MB 520.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.6/99.8 MB 520.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.6/99.8 MB 519.6 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.6/99.8 MB 520.4 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.7/99.8 MB 520.4 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.7/99.8 MB 519.6 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.7/99.8 MB 519.2 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.7/99.8 MB 518.8 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.8/99.8 MB 519.2 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.8/99.8 MB 521.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.8/99.8 MB 520.4 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.8/99.8 MB 519.2 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.8/99.8 MB 519.2 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.8/99.8 MB 518.0 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.8/99.8 MB 518.0 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.9/99.8 MB 518.8 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.9/99.8 MB 518.0 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.9/99.8 MB 517.6 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.9/99.8 MB 517.6 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 65.9/99.8 MB 517.6 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.0/99.8 MB 515.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.0/99.8 MB 515.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.0/99.8 MB 515.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.0/99.8 MB 515.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.0/99.8 MB 513.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.0/99.8 MB 513.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.1/99.8 MB 513.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.1/99.8 MB 512.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.1/99.8 MB 513.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.1/99.8 MB 512.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.1/99.8 MB 512.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.2/99.8 MB 511.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.2/99.8 MB 513.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.2/99.8 MB 512.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.2/99.8 MB 514.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.3/99.8 MB 514.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.3/99.8 MB 514.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.3/99.8 MB 513.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.3/99.8 MB 513.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.3/99.8 MB 513.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.4/99.8 MB 513.9 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.4/99.8 MB 513.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.4/99.8 MB 513.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.4/99.8 MB 513.1 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.4/99.8 MB 512.7 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.4/99.8 MB 512.7 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.5/99.8 MB 511.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.5/99.8 MB 510.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.5/99.8 MB 510.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.5/99.8 MB 510.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.6/99.8 MB 509.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.6/99.8 MB 509.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.6/99.8 MB 509.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.6/99.8 MB 508.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.7/99.8 MB 508.3 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.7/99.8 MB 507.9 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.7/99.8 MB 507.5 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.7/99.8 MB 507.2 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.8/99.8 MB 507.1 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.8/99.8 MB 506.7 kB/s eta 0:01:06\n",
      "   -------------------------- ------------- 66.8/99.8 MB 507.9 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.8/99.8 MB 507.1 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.9/99.8 MB 506.7 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.9/99.8 MB 507.1 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.9/99.8 MB 506.7 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 66.9/99.8 MB 505.9 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.0/99.8 MB 506.0 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.0/99.8 MB 505.2 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.0/99.8 MB 504.4 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.1/99.8 MB 505.9 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.1/99.8 MB 505.9 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.1/99.8 MB 505.6 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.2/99.8 MB 505.9 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.2/99.8 MB 505.5 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.2/99.8 MB 505.9 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.3/99.8 MB 505.5 kB/s eta 0:01:05\n",
      "   -------------------------- ------------- 67.3/99.8 MB 505.5 kB/s eta 0:01:05\n",
      "   --------------------------- ------------ 67.3/99.8 MB 506.3 kB/s eta 0:01:04\n",
      "   --------------------------- ------------ 67.4/99.8 MB 505.6 kB/s eta 0:01:05\n",
      "   --------------------------- ------------ 67.4/99.8 MB 507.1 kB/s eta 0:01:04\n",
      "   --------------------------- ------------ 67.5/99.8 MB 506.7 kB/s eta 0:01:04\n",
      "   --------------------------- ------------ 67.5/99.8 MB 506.7 kB/s eta 0:01:04\n",
      "   --------------------------- ------------ 67.5/99.8 MB 508.7 kB/s eta 0:01:04\n",
      "   --------------------------- ------------ 67.6/99.8 MB 507.1 kB/s eta 0:01:04\n",
      "   --------------------------- ------------ 67.6/99.8 MB 508.3 kB/s eta 0:01:04\n",
      "   --------------------------- ------------ 67.6/99.8 MB 507.5 kB/s eta 0:01:04\n",
      "   --------------------------- ------------ 67.7/99.8 MB 509.5 kB/s eta 0:01:03\n",
      "   --------------------------- ------------ 67.7/99.8 MB 510.3 kB/s eta 0:01:03\n",
      "   --------------------------- ------------ 67.7/99.8 MB 513.1 kB/s eta 0:01:03\n",
      "   --------------------------- ------------ 67.8/99.8 MB 513.1 kB/s eta 0:01:03\n",
      "   --------------------------- ------------ 67.8/99.8 MB 515.5 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 67.8/99.8 MB 515.5 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 67.9/99.8 MB 514.7 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 67.9/99.8 MB 517.1 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 67.9/99.8 MB 518.0 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 67.9/99.8 MB 518.0 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 68.0/99.8 MB 518.0 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 68.0/99.8 MB 518.0 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 68.0/99.8 MB 517.5 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 68.1/99.8 MB 517.1 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 68.1/99.8 MB 518.0 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 68.1/99.8 MB 518.4 kB/s eta 0:01:02\n",
      "   --------------------------- ------------ 68.1/99.8 MB 520.0 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.2/99.8 MB 518.4 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.2/99.8 MB 519.6 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.2/99.8 MB 520.0 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.3/99.8 MB 520.8 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.3/99.8 MB 519.6 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.3/99.8 MB 520.4 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.4/99.8 MB 520.4 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.4/99.8 MB 521.3 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.4/99.8 MB 522.1 kB/s eta 0:01:01\n",
      "   --------------------------- ------------ 68.4/99.8 MB 522.5 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.5/99.8 MB 523.8 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.5/99.8 MB 523.7 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.5/99.8 MB 522.9 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.6/99.8 MB 523.8 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.6/99.8 MB 524.2 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.6/99.8 MB 526.3 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.6/99.8 MB 525.4 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.7/99.8 MB 526.7 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.7/99.8 MB 525.9 kB/s eta 0:01:00\n",
      "   --------------------------- ------------ 68.7/99.8 MB 526.7 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 68.8/99.8 MB 527.6 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 68.8/99.8 MB 528.4 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 68.8/99.8 MB 529.3 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 68.8/99.8 MB 528.4 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 68.8/99.8 MB 528.4 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 68.9/99.8 MB 528.8 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.0/99.8 MB 528.8 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.0/99.8 MB 528.8 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.0/99.8 MB 528.9 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.0/99.8 MB 527.6 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.0/99.8 MB 528.4 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.1/99.8 MB 527.6 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.1/99.8 MB 528.0 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.1/99.8 MB 527.6 kB/s eta 0:00:59\n",
      "   --------------------------- ------------ 69.2/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.2/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.2/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.3/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.3/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.3/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.3/99.8 MB 527.1 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.4/99.8 MB 527.1 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.4/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.4/99.8 MB 527.1 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.5/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.5/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.5/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.6/99.8 MB 527.1 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.6/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.6/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.6/99.8 MB 527.6 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.7/99.8 MB 527.6 kB/s eta 0:00:57\n",
      "   --------------------------- ------------ 69.7/99.8 MB 526.3 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.7/99.8 MB 526.3 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.8/99.8 MB 525.9 kB/s eta 0:00:58\n",
      "   --------------------------- ------------ 69.8/99.8 MB 525.4 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 69.8/99.8 MB 525.9 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 69.9/99.8 MB 524.6 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 69.9/99.8 MB 524.6 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 69.9/99.8 MB 523.3 kB/s eta 0:00:58\n",
      "   ---------------------------- ----------- 69.9/99.8 MB 523.3 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.0/99.8 MB 523.3 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.0/99.8 MB 522.5 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.0/99.8 MB 522.9 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.0/99.8 MB 522.1 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.1/99.8 MB 522.1 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.1/99.8 MB 522.1 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.1/99.8 MB 521.3 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.1/99.8 MB 521.3 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.2/99.8 MB 520.4 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.2/99.8 MB 520.0 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.2/99.8 MB 519.2 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.2/99.8 MB 519.2 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.3/99.8 MB 519.2 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.3/99.8 MB 518.8 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.3/99.8 MB 518.8 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.4/99.8 MB 518.4 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.4/99.8 MB 519.2 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.5/99.8 MB 519.6 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.5/99.8 MB 519.2 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.5/99.8 MB 519.6 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.6/99.8 MB 520.0 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.6/99.8 MB 520.0 kB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 70.6/99.8 MB 520.4 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.6/99.8 MB 520.0 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.7/99.8 MB 520.0 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.7/99.8 MB 520.0 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.7/99.8 MB 519.6 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.7/99.8 MB 521.3 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.8/99.8 MB 520.8 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.8/99.8 MB 519.6 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.8/99.8 MB 519.2 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.9/99.8 MB 520.0 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.9/99.8 MB 519.6 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 70.9/99.8 MB 522.5 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 71.0/99.8 MB 521.7 kB/s eta 0:00:56\n",
      "   ---------------------------- ----------- 71.0/99.8 MB 523.3 kB/s eta 0:00:55\n",
      "   ---------------------------- ----------- 71.0/99.8 MB 523.8 kB/s eta 0:00:55\n",
      "   ---------------------------- ----------- 71.1/99.8 MB 526.3 kB/s eta 0:00:55\n",
      "   ---------------------------- ----------- 71.1/99.8 MB 527.6 kB/s eta 0:00:55\n",
      "   ---------------------------- ----------- 71.2/99.8 MB 531.0 kB/s eta 0:00:54\n",
      "   ---------------------------- ----------- 71.2/99.8 MB 532.7 kB/s eta 0:00:54\n",
      "   ---------------------------- ----------- 71.2/99.8 MB 535.3 kB/s eta 0:00:54\n",
      "   ---------------------------- ----------- 71.3/99.8 MB 536.6 kB/s eta 0:00:54\n",
      "   ---------------------------- ----------- 71.3/99.8 MB 538.9 kB/s eta 0:00:53\n",
      "   ---------------------------- ----------- 71.3/99.8 MB 538.9 kB/s eta 0:00:53\n",
      "   ---------------------------- ----------- 71.4/99.8 MB 542.4 kB/s eta 0:00:53\n",
      "   ---------------------------- ----------- 71.4/99.8 MB 542.9 kB/s eta 0:00:53\n",
      "   ---------------------------- ----------- 71.4/99.8 MB 543.3 kB/s eta 0:00:53\n",
      "   ---------------------------- ----------- 71.5/99.8 MB 543.8 kB/s eta 0:00:53\n",
      "   ---------------------------- ----------- 71.5/99.8 MB 544.3 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.5/99.8 MB 544.2 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.5/99.8 MB 544.2 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.6/99.8 MB 543.8 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.6/99.8 MB 543.8 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.6/99.8 MB 543.8 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.6/99.8 MB 543.3 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.6/99.8 MB 542.9 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.7/99.8 MB 542.9 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.7/99.8 MB 542.9 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.7/99.8 MB 542.9 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.8/99.8 MB 543.3 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.8/99.8 MB 542.9 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.8/99.8 MB 543.3 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.8/99.8 MB 543.8 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.9/99.8 MB 543.8 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.9/99.8 MB 544.2 kB/s eta 0:00:52\n",
      "   ---------------------------- ----------- 71.9/99.8 MB 546.5 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 545.6 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 546.1 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 548.8 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 548.8 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 548.8 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 548.8 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.1/99.8 MB 549.7 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.1/99.8 MB 549.7 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.2/99.8 MB 548.8 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.2/99.8 MB 548.8 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.2/99.8 MB 548.3 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.2/99.8 MB 548.3 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.2/99.8 MB 547.4 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.3/99.8 MB 547.0 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.3/99.8 MB 546.1 kB/s eta 0:00:51\n",
      "   ---------------------------- ----------- 72.3/99.8 MB 547.0 kB/s eta 0:00:51\n",
      "   ----------------------------- ---------- 72.3/99.8 MB 546.1 kB/s eta 0:00:51\n",
      "   ----------------------------- ---------- 72.3/99.8 MB 546.1 kB/s eta 0:00:51\n",
      "   ----------------------------- ---------- 72.4/99.8 MB 546.5 kB/s eta 0:00:51\n",
      "   ----------------------------- ---------- 72.4/99.8 MB 548.3 kB/s eta 0:00:50\n",
      "   ----------------------------- ---------- 72.4/99.8 MB 548.3 kB/s eta 0:00:50\n",
      "   ----------------------------- ---------- 72.4/99.8 MB 545.6 kB/s eta 0:00:51\n",
      "   ----------------------------- ---------- 72.5/99.8 MB 546.5 kB/s eta 0:00:50\n",
      "   ----------------------------- ---------- 72.5/99.8 MB 548.8 kB/s eta 0:00:50\n",
      "   ----------------------------- ---------- 72.5/99.8 MB 548.8 kB/s eta 0:00:50\n",
      "   ----------------------------- ---------- 72.6/99.8 MB 549.2 kB/s eta 0:00:50\n",
      "   ----------------------------- ---------- 72.6/99.8 MB 550.6 kB/s eta 0:00:50\n",
      "   ----------------------------- ---------- 72.6/99.8 MB 553.4 kB/s eta 0:00:50\n",
      "   ----------------------------- ---------- 72.7/99.8 MB 553.9 kB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 72.7/99.8 MB 553.4 kB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 72.7/99.8 MB 554.8 kB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 72.8/99.8 MB 556.7 kB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 72.8/99.8 MB 556.7 kB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 72.8/99.8 MB 558.6 kB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 72.8/99.8 MB 560.5 kB/s eta 0:00:49\n",
      "   ----------------------------- ---------- 72.9/99.8 MB 560.5 kB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 72.9/99.8 MB 564.4 kB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 72.9/99.8 MB 565.4 kB/s eta 0:00:48\n",
      "   ----------------------------- ---------- 73.0/99.8 MB 570.8 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.0/99.8 MB 570.3 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.1/99.8 MB 572.8 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.1/99.8 MB 574.3 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.1/99.8 MB 574.3 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.1/99.8 MB 574.8 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.2/99.8 MB 576.3 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.2/99.8 MB 575.8 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.2/99.8 MB 576.8 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.2/99.8 MB 576.8 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.3/99.8 MB 575.3 kB/s eta 0:00:47\n",
      "   ----------------------------- ---------- 73.3/99.8 MB 576.8 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.3/99.8 MB 575.8 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.4/99.8 MB 577.3 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.4/99.8 MB 576.8 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.4/99.8 MB 579.4 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.5/99.8 MB 579.9 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.5/99.8 MB 579.4 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.5/99.8 MB 581.4 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.6/99.8 MB 580.9 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.6/99.8 MB 580.4 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.6/99.8 MB 580.4 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.6/99.8 MB 580.4 kB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 73.7/99.8 MB 579.9 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.7/99.8 MB 579.4 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.7/99.8 MB 579.4 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.7/99.8 MB 579.4 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.7/99.8 MB 577.9 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.8/99.8 MB 578.4 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.8/99.8 MB 577.9 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.8/99.8 MB 577.3 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.8/99.8 MB 577.3 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.9/99.8 MB 576.8 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.9/99.8 MB 576.8 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.9/99.8 MB 576.8 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 73.9/99.8 MB 575.8 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.0/99.8 MB 576.8 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.0/99.8 MB 576.3 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.0/99.8 MB 577.9 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.1/99.8 MB 577.3 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.1/99.8 MB 577.3 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.1/99.8 MB 577.9 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.2/99.8 MB 577.9 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.2/99.8 MB 577.8 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.2/99.8 MB 578.4 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.3/99.8 MB 578.4 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.3/99.8 MB 578.4 kB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 74.3/99.8 MB 579.4 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.4/99.8 MB 578.3 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.4/99.8 MB 579.4 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.5/99.8 MB 578.3 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.5/99.8 MB 580.9 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.5/99.8 MB 580.4 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.6/99.8 MB 580.4 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.6/99.8 MB 580.9 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.6/99.8 MB 582.0 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.7/99.8 MB 582.0 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.7/99.8 MB 580.9 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.7/99.8 MB 582.5 kB/s eta 0:00:43\n",
      "   ----------------------------- ---------- 74.8/99.8 MB 580.9 kB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 74.8/99.8 MB 580.9 kB/s eta 0:00:43\n",
      "   ------------------------------ --------- 74.8/99.8 MB 583.0 kB/s eta 0:00:43\n",
      "   ------------------------------ --------- 74.9/99.8 MB 584.6 kB/s eta 0:00:43\n",
      "   ------------------------------ --------- 74.9/99.8 MB 585.6 kB/s eta 0:00:43\n",
      "   ------------------------------ --------- 74.9/99.8 MB 590.9 kB/s eta 0:00:42\n",
      "   ------------------------------ --------- 75.0/99.8 MB 589.3 kB/s eta 0:00:43\n",
      "   ------------------------------ --------- 75.0/99.8 MB 591.4 kB/s eta 0:00:42\n",
      "   ------------------------------ --------- 75.0/99.8 MB 594.6 kB/s eta 0:00:42\n",
      "   ------------------------------ --------- 75.1/99.8 MB 595.8 kB/s eta 0:00:42\n",
      "   ------------------------------ --------- 75.1/99.8 MB 599.5 kB/s eta 0:00:42\n",
      "   ------------------------------ --------- 75.2/99.8 MB 602.3 kB/s eta 0:00:41\n",
      "   ------------------------------ --------- 75.2/99.8 MB 605.7 kB/s eta 0:00:41\n",
      "   ------------------------------ --------- 75.3/99.8 MB 606.2 kB/s eta 0:00:41\n",
      "   ------------------------------ --------- 75.3/99.8 MB 609.0 kB/s eta 0:00:41\n",
      "   ------------------------------ --------- 75.4/99.8 MB 610.1 kB/s eta 0:00:40\n",
      "   ------------------------------ --------- 75.4/99.8 MB 612.5 kB/s eta 0:00:40\n",
      "   ------------------------------ --------- 75.5/99.8 MB 613.6 kB/s eta 0:00:40\n",
      "   ------------------------------ --------- 75.5/99.8 MB 614.7 kB/s eta 0:00:40\n",
      "   ------------------------------ --------- 75.5/99.8 MB 617.1 kB/s eta 0:00:40\n",
      "   ------------------------------ --------- 75.6/99.8 MB 616.5 kB/s eta 0:00:40\n",
      "   ------------------------------ --------- 75.6/99.8 MB 618.2 kB/s eta 0:00:40\n",
      "   ------------------------------ --------- 75.6/99.8 MB 618.2 kB/s eta 0:00:40\n",
      "   ------------------------------ --------- 75.7/99.8 MB 617.7 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 75.7/99.8 MB 620.0 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 75.7/99.8 MB 620.0 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 75.7/99.8 MB 620.0 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 75.7/99.8 MB 615.9 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 75.8/99.8 MB 617.1 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 75.8/99.8 MB 619.4 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 75.9/99.8 MB 621.2 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 76.0/99.8 MB 625.3 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 76.0/99.8 MB 624.7 kB/s eta 0:00:39\n",
      "   ------------------------------ --------- 76.0/99.8 MB 625.3 kB/s eta 0:00:38\n",
      "   ------------------------------ --------- 76.0/99.8 MB 627.7 kB/s eta 0:00:38\n",
      "   ------------------------------ --------- 76.1/99.8 MB 631.9 kB/s eta 0:00:38\n",
      "   ------------------------------ --------- 76.1/99.8 MB 631.3 kB/s eta 0:00:38\n",
      "   ------------------------------ --------- 76.2/99.8 MB 635.0 kB/s eta 0:00:38\n",
      "   ------------------------------ --------- 76.2/99.8 MB 637.5 kB/s eta 0:00:37\n",
      "   ------------------------------ --------- 76.2/99.8 MB 637.5 kB/s eta 0:00:37\n",
      "   ------------------------------ --------- 76.2/99.8 MB 641.2 kB/s eta 0:00:37\n",
      "   ------------------------------ --------- 76.3/99.8 MB 641.8 kB/s eta 0:00:37\n",
      "   ------------------------------ --------- 76.3/99.8 MB 645.7 kB/s eta 0:00:37\n",
      "   ------------------------------ --------- 76.4/99.8 MB 646.3 kB/s eta 0:00:37\n",
      "   ------------------------------ --------- 76.4/99.8 MB 649.5 kB/s eta 0:00:36\n",
      "   ------------------------------ --------- 76.5/99.8 MB 652.1 kB/s eta 0:00:36\n",
      "   ------------------------------ --------- 76.5/99.8 MB 652.7 kB/s eta 0:00:36\n",
      "   ------------------------------ --------- 76.6/99.8 MB 656.0 kB/s eta 0:00:36\n",
      "   ------------------------------ --------- 76.6/99.8 MB 658.6 kB/s eta 0:00:36\n",
      "   ------------------------------ --------- 76.6/99.8 MB 658.6 kB/s eta 0:00:36\n",
      "   ------------------------------ --------- 76.7/99.8 MB 658.7 kB/s eta 0:00:36\n",
      "   ------------------------------ --------- 76.7/99.8 MB 661.3 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 76.7/99.8 MB 663.3 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 76.8/99.8 MB 665.4 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 76.8/99.8 MB 665.4 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 76.8/99.8 MB 664.7 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 76.9/99.8 MB 667.4 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 76.9/99.8 MB 666.7 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 76.9/99.8 MB 666.7 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 76.9/99.8 MB 666.0 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 77.0/99.8 MB 666.0 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 77.0/99.8 MB 665.4 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 77.0/99.8 MB 666.7 kB/s eta 0:00:35\n",
      "   ------------------------------ --------- 77.1/99.8 MB 669.4 kB/s eta 0:00:34\n",
      "   ------------------------------ --------- 77.1/99.8 MB 669.4 kB/s eta 0:00:34\n",
      "   ------------------------------ --------- 77.2/99.8 MB 672.2 kB/s eta 0:00:34\n",
      "   ------------------------------ --------- 77.3/99.8 MB 675.6 kB/s eta 0:00:34\n",
      "   ------------------------------ --------- 77.3/99.8 MB 676.3 kB/s eta 0:00:34\n",
      "   ------------------------------- -------- 77.4/99.8 MB 677.0 kB/s eta 0:00:34\n",
      "   ------------------------------- -------- 77.4/99.8 MB 678.5 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.4/99.8 MB 680.6 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.5/99.8 MB 678.5 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.5/99.8 MB 679.9 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.6/99.8 MB 680.6 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.6/99.8 MB 679.9 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.7/99.8 MB 681.3 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.7/99.8 MB 682.0 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.7/99.8 MB 682.7 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.8/99.8 MB 682.0 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.8/99.8 MB 684.1 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.9/99.8 MB 683.4 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.9/99.8 MB 682.0 kB/s eta 0:00:33\n",
      "   ------------------------------- -------- 77.9/99.8 MB 682.0 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.0/99.8 MB 682.7 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.0/99.8 MB 683.4 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.0/99.8 MB 682.0 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.1/99.8 MB 685.5 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.1/99.8 MB 685.5 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.2/99.8 MB 689.2 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.2/99.8 MB 689.2 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.3/99.8 MB 692.1 kB/s eta 0:00:32\n",
      "   ------------------------------- -------- 78.4/99.8 MB 696.5 kB/s eta 0:00:31\n",
      "   ------------------------------- -------- 78.4/99.8 MB 699.5 kB/s eta 0:00:31\n",
      "   ------------------------------- -------- 78.4/99.8 MB 699.5 kB/s eta 0:00:31\n",
      "   ------------------------------- -------- 78.5/99.8 MB 697.2 kB/s eta 0:00:31\n",
      "   ------------------------------- -------- 78.5/99.8 MB 699.5 kB/s eta 0:00:31\n",
      "   ------------------------------- -------- 78.6/99.8 MB 702.4 kB/s eta 0:00:31\n",
      "   ------------------------------- -------- 78.7/99.8 MB 705.5 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 78.7/99.8 MB 705.5 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 78.7/99.8 MB 704.8 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 78.8/99.8 MB 707.8 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 78.9/99.8 MB 708.6 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 78.9/99.8 MB 711.6 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 78.9/99.8 MB 711.6 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 78.9/99.8 MB 707.8 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 79.0/99.8 MB 711.6 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 79.1/99.8 MB 713.2 kB/s eta 0:00:30\n",
      "   ------------------------------- -------- 79.1/99.8 MB 719.4 kB/s eta 0:00:29\n",
      "   ------------------------------- -------- 79.1/99.8 MB 717.9 kB/s eta 0:00:29\n",
      "   ------------------------------- -------- 79.1/99.8 MB 713.2 kB/s eta 0:00:29\n",
      "   ------------------------------- -------- 79.2/99.8 MB 714.0 kB/s eta 0:00:29\n",
      "   ------------------------------- -------- 79.2/99.8 MB 718.6 kB/s eta 0:00:29\n",
      "   ------------------------------- -------- 79.3/99.8 MB 722.6 kB/s eta 0:00:29\n",
      "   ------------------------------- -------- 79.3/99.8 MB 725.0 kB/s eta 0:00:29\n",
      "   ------------------------------- -------- 79.4/99.8 MB 724.2 kB/s eta 0:00:29\n",
      "   ------------------------------- -------- 79.5/99.8 MB 727.4 kB/s eta 0:00:28\n",
      "   ------------------------------- -------- 79.5/99.8 MB 729.0 kB/s eta 0:00:28\n",
      "   ------------------------------- -------- 79.6/99.8 MB 731.5 kB/s eta 0:00:28\n",
      "   ------------------------------- -------- 79.6/99.8 MB 733.2 kB/s eta 0:00:28\n",
      "   ------------------------------- -------- 79.7/99.8 MB 736.5 kB/s eta 0:00:28\n",
      "   ------------------------------- -------- 79.8/99.8 MB 739.0 kB/s eta 0:00:28\n",
      "   ------------------------------- -------- 79.8/99.8 MB 738.1 kB/s eta 0:00:28\n",
      "   -------------------------------- ------- 79.8/99.8 MB 738.9 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 79.9/99.8 MB 742.3 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 79.9/99.8 MB 739.7 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 80.0/99.8 MB 741.5 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 80.0/99.8 MB 744.0 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 80.1/99.8 MB 744.0 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 80.1/99.8 MB 746.5 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 80.1/99.8 MB 748.2 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 80.2/99.8 MB 750.8 kB/s eta 0:00:27\n",
      "   -------------------------------- ------- 80.3/99.8 MB 757.8 kB/s eta 0:00:26\n",
      "   -------------------------------- ------- 80.4/99.8 MB 764.8 kB/s eta 0:00:26\n",
      "   -------------------------------- ------- 80.4/99.8 MB 769.4 kB/s eta 0:00:26\n",
      "   -------------------------------- ------- 80.5/99.8 MB 772.9 kB/s eta 0:00:25\n",
      "   -------------------------------- ------- 80.6/99.8 MB 780.3 kB/s eta 0:00:25\n",
      "   -------------------------------- ------- 80.7/99.8 MB 781.3 kB/s eta 0:00:25\n",
      "   -------------------------------- ------- 80.8/99.8 MB 787.9 kB/s eta 0:00:25\n",
      "   -------------------------------- ------- 80.9/99.8 MB 791.7 kB/s eta 0:00:24\n",
      "   -------------------------------- ------- 81.0/99.8 MB 799.4 kB/s eta 0:00:24\n",
      "   -------------------------------- ------- 81.0/99.8 MB 804.3 kB/s eta 0:00:24\n",
      "   -------------------------------- ------- 81.1/99.8 MB 807.3 kB/s eta 0:00:24\n",
      "   -------------------------------- ------- 81.2/99.8 MB 810.3 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.2/99.8 MB 813.2 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.3/99.8 MB 813.3 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.3/99.8 MB 813.3 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.3/99.8 MB 808.2 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.4/99.8 MB 812.3 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.5/99.8 MB 815.3 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.5/99.8 MB 816.4 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.5/99.8 MB 816.4 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.5/99.8 MB 816.4 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.5/99.8 MB 816.4 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 81.5/99.8 MB 816.4 kB/s eta 0:00:23\n",
      "   -------------------------------- ------- 82.0/99.8 MB 852.5 kB/s eta 0:00:21\n",
      "   -------------------------------- ------- 82.0/99.8 MB 860.4 kB/s eta 0:00:21\n",
      "   -------------------------------- ------- 82.1/99.8 MB 861.5 kB/s eta 0:00:21\n",
      "   -------------------------------- ------- 82.1/99.8 MB 859.2 kB/s eta 0:00:21\n",
      "   -------------------------------- ------- 82.1/99.8 MB 863.7 kB/s eta 0:00:21\n",
      "   -------------------------------- ------- 82.2/99.8 MB 866.0 kB/s eta 0:00:21\n",
      "   --------------------------------- ------ 82.3/99.8 MB 877.6 kB/s eta 0:00:20\n",
      "   --------------------------------- ------ 82.4/99.8 MB 884.8 kB/s eta 0:00:20\n",
      "   --------------------------------- ------ 82.5/99.8 MB 896.9 kB/s eta 0:00:20\n",
      "   --------------------------------- ------ 82.5/99.8 MB 898.1 kB/s eta 0:00:20\n",
      "   --------------------------------- ------ 82.5/99.8 MB 898.1 kB/s eta 0:00:20\n",
      "   --------------------------------- ------ 82.5/99.8 MB 898.1 kB/s eta 0:00:20\n",
      "   --------------------------------- ------ 82.5/99.8 MB 889.5 kB/s eta 0:00:20\n",
      "   --------------------------------- ------ 82.7/99.8 MB 917.0 kB/s eta 0:00:19\n",
      "   --------------------------------- ------ 82.8/99.8 MB 923.4 kB/s eta 0:00:19\n",
      "   --------------------------------- ------ 82.8/99.8 MB 923.4 kB/s eta 0:00:19\n",
      "   --------------------------------- ------ 82.9/99.8 MB 930.0 kB/s eta 0:00:19\n",
      "   --------------------------------- ------ 83.0/99.8 MB 934.0 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 83.1/99.8 MB 939.3 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 83.1/99.8 MB 942.1 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 83.2/99.8 MB 947.5 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 83.3/99.8 MB 950.3 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 83.3/99.8 MB 954.4 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 83.4/99.8 MB 961.4 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 83.4/99.8 MB 964.2 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 83.4/99.8 MB 964.2 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 83.5/99.8 MB 969.9 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 83.5/99.8 MB 969.9 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 83.7/99.8 MB 989.1 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 83.8/99.8 MB 995.0 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 83.8/99.8 MB 999.5 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 83.9/99.8 MB 1.0 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 84.0/99.8 MB 1.0 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 84.0/99.8 MB 1.0 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 84.1/99.8 MB 1.0 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 84.2/99.8 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.2/99.8 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.3/99.8 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.4/99.8 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.4/99.8 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.5/99.8 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.6/99.8 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.6/99.8 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.6/99.8 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.7/99.8 MB 1.1 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 84.8/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 84.8/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 84.9/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.0/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.0/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.0/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.0/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.1/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.2/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.3/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.3/99.8 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 85.4/99.8 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 85.5/99.8 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 85.6/99.8 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 85.6/99.8 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 85.6/99.8 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 85.7/99.8 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 85.8/99.8 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 85.9/99.8 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 86.0/99.8 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 86.1/99.8 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 86.2/99.8 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 86.2/99.8 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 86.3/99.8 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 86.4/99.8 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 86.4/99.8 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 86.5/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 86.6/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 86.6/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 86.7/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 86.8/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 86.8/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 86.8/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 86.8/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 86.9/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 87.0/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 87.0/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 87.1/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 87.2/99.8 MB 1.2 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 87.2/99.8 MB 1.3 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 87.3/99.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.3/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.4/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.4/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.4/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.5/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.5/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.6/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.7/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.7/99.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.8/99.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.8/99.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.8/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.9/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.9/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.9/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 87.9/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 88.0/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 88.1/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 88.1/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 88.1/99.8 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 88.3/99.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 88.3/99.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 88.4/99.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 88.4/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.4/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.5/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.5/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.6/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.6/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.7/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.7/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.8/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.8/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.9/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.9/99.8 MB 1.3 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.9/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 88.9/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.0/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.0/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.0/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.0/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.1/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.1/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.1/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.1/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.2/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.2/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.2/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.3/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.4/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.4/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.4/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.6/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.6/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.6/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.7/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.7/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.7/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.7/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.7/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 89.8/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 89.8/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 89.8/99.8 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 89.9/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 89.9/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 89.9/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.0/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.0/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.0/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.1/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.1/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.1/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.1/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.2/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.2/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.2/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.2/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.3/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.3/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.4/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.4/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.4/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.5/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.5/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.5/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.5/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.6/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.6/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.7/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.7/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.7/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.7/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.8/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.8/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.9/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.9/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 90.9/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.0/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.0/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.0/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.1/99.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.1/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.2/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.2/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.2/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.2/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.3/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.3/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.4/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.4/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.5/99.8 MB 1.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 91.5/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 91.6/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 91.6/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 91.7/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 91.8/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 91.8/99.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 91.9/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 91.9/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 92.0/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 92.0/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 92.1/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 92.1/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 92.2/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 92.2/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.3/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.3/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.3/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.4/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.4/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.5/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.5/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.5/99.8 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.5/99.8 MB 996.5 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.5/99.8 MB 995.0 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.6/99.8 MB 991.9 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.7/99.8 MB 992.0 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.7/99.8 MB 991.9 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 92.8/99.8 MB 1.0 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 92.8/99.8 MB 999.6 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 92.9/99.8 MB 995.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 92.9/99.8 MB 992.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.0/99.8 MB 992.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.0/99.8 MB 987.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.1/99.8 MB 989.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.1/99.8 MB 987.4 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.2/99.8 MB 986.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.2/99.8 MB 986.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.3/99.8 MB 986.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.3/99.8 MB 986.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.4/99.8 MB 986.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.5/99.8 MB 984.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.5/99.8 MB 984.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.6/99.8 MB 981.6 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.6/99.8 MB 981.6 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.7/99.8 MB 989.0 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.7/99.8 MB 984.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.8/99.8 MB 990.4 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.8/99.8 MB 984.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 93.9/99.8 MB 980.0 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 93.9/99.8 MB 980.0 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 93.9/99.8 MB 971.3 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.0/99.8 MB 969.9 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.0/99.8 MB 967.1 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.1/99.8 MB 965.7 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.1/99.8 MB 965.6 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.1/99.8 MB 965.6 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.1/99.8 MB 965.6 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.2/99.8 MB 955.8 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.2/99.8 MB 954.4 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.3/99.8 MB 955.8 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.3/99.8 MB 954.4 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.4/99.8 MB 955.8 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.5/99.8 MB 954.4 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.5/99.8 MB 953.0 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.6/99.8 MB 953.0 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.6/99.8 MB 950.2 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.7/99.8 MB 950.2 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.7/99.8 MB 951.6 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 94.8/99.8 MB 948.9 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 94.8/99.8 MB 948.9 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 94.8/99.8 MB 943.4 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 94.9/99.8 MB 947.5 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 95.0/99.8 MB 947.5 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 95.0/99.8 MB 947.5 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 95.0/99.8 MB 943.4 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.1/99.8 MB 943.4 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.1/99.8 MB 943.4 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.1/99.8 MB 943.4 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.2/99.8 MB 939.3 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.3/99.8 MB 938.0 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.3/99.8 MB 944.8 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.4/99.8 MB 946.1 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.4/99.8 MB 943.4 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.5/99.8 MB 942.0 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.5/99.8 MB 942.0 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.5/99.8 MB 936.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.6/99.8 MB 938.0 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.6/99.8 MB 938.0 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.6/99.8 MB 930.0 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.7/99.8 MB 928.7 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.7/99.8 MB 928.7 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.7/99.8 MB 923.4 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.8/99.8 MB 922.1 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.8/99.8 MB 919.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.9/99.8 MB 924.7 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.9/99.8 MB 923.4 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 95.9/99.8 MB 923.4 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 96.0/99.8 MB 916.9 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 96.0/99.8 MB 917.0 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 96.0/99.8 MB 917.0 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 96.1/99.8 MB 910.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 96.2/99.8 MB 908.0 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.2/99.8 MB 906.8 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.2/99.8 MB 904.2 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.3/99.8 MB 903.0 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.3/99.8 MB 901.8 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.3/99.8 MB 901.8 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.4/99.8 MB 894.4 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.4/99.8 MB 893.2 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.5/99.8 MB 891.9 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.5/99.8 MB 892.0 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.5/99.8 MB 892.0 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.6/99.8 MB 885.9 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.6/99.8 MB 893.2 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.7/99.8 MB 889.5 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.7/99.8 MB 884.8 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.8/99.8 MB 882.3 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.9/99.8 MB 884.8 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 96.9/99.8 MB 885.9 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 97.0/99.8 MB 888.4 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 97.0/99.8 MB 889.5 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 97.0/99.8 MB 889.5 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 97.1/99.8 MB 887.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 97.2/99.8 MB 887.2 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 97.2/99.8 MB 887.1 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.3/99.8 MB 882.3 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.3/99.8 MB 883.6 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.4/99.8 MB 882.3 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.4/99.8 MB 882.3 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.5/99.8 MB 882.3 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.5/99.8 MB 882.3 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.5/99.8 MB 877.6 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.5/99.8 MB 878.8 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.6/99.8 MB 877.6 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.6/99.8 MB 881.2 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.7/99.8 MB 881.2 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.8/99.8 MB 881.2 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.8/99.8 MB 882.3 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.9/99.8 MB 881.2 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.9/99.8 MB 881.2 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.9/99.8 MB 881.2 kB/s eta 0:00:03\n",
      "   ---------------------------------------  97.9/99.8 MB 875.3 kB/s eta 0:00:03\n",
      "   ---------------------------------------  98.0/99.8 MB 880.0 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.1/99.8 MB 881.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.2/99.8 MB 888.3 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.2/99.8 MB 890.8 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.3/99.8 MB 890.8 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.3/99.8 MB 885.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.4/99.8 MB 890.8 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.4/99.8 MB 886.0 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.5/99.8 MB 887.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.5/99.8 MB 887.1 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.6/99.8 MB 887.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.6/99.8 MB 883.6 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.7/99.8 MB 885.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.7/99.8 MB 884.8 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.8/99.8 MB 888.3 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.8/99.8 MB 890.7 kB/s eta 0:00:02\n",
      "   ---------------------------------------  98.9/99.8 MB 889.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  98.9/99.8 MB 887.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  98.9/99.8 MB 887.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.0/99.8 MB 884.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.1/99.8 MB 885.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.1/99.8 MB 885.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.1/99.8 MB 885.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.1/99.8 MB 885.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.1/99.8 MB 887.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.2/99.8 MB 891.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/99.8 MB 899.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/99.8 MB 900.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/99.8 MB 900.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/99.8 MB 900.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/99.8 MB 900.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 923.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 937.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 936.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 828.1 kB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       151\n",
      "           1       0.60      0.65      0.62        80\n",
      "\n",
      "    accuracy                           0.73       231\n",
      "   macro avg       0.70      0.71      0.70       231\n",
      "weighted avg       0.73      0.73      0.73       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'device': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'gamma': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'multi_strategy': None,\n",
       " 'n_estimators': None,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        XGBClassifier\n",
       "\u001b[1;31mString form:\u001b[0m\n",
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "           colsample_bylevel=None <...> ne, n_estimators=None, n_jobs=None,\n",
       "           num_parallel_tree=None, random_state=None, ...)\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Implementation of the scikit-learn API for XGBoost classification.\n",
       "See :doc:`/python/sklearn_estimator` for more information.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "    n_estimators : Optional[int]\n",
       "        Number of boosting rounds.\n",
       "\n",
       "    max_depth :  Optional[int]\n",
       "        Maximum tree depth for base learners.\n",
       "    max_leaves :\n",
       "        Maximum number of leaves; 0 indicates no limit.\n",
       "    max_bin :\n",
       "        If using histogram-based algorithm, maximum number of bins per feature\n",
       "    grow_policy :\n",
       "        Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
       "        depth-wise. 1: favor splitting at nodes with highest loss change.\n",
       "    learning_rate : Optional[float]\n",
       "        Boosting learning rate (xgb's \"eta\")\n",
       "    verbosity : Optional[int]\n",
       "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
       "    objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
       "        Specify the learning task and the corresponding learning objective or\n",
       "        a custom objective function to be used (see note below).\n",
       "    booster: Optional[str]\n",
       "        Specify which booster to use: gbtree, gblinear or dart.\n",
       "    tree_method: Optional[str]\n",
       "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
       "        default, XGBoost will choose the most conservative option available.  It's\n",
       "        recommended to study this option from the parameters document :doc:`tree method\n",
       "        </treemethod>`\n",
       "    n_jobs : Optional[int]\n",
       "        Number of parallel threads used to run xgboost.  When used with other\n",
       "        Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
       "        parallelize and balance the threads.  Creating thread contention will\n",
       "        significantly slow down both algorithms.\n",
       "    gamma : Optional[float]\n",
       "        (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
       "        leaf node of the tree.\n",
       "    min_child_weight : Optional[float]\n",
       "        Minimum sum of instance weight(hessian) needed in a child.\n",
       "    max_delta_step : Optional[float]\n",
       "        Maximum delta step we allow each tree's weight estimation to be.\n",
       "    subsample : Optional[float]\n",
       "        Subsample ratio of the training instance.\n",
       "    sampling_method :\n",
       "        Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
       "          - ``uniform``: select random training instances uniformly.\n",
       "          - ``gradient_based`` select random training instances with higher probability\n",
       "            when the gradient and hessian are larger. (cf. CatBoost)\n",
       "    colsample_bytree : Optional[float]\n",
       "        Subsample ratio of columns when constructing each tree.\n",
       "    colsample_bylevel : Optional[float]\n",
       "        Subsample ratio of columns for each level.\n",
       "    colsample_bynode : Optional[float]\n",
       "        Subsample ratio of columns for each split.\n",
       "    reg_alpha : Optional[float]\n",
       "        L1 regularization term on weights (xgb's alpha).\n",
       "    reg_lambda : Optional[float]\n",
       "        L2 regularization term on weights (xgb's lambda).\n",
       "    scale_pos_weight : Optional[float]\n",
       "        Balancing of positive and negative weights.\n",
       "    base_score : Optional[float]\n",
       "        The initial prediction score of all instances, global bias.\n",
       "    random_state : Optional[Union[numpy.random.RandomState, int]]\n",
       "        Random number seed.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           Using gblinear booster with shotgun updater is nondeterministic as\n",
       "           it uses Hogwild algorithm.\n",
       "\n",
       "    missing : float, default np.nan\n",
       "        Value in the data which needs to be present as a missing value.\n",
       "    num_parallel_tree: Optional[int]\n",
       "        Used for boosting random forest.\n",
       "    monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
       "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
       "        for more information.\n",
       "    interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
       "        Constraints for interaction representing permitted interactions.  The\n",
       "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
       "        3, 4]]``, where each inner list is a group of indices of features that are\n",
       "        allowed to interact with each other.  See :doc:`tutorial\n",
       "        </tutorials/feature_interaction_constraint>` for more information\n",
       "    importance_type: Optional[str]\n",
       "        The feature importance type for the feature_importances\\_ property:\n",
       "\n",
       "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
       "          \"total_cover\".\n",
       "        * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
       "          without bias.\n",
       "\n",
       "    device : Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
       "\n",
       "    validate_parameters : Optional[bool]\n",
       "\n",
       "        Give warnings for unknown parameter.\n",
       "\n",
       "    enable_categorical : bool\n",
       "\n",
       "        .. versionadded:: 1.5.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
       "        should be used to specify categorical data type.  Also, JSON/UBJSON\n",
       "        serialization format is required.\n",
       "\n",
       "    feature_types : Optional[FeatureTypes]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        Used for specifying feature types without constructing a dataframe. See\n",
       "        :py:class:`DMatrix` for details.\n",
       "\n",
       "    max_cat_to_onehot : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
       "        for categorical data.  When number of categories is lesser than the threshold\n",
       "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
       "        into children nodes. Also, `enable_categorical` needs to be set to have\n",
       "        categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    max_cat_threshold : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        Maximum number of categories considered for each split. Used only by\n",
       "        partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
       "        needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    multi_strategy : Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        .. note:: This parameter is working-in-progress.\n",
       "\n",
       "        The strategy used for training multi-target models, including multi-target\n",
       "        regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
       "        more information.\n",
       "\n",
       "        - ``one_output_per_tree``: One model for each target.\n",
       "        - ``multi_output_tree``:  Use multi-target trees.\n",
       "\n",
       "    eval_metric : Optional[Union[str, List[str], Callable]]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        Metric used for monitoring the training result and early stopping.  It can be a\n",
       "        string or list of strings as names of predefined metric in XGBoost (See\n",
       "        doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
       "        user defined metric that looks like `sklearn.metrics`.\n",
       "\n",
       "        If custom objective is also provided, then custom metric should implement the\n",
       "        corresponding reverse link function.\n",
       "\n",
       "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
       "        object is provided, it's assumed to be a cost function and by default XGBoost will\n",
       "        minimize the result during early stopping.\n",
       "\n",
       "        For advanced usage on Early stopping like directly choosing to maximize instead of\n",
       "        minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
       "        for more.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "             This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old\n",
       "             one receives un-transformed prediction regardless of whether custom\n",
       "             objective is being used.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from sklearn.datasets import load_diabetes\n",
       "            from sklearn.metrics import mean_absolute_error\n",
       "            X, y = load_diabetes(return_X_y=True)\n",
       "            reg = xgb.XGBRegressor(\n",
       "                tree_method=\"hist\",\n",
       "                eval_metric=mean_absolute_error,\n",
       "            )\n",
       "            reg.fit(X, y, eval_set=[(X, y)])\n",
       "\n",
       "    early_stopping_rounds : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        - Activates early stopping. Validation metric needs to improve at least once in\n",
       "          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
       "          least one item in **eval_set** in :py:meth:`fit`.\n",
       "\n",
       "        - If early stopping occurs, the model will have two additional attributes:\n",
       "          :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
       "          :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
       "          number of trees during inference. If users want to access the full model\n",
       "          (including trees built after early stopping), they can specify the\n",
       "          `iteration_range` in these inference methods. In addition, other utilities\n",
       "          like model plotting can also use the entire model.\n",
       "\n",
       "        - If you prefer to discard the trees after `best_iteration`, consider using the\n",
       "          callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        - If there's more than one item in **eval_set**, the last entry will be used for\n",
       "          early stopping.  If there's more than one metric in **eval_metric**, the last\n",
       "          metric will be used for early stopping.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "            This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
       "\n",
       "    callbacks : Optional[List[TrainingCallback]]\n",
       "        List of callback functions that are applied at end of each iteration.\n",
       "        It is possible to use predefined callbacks by using\n",
       "        :ref:`Callback API <callback_api>`.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           States in callback are not preserved during training, which means callback\n",
       "           objects can not be reused for multiple training sessions without\n",
       "           reinitialization or deepcopy.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            for params in parameters_grid:\n",
       "                # be sure to (re)initialize the callbacks before each run\n",
       "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
       "                reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
       "                reg.fit(X, y)\n",
       "\n",
       "    kwargs : dict, optional\n",
       "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
       "        can be found :doc:`here </parameter>`.\n",
       "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
       "        dict simultaneously will result in a TypeError.\n",
       "\n",
       "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
       "\n",
       "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
       "            that parameters passed via this argument will interact properly\n",
       "            with scikit-learn.\n",
       "\n",
       "        .. note::  Custom objective function\n",
       "\n",
       "            A custom objective function can be provided for the ``objective``\n",
       "            parameter. In this case, it should have the signature\n",
       "            ``objective(y_true, y_pred) -> grad, hess``:\n",
       "\n",
       "            y_true: array_like of shape [n_samples]\n",
       "                The target values\n",
       "            y_pred: array_like of shape [n_samples]\n",
       "                The predicted values\n",
       "\n",
       "            grad: array_like of shape [n_samples]\n",
       "                The value of the gradient for each sample point.\n",
       "            hess: array_like of shape [n_samples]\n",
       "                The value of the second derivative for each sample point"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'n_estimators': [100, 500, 1000, 2000],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5,6],\n",
    "        'learning_rate': [0.1,0.01,0.02,0.05],\n",
    "        \"min_samples_split\": [2,5,10]}\n",
    "\n",
    "#'subsample'; Eğitim örneğinin alt örnek oranı. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 576 candidates, totalling 5760 fits\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 3,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate = 0.05, \n",
    "                    max_depth = 3,\n",
    "                    min_samples_split = 2,\n",
    "                    n_estimators = 100,\n",
    "                    subsample = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned =  xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7662337662337663"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xgb_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       151\n",
      "           1       0.66      0.66      0.66        80\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.74      0.74      0.74       231\n",
      "weighted avg       0.77      0.77      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = diabetes.copy()\n",
    "df = df.dropna()\n",
    "y = df[\"Outcome\"]\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/b3/f8/ee33e36194eb03a76eccf3adac3fba51f0e56fbd20609bb531659d48d3cb/lightgbm-4.1.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading lightgbm-4.1.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages (from lightgbm) (1.10.1)\n",
      "Downloading lightgbm-4.1.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.3 MB 1.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/1.3 MB 871.5 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.3 MB 798.9 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 871.5 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.3 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.4/1.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.6/1.3 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.9/1.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.2/1.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.1.0\n"
     ]
    }
   ],
   "source": [
    "#!conda install -c conda-forge lightgbm\n",
    "!pip install lightgbm\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 188, number of negative: 349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 606\n",
      "[LightGBM] [Info] Number of data points in the train set: 537, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350093 -> initscore=-0.618630\n",
      "[LightGBM] [Info] Start training from score -0.618630\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgbm_model = LGBMClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7229437229437229"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lgbm_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78       151\n",
      "           1       0.59      0.65      0.62        80\n",
      "\n",
      "    accuracy                           0.72       231\n",
      "   macro avg       0.70      0.71      0.70       231\n",
      "weighted avg       0.73      0.72      0.73       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m           LGBMClassifier\n",
       "\u001b[1;31mString form:\u001b[0m    LGBMClassifier()\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\fatma zehra\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM classifier.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (Hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    Number of parallel threads to use for training (can be changed at prediction time by\n",
       "    passing it as an extra keyword argument).\n",
       "\n",
       "    For better performance, it is recommended to set this to the number of physical cores\n",
       "    in the CPU.\n",
       "\n",
       "    Negative integers are interpreted as following joblib's formula (n_cpus + 1 + n_jobs), just like\n",
       "    scikit-learn (so e.g. -1 means using all threads). A value of zero corresponds the default number of\n",
       "    threads configured for OpenMP in the system. A value of ``None`` (the default) corresponds\n",
       "    to using the number of physical cores in the system (its correct detection requires\n",
       "    either the ``joblib`` or the ``psutil`` util libraries to be installed).\n",
       "\n",
       "    .. versionchanged:: 4.0.0\n",
       "\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess``,\n",
       "``objective(y_true, y_pred, weight) -> grad, hess``\n",
       "or ``objective(y_true, y_pred, weight, group) -> grad, hess``:\n",
       "\n",
       "    y_true : numpy 1-D array of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    weight : numpy 1-D array of shape = [n_samples]\n",
       "        The weight of samples. Weights should be non-negative.\n",
       "    group : numpy 1-D array\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes],\n",
       "and grad and hess should be returned in the same format."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lgbm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "        'n_estimators': [100, 500, 1000, 2000],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5,6],\n",
    "        'learning_rate': [0.1,0.01,0.02,0.05],\n",
    "        \"min_child_samples\": [5,10,20]}\n",
    "\n",
    "#\"min_child_samples\": Bir yaprak node için ihtiyaç duyulan minimum veri sayısı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 576 candidates, totalling 5760 fits\n",
      "[LightGBM] [Info] Number of positive: 188, number of negative: 349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 606\n",
      "[LightGBM] [Info] Number of data points in the train set: 537, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350093 -> initscore=-0.618630\n",
      "[LightGBM] [Info] Start training from score -0.618630\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier()\n",
    "\n",
    "lgbm_cv_model = GridSearchCV(lgbm, lgbm_params, \n",
    "                             cv = 10, \n",
    "                             n_jobs = -1, \n",
    "                             verbose = 2).fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 3,\n",
       " 'min_child_samples': 20,\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 0.6}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(learning_rate = 0.05, \n",
    "                       max_depth = 3,\n",
    "                       subsample = 0.6,\n",
    "                       n_estimators = 100,\n",
    "                       min_child_samples = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 188, number of negative: 349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 606\n",
      "[LightGBM] [Info] Number of data points in the train set: 537, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350093 -> initscore=-0.618630\n",
      "[LightGBM] [Info] Start training from score -0.618630\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgbm_tuned = lgbm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489177489177489"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lgbm_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81       151\n",
      "           1       0.63      0.65      0.64        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.72      0.73      0.72       231\n",
      "weighted avg       0.75      0.75      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tüm Modellerin Karşılaştırılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "KNeighborsClassifier:\n",
      "Accuracy: 73.1602%\n",
      "----------------------------\n",
      "LogisticRegression:\n",
      "Accuracy: 75.3247%\n",
      "----------------------------\n",
      "SVC:\n",
      "Accuracy: 73.5931%\n",
      "----------------------------\n",
      "GaussianNB:\n",
      "Accuracy: 74.4589%\n",
      "----------------------------\n",
      "MLPClassifier:\n",
      "Accuracy: 74.8918%\n",
      "----------------------------\n",
      "DecisionTreeClassifier:\n",
      "Accuracy: 75.3247%\n",
      "----------------------------\n",
      "RandomForestClassifier:\n",
      "Accuracy: 76.1905%\n",
      "----------------------------\n",
      "GradientBoostingClassifier:\n",
      "Accuracy: 73.5931%\n",
      "----------------------------\n",
      "LGBMClassifier:\n",
      "Accuracy: 74.8918%\n",
      "----------------------------\n",
      "XGBClassifier:\n",
      "Accuracy: 76.6234%\n"
     ]
    }
   ],
   "source": [
    "modeller = [\n",
    "    knn_tuned,\n",
    "    loj_model,\n",
    "    svc_tuned,\n",
    "    nb_model,\n",
    "    mlpc_tuned,\n",
    "    cart_tuned,\n",
    "    rf_tuned,\n",
    "    gbm_tuned,\n",
    "    lgbm_tuned,\n",
    "    xgb_tuned\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "for model in modeller:\n",
    "    isimler = model.__class__.__name__\n",
    "    if isimler == \"MLPClassifier\":\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    dogruluk = accuracy_score(y_test, y_pred)\n",
    "    print(\"-\"*28)\n",
    "    print(isimler + \":\" )\n",
    "    print(\"Accuracy: {:.4%}\".format(dogruluk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sonuc = []\n",
    "\n",
    "sonuclar = pd.DataFrame(columns= [\"Modeller\",\"Accuracy\"])\n",
    "\n",
    "for model in modeller:\n",
    "    isimler = model.__class__.__name__\n",
    "    if isimler == \"MLPClassifier\":\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    dogruluk = accuracy_score(y_test, y_pred)    \n",
    "    sonuc = pd.DataFrame([[isimler, dogruluk*100]], columns= [\"Modeller\",\"Accuracy\"])\n",
    "    sonuclar = sonuclar.append(sonuc)\n",
    "    \n",
    "    \n",
    "sns.barplot(x= 'Accuracy', y = 'Modeller', data=sonuclar, color=\"r\")\n",
    "plt.xlabel('Accuracy %')\n",
    "plt.title('Modellerin Doğruluk Oranları');    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
